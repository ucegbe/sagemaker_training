{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "270633d3",
   "metadata": {},
   "source": [
    "# Amazon SageMaker LightGBM Distributed training using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c993e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41dbfb",
   "metadata": {},
   "source": [
    "---\n",
    "Losing customers is costly for any business. Identifying unhappy customers early on gives you a chance to offer them incentives to stay. This notebook describes using machine learning (ML) for the automated identification of unhappy customers, also known as customer churn prediction. ML models rarely give perfect predictions though, so this notebook is also about how to incorporate the relative costs of prediction mistakes when determining the financial outcome of using ML.\n",
    "\n",
    "This notebook demonstrates the use of distributed training for Amazon SageMaker’s implementation of the [LightGBM](https://lightgbm.readthedocs.io/en/latest/) with Dask.\n",
    "\n",
    "In this notebook, we demonstrate two use cases:\n",
    "\n",
    "* How to distributedly train a tabular model using Dask on the customer churn dataset.\n",
    "* How to use the trained tabular model to perform inference, i.e., classifying new samples.\n",
    "\n",
    "\n",
    "Note: This notebook was tested in Amazon SageMaker Studio on ml.t3.medium instance with Python 3 (Data Science) kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c6b9ec",
   "metadata": {},
   "source": [
    "## 1. Set Up\n",
    "\n",
    "---\n",
    "Before executing the notebook, there are some initial steps required for setup. This notebook requires latest version of sagemaker and ipywidgets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da04d4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca63c32-6928-48ae-bf8d-392cc25a637c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install s3fs --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb779656",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "To train and host on Amazon SageMaker, we need to setup and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook instance as the AWS account role with SageMaker access. It has necessary permissions, including access to your data in S3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d15a71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"sagemaker/test-churn-dt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adf9b9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40198d95",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Visualization\n",
    "\n",
    "Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes. After all, predicting the future is tricky business! But we’ll learn how to deal with prediction errors.\n",
    "\n",
    "The dataset we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets. Let’s download and read that dataset in now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "687a827d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    f\"sagemaker-example-files-prod-{aws_region}\",\n",
    "    \"datasets/tabular/synthetic/churn.txt\",\n",
    "    \"churn.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df8cfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Account Length</th>\n",
       "      <th>Area Code</th>\n",
       "      <th>Phone</th>\n",
       "      <th>Int'l Plan</th>\n",
       "      <th>VMail Plan</th>\n",
       "      <th>VMail Message</th>\n",
       "      <th>Day Mins</th>\n",
       "      <th>Day Calls</th>\n",
       "      <th>Day Charge</th>\n",
       "      <th>Eve Mins</th>\n",
       "      <th>Eve Calls</th>\n",
       "      <th>Eve Charge</th>\n",
       "      <th>Night Mins</th>\n",
       "      <th>Night Calls</th>\n",
       "      <th>Night Charge</th>\n",
       "      <th>Intl Mins</th>\n",
       "      <th>Intl Calls</th>\n",
       "      <th>Intl Charge</th>\n",
       "      <th>CustServ Calls</th>\n",
       "      <th>Churn?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PA</td>\n",
       "      <td>163</td>\n",
       "      <td>806</td>\n",
       "      <td>403-2562</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>300</td>\n",
       "      <td>8.162204</td>\n",
       "      <td>3</td>\n",
       "      <td>7.579174</td>\n",
       "      <td>3.933035</td>\n",
       "      <td>4</td>\n",
       "      <td>6.508639</td>\n",
       "      <td>4.065759</td>\n",
       "      <td>100</td>\n",
       "      <td>5.111624</td>\n",
       "      <td>4.928160</td>\n",
       "      <td>6</td>\n",
       "      <td>5.673203</td>\n",
       "      <td>3</td>\n",
       "      <td>True.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SC</td>\n",
       "      <td>15</td>\n",
       "      <td>836</td>\n",
       "      <td>158-8416</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>10.018993</td>\n",
       "      <td>4</td>\n",
       "      <td>4.226289</td>\n",
       "      <td>2.325005</td>\n",
       "      <td>0</td>\n",
       "      <td>9.972592</td>\n",
       "      <td>7.141040</td>\n",
       "      <td>200</td>\n",
       "      <td>6.436188</td>\n",
       "      <td>3.221748</td>\n",
       "      <td>6</td>\n",
       "      <td>2.559749</td>\n",
       "      <td>8</td>\n",
       "      <td>False.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MO</td>\n",
       "      <td>131</td>\n",
       "      <td>777</td>\n",
       "      <td>896-6253</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>300</td>\n",
       "      <td>4.708490</td>\n",
       "      <td>3</td>\n",
       "      <td>4.768160</td>\n",
       "      <td>4.537466</td>\n",
       "      <td>3</td>\n",
       "      <td>4.566715</td>\n",
       "      <td>5.363235</td>\n",
       "      <td>100</td>\n",
       "      <td>5.142451</td>\n",
       "      <td>7.139023</td>\n",
       "      <td>2</td>\n",
       "      <td>6.254157</td>\n",
       "      <td>4</td>\n",
       "      <td>False.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WY</td>\n",
       "      <td>75</td>\n",
       "      <td>878</td>\n",
       "      <td>817-5729</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>700</td>\n",
       "      <td>1.268734</td>\n",
       "      <td>3</td>\n",
       "      <td>2.567642</td>\n",
       "      <td>2.528748</td>\n",
       "      <td>5</td>\n",
       "      <td>2.333624</td>\n",
       "      <td>3.773586</td>\n",
       "      <td>450</td>\n",
       "      <td>3.814413</td>\n",
       "      <td>2.245779</td>\n",
       "      <td>6</td>\n",
       "      <td>1.080692</td>\n",
       "      <td>6</td>\n",
       "      <td>False.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WY</td>\n",
       "      <td>146</td>\n",
       "      <td>878</td>\n",
       "      <td>450-4942</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>2.696177</td>\n",
       "      <td>3</td>\n",
       "      <td>5.908916</td>\n",
       "      <td>6.015337</td>\n",
       "      <td>3</td>\n",
       "      <td>3.670408</td>\n",
       "      <td>3.751673</td>\n",
       "      <td>250</td>\n",
       "      <td>2.796812</td>\n",
       "      <td>6.905545</td>\n",
       "      <td>4</td>\n",
       "      <td>7.134343</td>\n",
       "      <td>6</td>\n",
       "      <td>True.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State  Account Length  Area Code     Phone Int'l Plan VMail Plan  \\\n",
       "0    PA             163        806  403-2562         no        yes   \n",
       "1    SC              15        836  158-8416        yes         no   \n",
       "2    MO             131        777  896-6253         no        yes   \n",
       "3    WY              75        878  817-5729        yes        yes   \n",
       "4    WY             146        878  450-4942        yes         no   \n",
       "\n",
       "   VMail Message   Day Mins  Day Calls  Day Charge  Eve Mins  Eve Calls  \\\n",
       "0            300   8.162204          3    7.579174  3.933035          4   \n",
       "1              0  10.018993          4    4.226289  2.325005          0   \n",
       "2            300   4.708490          3    4.768160  4.537466          3   \n",
       "3            700   1.268734          3    2.567642  2.528748          5   \n",
       "4              0   2.696177          3    5.908916  6.015337          3   \n",
       "\n",
       "   Eve Charge  Night Mins  Night Calls  Night Charge  Intl Mins  Intl Calls  \\\n",
       "0    6.508639    4.065759          100      5.111624   4.928160           6   \n",
       "1    9.972592    7.141040          200      6.436188   3.221748           6   \n",
       "2    4.566715    5.363235          100      5.142451   7.139023           2   \n",
       "3    2.333624    3.773586          450      3.814413   2.245779           6   \n",
       "4    3.670408    3.751673          250      2.796812   6.905545           4   \n",
       "\n",
       "   Intl Charge  CustServ Calls  Churn?  \n",
       "0     5.673203               3   True.  \n",
       "1     2.559749               8  False.  \n",
       "2     6.254157               4  False.  \n",
       "3     1.080692               6  False.  \n",
       "4     7.134343               6   True.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn = pd.read_csv(\"./churn.txt\")\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "churn.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b43e5c86-1af0-477d-b67e-05efad2287ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We increase the size of the table for the purpose of increasing the training data size\n",
    "churn = pd.concat([churn]*50, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcc68a02-1441-478d-baeb-fded04b30241",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250000 entries, 0 to 249999\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   State           250000 non-null  object \n",
      " 1   Account Length  250000 non-null  int64  \n",
      " 2   Area Code       250000 non-null  int64  \n",
      " 3   Phone           250000 non-null  object \n",
      " 4   Int'l Plan      250000 non-null  object \n",
      " 5   VMail Plan      250000 non-null  object \n",
      " 6   VMail Message   250000 non-null  int64  \n",
      " 7   Day Mins        250000 non-null  float64\n",
      " 8   Day Calls       250000 non-null  int64  \n",
      " 9   Day Charge      250000 non-null  float64\n",
      " 10  Eve Mins        250000 non-null  float64\n",
      " 11  Eve Calls       250000 non-null  int64  \n",
      " 12  Eve Charge      250000 non-null  float64\n",
      " 13  Night Mins      250000 non-null  float64\n",
      " 14  Night Calls     250000 non-null  int64  \n",
      " 15  Night Charge    250000 non-null  float64\n",
      " 16  Intl Mins       250000 non-null  float64\n",
      " 17  Intl Calls      250000 non-null  int64  \n",
      " 18  Intl Charge     250000 non-null  float64\n",
      " 19  CustServ Calls  250000 non-null  int64  \n",
      " 20  Churn?          250000 non-null  object \n",
      "dtypes: float64(8), int64(8), object(5)\n",
      "memory usage: 40.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(churn.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f8234",
   "metadata": {},
   "source": [
    "By modern standards, it’s a relatively small dataset, with only 5,000 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n",
    "\n",
    "`State`: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\n",
    "\n",
    "`Account Length`: the number of days that this account has been active\n",
    "\n",
    "`Area Code`: the three-digit area code of the corresponding customer’s phone number\n",
    "\n",
    "`Phone`: the remaining seven-digit phone number\n",
    "\n",
    "`Int’l Plan`: whether the customer has an international calling plan: yes/no\n",
    "\n",
    "`VMail Plan`: whether the customer has a voice mail feature: yes/no\n",
    "\n",
    "`VMail Message`: the average number of voice mail messages per month\n",
    "\n",
    "`Day Mins`: the total number of calling minutes used during the day\n",
    "\n",
    "`Day Calls`: the total number of calls placed during the day\n",
    "\n",
    "`Day Charge`: the billed cost of daytime calls\n",
    "\n",
    "`Eve Mins`, `Eve Calls`, `Eve Charge`: the billed cost for calls placed during the evening\n",
    "\n",
    "`Night Mins`, `Night Calls`, `Night Charge`: the billed cost for calls placed during nighttime\n",
    "\n",
    "`Intl Mins`, `Intl Calls`, `Intl Charge`: the billed cost for international calls\n",
    "\n",
    "`CustServ Calls`: the number of calls placed to Customer Service\n",
    "\n",
    "`Churn?`: whether the customer left the service: true/false\n",
    "\n",
    "The last attribute, `Churn?`, is known as the target attribute: the attribute that we want the ML model to predict. Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification.\n",
    "\n",
    "Let’s begin exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27af5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Histograms for each numeric features\n",
    "display(churn.describe())\n",
    "%matplotlib inline\n",
    "hist = churn.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987d494",
   "metadata": {},
   "source": [
    "We can see immediately that: - `State` appears to be quite evenly distributed. - `Phone` takes on too many unique values to be of any practical use. It’s possible that parsing out the prefix could have some value, but without more context on how these are allocated, we should avoid using it. - Most of the numeric features are surprisingly nicely distributed, with many showing bell-like gaussianity. `VMail Message` is a notable exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eb8b330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "churn = churn.drop(\"Phone\", axis=1)\n",
    "churn[\"Area Code\"] = churn[\"Area Code\"].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ceceb7",
   "metadata": {},
   "source": [
    "Next let’s look at the relationship between each of the features and our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06492d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for column in churn.select_dtypes(include=[\"object\"]).columns:\n",
    "    if column != \"Churn?\":\n",
    "        display(pd.crosstab(index=churn[column], columns=churn[\"Churn?\"], normalize=\"columns\"))\n",
    "\n",
    "for column in churn.select_dtypes(exclude=[\"object\"]).columns:\n",
    "    print(column)\n",
    "    hist = churn[[column, \"Churn?\"]].hist(by=\"Churn?\", bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35cecc",
   "metadata": {},
   "source": [
    "We convert the target attribute to binary value and move it to the first column of the dataset to meet requirements of SageMaker built-in tabular algorithms (For an example, see [SageMaker LightGBM documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/lightgbm.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "565c809d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "churn[\"target\"] = churn[\"Churn?\"].map({\"True.\": 1, \"False.\": 0})\n",
    "churn.drop([\"Churn?\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a231510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "churn = churn[[\"target\"] + churn.columns.tolist()[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a15e08",
   "metadata": {},
   "source": [
    "We identify the column indexes of the categorical attribute, which is required by LightGBM, CatBoost, and TabTransformer algorithm (AutoGluon-Tabular has built-in feature engineering to identify the categorical attribute automatically, and thus does not require such input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd4c11ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 8, 11, 14, 17, 19]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns = [\n",
    "    \"State\",\n",
    "    \"Account Length\",\n",
    "    \"Area Code\",\n",
    "    \"Phone\",\n",
    "    \"Int'l Plan\",\n",
    "    \"VMail Plan\",\n",
    "    \"VMail Message\",\n",
    "    \"Day Calls\",\n",
    "    \"Eve Calls\",\n",
    "    \"Night Calls\",\n",
    "    \"Intl Calls\",\n",
    "    \"CustServ Calls\",\n",
    "]\n",
    "\n",
    "cat_idx = []\n",
    "for idx, col_name in enumerate(churn.columns.tolist()):\n",
    "    if col_name in cat_columns:\n",
    "        cat_idx.append(idx)\n",
    "cat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "466d84a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"cat_idx.json\", \"w\") as outfile:\n",
    "    json.dump({\"cat_idx\": cat_idx}, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea910d97",
   "metadata": {},
   "source": [
    "[LightGBM official documentation](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#categorical-feature-support) requires that all categorical features should be encoded as non-negative integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6bb951cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, col_name in enumerate(churn.columns.tolist()):\n",
    "    if col_name in cat_columns:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        churn[col_name] = le.fit_transform(churn[col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ee7685d-25fe-4e71-920c-34a76f6d66c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>State</th>\n",
       "      <th>Account Length</th>\n",
       "      <th>Area Code</th>\n",
       "      <th>Int'l Plan</th>\n",
       "      <th>VMail Plan</th>\n",
       "      <th>VMail Message</th>\n",
       "      <th>Day Mins</th>\n",
       "      <th>Day Calls</th>\n",
       "      <th>Day Charge</th>\n",
       "      <th>Eve Mins</th>\n",
       "      <th>Eve Calls</th>\n",
       "      <th>Eve Charge</th>\n",
       "      <th>Night Mins</th>\n",
       "      <th>Night Calls</th>\n",
       "      <th>Night Charge</th>\n",
       "      <th>Intl Mins</th>\n",
       "      <th>Intl Calls</th>\n",
       "      <th>Intl Charge</th>\n",
       "      <th>CustServ Calls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>162</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8.162204</td>\n",
       "      <td>3</td>\n",
       "      <td>7.579174</td>\n",
       "      <td>3.933035</td>\n",
       "      <td>4</td>\n",
       "      <td>6.508639</td>\n",
       "      <td>4.065759</td>\n",
       "      <td>2</td>\n",
       "      <td>5.111624</td>\n",
       "      <td>4.928160</td>\n",
       "      <td>6</td>\n",
       "      <td>5.673203</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.018993</td>\n",
       "      <td>4</td>\n",
       "      <td>4.226289</td>\n",
       "      <td>2.325005</td>\n",
       "      <td>0</td>\n",
       "      <td>9.972592</td>\n",
       "      <td>7.141040</td>\n",
       "      <td>4</td>\n",
       "      <td>6.436188</td>\n",
       "      <td>3.221748</td>\n",
       "      <td>6</td>\n",
       "      <td>2.559749</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>130</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.708490</td>\n",
       "      <td>3</td>\n",
       "      <td>4.768160</td>\n",
       "      <td>4.537466</td>\n",
       "      <td>3</td>\n",
       "      <td>4.566715</td>\n",
       "      <td>5.363235</td>\n",
       "      <td>2</td>\n",
       "      <td>5.142451</td>\n",
       "      <td>7.139023</td>\n",
       "      <td>2</td>\n",
       "      <td>6.254157</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>74</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.268734</td>\n",
       "      <td>3</td>\n",
       "      <td>2.567642</td>\n",
       "      <td>2.528748</td>\n",
       "      <td>5</td>\n",
       "      <td>2.333624</td>\n",
       "      <td>3.773586</td>\n",
       "      <td>9</td>\n",
       "      <td>3.814413</td>\n",
       "      <td>2.245779</td>\n",
       "      <td>6</td>\n",
       "      <td>1.080692</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>145</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.696177</td>\n",
       "      <td>3</td>\n",
       "      <td>5.908916</td>\n",
       "      <td>6.015337</td>\n",
       "      <td>3</td>\n",
       "      <td>3.670408</td>\n",
       "      <td>3.751673</td>\n",
       "      <td>5</td>\n",
       "      <td>2.796812</td>\n",
       "      <td>6.905545</td>\n",
       "      <td>4</td>\n",
       "      <td>7.134343</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249995</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10.862632</td>\n",
       "      <td>5</td>\n",
       "      <td>7.250969</td>\n",
       "      <td>6.936164</td>\n",
       "      <td>1</td>\n",
       "      <td>8.026482</td>\n",
       "      <td>4.921314</td>\n",
       "      <td>7</td>\n",
       "      <td>6.748489</td>\n",
       "      <td>4.872570</td>\n",
       "      <td>8</td>\n",
       "      <td>2.122530</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249996</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>139</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.581127</td>\n",
       "      <td>8</td>\n",
       "      <td>3.758307</td>\n",
       "      <td>7.377591</td>\n",
       "      <td>7</td>\n",
       "      <td>1.328827</td>\n",
       "      <td>0.939932</td>\n",
       "      <td>6</td>\n",
       "      <td>4.522661</td>\n",
       "      <td>6.938571</td>\n",
       "      <td>2</td>\n",
       "      <td>4.600473</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249997</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.163836</td>\n",
       "      <td>5</td>\n",
       "      <td>4.243980</td>\n",
       "      <td>5.841852</td>\n",
       "      <td>3</td>\n",
       "      <td>2.340554</td>\n",
       "      <td>0.939469</td>\n",
       "      <td>9</td>\n",
       "      <td>5.157898</td>\n",
       "      <td>4.388328</td>\n",
       "      <td>7</td>\n",
       "      <td>1.060340</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249998</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>141</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2.034454</td>\n",
       "      <td>5</td>\n",
       "      <td>3.014859</td>\n",
       "      <td>4.140554</td>\n",
       "      <td>3</td>\n",
       "      <td>3.470372</td>\n",
       "      <td>6.076043</td>\n",
       "      <td>3</td>\n",
       "      <td>4.362780</td>\n",
       "      <td>7.173376</td>\n",
       "      <td>3</td>\n",
       "      <td>4.871900</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.803907</td>\n",
       "      <td>0</td>\n",
       "      <td>5.125716</td>\n",
       "      <td>8.357508</td>\n",
       "      <td>0</td>\n",
       "      <td>2.109823</td>\n",
       "      <td>2.624299</td>\n",
       "      <td>8</td>\n",
       "      <td>3.713631</td>\n",
       "      <td>5.798783</td>\n",
       "      <td>6</td>\n",
       "      <td>5.485345</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target  State  Account Length  Area Code  Int'l Plan  VMail Plan  \\\n",
       "0            1     38             162         22           0           1   \n",
       "1            0     40              14         24           1           0   \n",
       "2            0     24             130         15           0           1   \n",
       "3            0     50              74         32           1           1   \n",
       "4            1     50             145         32           1           0   \n",
       "...        ...    ...             ...        ...         ...         ...   \n",
       "249995       0     30               3         18           1           1   \n",
       "249996       0     41             139         24           0           0   \n",
       "249997       0     40              31         24           0           1   \n",
       "249998       1     19             141         14           1           1   \n",
       "249999       0      1             140          0           1           1   \n",
       "\n",
       "        VMail Message   Day Mins  Day Calls  Day Charge  Eve Mins  Eve Calls  \\\n",
       "0                   3   8.162204          3    7.579174  3.933035          4   \n",
       "1                   0  10.018993          4    4.226289  2.325005          0   \n",
       "2                   3   4.708490          3    4.768160  4.537466          3   \n",
       "3                   7   1.268734          3    2.567642  2.528748          5   \n",
       "4                   0   2.696177          3    5.908916  6.015337          3   \n",
       "...               ...        ...        ...         ...       ...        ...   \n",
       "249995              8  10.862632          5    7.250969  6.936164          1   \n",
       "249996              0   1.581127          8    3.758307  7.377591          7   \n",
       "249997              7   0.163836          5    4.243980  5.841852          3   \n",
       "249998              6   2.034454          5    3.014859  4.140554          3   \n",
       "249999              5   1.803907          0    5.125716  8.357508          0   \n",
       "\n",
       "        Eve Charge  Night Mins  Night Calls  Night Charge  Intl Mins  \\\n",
       "0         6.508639    4.065759            2      5.111624   4.928160   \n",
       "1         9.972592    7.141040            4      6.436188   3.221748   \n",
       "2         4.566715    5.363235            2      5.142451   7.139023   \n",
       "3         2.333624    3.773586            9      3.814413   2.245779   \n",
       "4         3.670408    3.751673            5      2.796812   6.905545   \n",
       "...            ...         ...          ...           ...        ...   \n",
       "249995    8.026482    4.921314            7      6.748489   4.872570   \n",
       "249996    1.328827    0.939932            6      4.522661   6.938571   \n",
       "249997    2.340554    0.939469            9      5.157898   4.388328   \n",
       "249998    3.470372    6.076043            3      4.362780   7.173376   \n",
       "249999    2.109823    2.624299            8      3.713631   5.798783   \n",
       "\n",
       "        Intl Calls  Intl Charge  CustServ Calls  \n",
       "0                6     5.673203               3  \n",
       "1                6     2.559749               8  \n",
       "2                2     6.254157               4  \n",
       "3                6     1.080692               6  \n",
       "4                4     7.134343               6  \n",
       "...            ...          ...             ...  \n",
       "249995           8     2.122530               9  \n",
       "249996           2     4.600473               4  \n",
       "249997           7     1.060340               6  \n",
       "249998           3     4.871900               7  \n",
       "249999           6     5.485345               7  \n",
       "\n",
       "[250000 rows x 20 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff6f07",
   "metadata": {},
   "source": [
    "We split the churn dataset into train, validation, and test set using stratified sampling. Validation set is used for early stopping and AMT. Test set is used for performance evaluations in the end. Next, we upload them into a S3 path for training.\n",
    "\n",
    "The structure of the S3 path for training should be structured as below.\n",
    "\n",
    "* The supported input data format for training is `csv`. You are allowed to put more than 1 data file under both train and valdiation channel. The name of data file can be any one as long as it ends with `.csv`.\n",
    "* The first column corresponds to the target and the rest of columns correspond to features. This follows the convention of [SageMaker XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). \n",
    "* The `cat_idx.json` is categorical column indexes. It contains a dictionary of a key-value pair. The key can be any string. The value is the list of column indexes of categorical features. The index starts with value 1 as value 0 corresponds to the target variable. Please see example above to format the `cat_idx.json`.\n",
    "* For the validation data, we encourage you to include one data file under its channel such that the all of the validation data points can be assigned to one machine. Thus, the validation score is for all of the validation data points and can be easily parsed by the AMT for hyperparameter optimization.\n",
    "* Current distributed training only supports CPU.\n",
    "\n",
    "-- `train`<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- `data_1.csv`<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- `data_2.csv`<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- `data_3.csv`<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- `cat_idx.json`\n",
    "    \n",
    "-- `validation`<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-- `data.csv`    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fede804a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val_n_test = train_test_split(\n",
    "    churn, test_size=0.3, random_state=42, stratify=churn[\"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "43d49126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val, test = train_test_split(\n",
    "    val_n_test, test_size=0.3, random_state=42, stratify=val_n_test[\"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3ef7194c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\", header=False, index=False)\n",
    "val.to_csv(\"validation.csv\", header=False, index=False)\n",
    "test.to_csv(\"test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4fe64",
   "metadata": {},
   "source": [
    "For demonstartion purpose on including multiple files under the training channel, we simply duplicate the training data multiple times as shown below. However, this is not mandatory and you can use just a single file for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "22bb7a84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:15<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(200)):\n",
    "    boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "        os.path.join(prefix, f\"train_set/data_{i}.csv\")\n",
    "    ).upload_file(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6a358d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"validation_set/data.csv\")\n",
    ").upload_file(\"validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "54c33e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"test_set/data.csv\")\n",
    ").upload_file(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f8c71dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train_set/cat_idx.json\")\n",
    ").upload_file(\"cat_idx.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0418e02",
   "metadata": {},
   "source": [
    "## 3. Distributedly Train A SageMaker LightGBM Model with AMT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20512887",
   "metadata": {},
   "source": [
    "### 3.1. Retrieve Training Artifacts\n",
    "\n",
    "___\n",
    "\n",
    "Here, we retrieve the training docker container, the training algorithm source, and the tabular algorithm. Note that model_version=\"*\" fetches the latest model.\n",
    "\n",
    "For the training algorithm, we have four choices in this demonstration for classification task.\n",
    "* [LightGBM](https://lightgbm.readthedocs.io/en/latest/): To use this algorithm, specify `train_model_id` as `lightgbm-classification-model` in the cell below.\n",
    "\n",
    "For regression task, the `train_model_id` is `lightgbm-regression-model`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8565b382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'lightgbm-classification-model' with wildcard version identifier '*'. You can pin to version '2.1.3' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "train_model_id, train_model_version, train_scope = \"lightgbm-classification-model\", \"*\", \"training\"\n",
    "training_instance_type = \"ml.m5.2xlarge\" \n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e958e5",
   "metadata": {},
   "source": [
    "### 3.2. Set Training Parameters\n",
    "\n",
    "---\n",
    "\n",
    "Now that we are done with all the setup that is needed, we are ready to train our tabular algorithm. To begin, let us create a [``sageMaker.estimator.Estimator``](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) object. This estimator will launch the training job. \n",
    "\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training.\n",
    "\n",
    "The second set of parameters are algorithm specific training hyper-parameters. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5c2074",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://sagemaker-us-east-1-715253196401/sagemaker/DEMO-churn-dt/train_set/',\n",
       " 's3://sagemaker-us-east-1-715253196401/sagemaker/DEMO-churn-dt/validation_set/')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_s3_path = f\"s3://{bucket}/{prefix}/train_set/\" \n",
    "validation_dataset_s3_path = f\"s3://{bucket}/{prefix}/validation_set/\"\n",
    "\n",
    "output_prefix = \"jumpstart-example-tabular-training\"\n",
    "s3_output_location = f\"s3://{bucket}/{output_prefix}/output_lgb\"\n",
    "training_dataset_s3_path, validation_dataset_s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74273473",
   "metadata": {},
   "source": [
    "---\n",
    "For algorithm specific hyper-parameters, we start by fetching python dictionary of the training hyper-parameters that the algorithm accepts with their default values. This can then be overridden to custom values. For the evaluation metric that is used by early stopping and automatic model tuning, we choose `auc` score. Note. LightGBM does not have built-in F1 score supported. See [LightGBM documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric-parameters).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1c5b649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_boost_round': '200', 'early_stopping_rounds': '30', 'metric': 'auc', 'learning_rate': '0.009', 'num_leaves': '67', 'feature_fraction': '0.74', 'bagging_fraction': '0.53', 'bagging_freq': '5', 'max_depth': '11', 'min_data_in_leaf': '26', 'max_delta_step': '0.0', 'lambda_l1': '0.0', 'lambda_l2': '0.0', 'boosting': 'gbdt', 'min_gain_to_split': '0.0', 'scale_pos_weight': '1.0', 'tree_learner': 'serial', 'feature_fraction_bynode': '1.0', 'is_unbalance': 'False', 'max_bin': '255', 'num_threads': '0', 'verbosity': '1', 'use_dask': 'False'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version\n",
    ")\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"num_boost_round\"] = \"200\"\n",
    "hyperparameters[\"metric\"] = \"auc\"\n",
    "\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b3d87",
   "metadata": {},
   "source": [
    "### 3.3. Train with Automatic Model Tuning  \n",
    "\n",
    "\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. We will use a HyperparameterTuner object to interact with Amazon SageMaker hyperparameter tuning APIs.\n",
    "\n",
    "* Note. In this notebook, we set AMT budget (total tuning jobs) as 10 for each of the tabular algorithm except AutoGluon-Tabular. For [AutoGluon-Tabular](https://arxiv.org/abs/2003.06505), it succeeds by ensembling multiple models and stacking them in multiple layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7c8d7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter, HyperparameterTuner\n",
    "#Enable HyperParameterTuning\n",
    "use_amt = False\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges_lgb = {\n",
    "    \"learning_rate\": ContinuousParameter(1e-4, 1, scaling_type=\"Logarithmic\"),\n",
    "    \"num_boost_round\": IntegerParameter(2, 30),\n",
    "    \"num_leaves\": IntegerParameter(10, 50),\n",
    "    \"feature_fraction\": ContinuousParameter(0.1, 1),\n",
    "    \"bagging_fraction\": ContinuousParameter(0.1, 1),\n",
    "    \"bagging_freq\": IntegerParameter(1, 10),\n",
    "    \"max_depth\": IntegerParameter(5, 30),\n",
    "    \"min_data_in_leaf\": IntegerParameter(5, 50),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60b812",
   "metadata": {},
   "source": [
    "### 3.4. Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d427761",
   "metadata": {},
   "source": [
    "---\n",
    "We start by creating the estimator object with all the required assets and then launch the training job. \n",
    "\n",
    "* To enable distributed training, you only need specify the number of instances to be more than 1.\n",
    "* You might need increase the argument volumn_size if your dataset size is larger than the default value (30GB). Otherwise, you may see insufficient disk memory error.\n",
    "\n",
    "SageMaker Training Directory Setup for Script Mode:\n",
    "\n",
    "- Create a root project directory.\n",
    "- Place main training script (e.g., train.py) in root.\n",
    "- Add other Python modules/scripts to root or subdirectories.\n",
    "- Include requirements.txt for dependencies. Sagemaker automatically installs all libs listed in this text file\n",
    "\n",
    "Example structure:\n",
    "```\n",
    "project/\n",
    "    ├── train.py\n",
    "    ├── requirements.txt\n",
    "    ├── utils.py\n",
    "\n",
    "```\n",
    "\n",
    "SageMaker estimator setup:\n",
    "```\n",
    "estimator = Estimator(\n",
    "    entry_point='train.py',\n",
    "    source_dir='path/to/project',\n",
    "    ...\n",
    ")\n",
    "```\n",
    "Key points:\n",
    "\n",
    "    Include all necessary code files.\n",
    "    List dependencies in requirements.txt.\n",
    "    SageMaker packages entire directory content.    \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46cfd39e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-jumpstart-2024-10-27-04-30-43-804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-27 04:30:46 Starting - Starting the training job...\n",
      "2024-10-27 04:31:00 Starting - Preparing the instances for training...\n",
      "2024-10-27 04:31:36 Downloading - Downloading input data......\n",
      "2024-10-27 04:32:37 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-10-27 04:32:48,303 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-10-27 04:32:48,305 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-10-27 04:32:48,315 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:32:48,317 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:32:48,798 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/dask/dask-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/distributed/distributed-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/graphviz/graphviz-0.17-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/HeapDict/HeapDict-1.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lightgbm/lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/locket/locket-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/msgpack/msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/partd/partd-1.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sortedcontainers/sortedcontainers-2.4.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/toolz/toolz-0.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/zict/zict-2.2.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting mlflow==2.13.2\u001b[0m\n",
      "\u001b[34mDownloading mlflow-2.13.2-py3-none-any.whl (25.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker-mlflow==0.1.0\u001b[0m\n",
      "\u001b[34mDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging<25 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz<2025 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (5.0.0)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6,>=5.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.4.3)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-api<3,>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting gitpython<4,>=3.1.9\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<3 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[34mCollecting graphene<4\u001b[0m\n",
      "\u001b[34mDownloading graphene-3.4-py2.py3-none-any.whl (114 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (0.24.2)\u001b[0m\n",
      "\u001b[34mCollecting markdown<4,>=3.3\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.7-py3-none-any.whl (106 kB)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints<1\u001b[0m\n",
      "\u001b[34mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting querystring-parser<2\u001b[0m\n",
      "\u001b[34mDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (4.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting Flask<4\u001b[0m\n",
      "\u001b[34mDownloading flask-3.0.3-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy<3,>=1.4.0\u001b[0m\n",
      "\u001b[34mDownloading SQLAlchemy-2.0.36-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-sdk<3,>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker<8,>=4.0.0\u001b[0m\n",
      "\u001b[34mDownloading docker-7.1.0-py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mCollecting sqlparse<1,>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.0.2)\u001b[0m\n",
      "\u001b[34mCollecting gunicorn<23\u001b[0m\n",
      "\u001b[34mDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic!=1.10.0,<2\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-10-27 04:32:48,570 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-10-27 04:32:48,572 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-10-27 04:32:48,581 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-10-27 04:32:48,583 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2024-10-27 04:32:48,856 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing ./lib/dask/dask-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/distributed/distributed-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/graphviz/graphviz-0.17-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/HeapDict/HeapDict-1.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/lightgbm/lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/locket/locket-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/msgpack/msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/partd/partd-1.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sortedcontainers/sortedcontainers-2.4.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/toolz/toolz-0.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/zict/zict-2.2.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mCollecting mlflow==2.13.2\u001b[0m\n",
      "\u001b[35mDownloading mlflow-2.13.2-py3-none-any.whl (25.0 MB)\u001b[0m\n",
      "\u001b[35mCollecting sagemaker-mlflow==0.1.0\u001b[0m\n",
      "\u001b[35mDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[35mCollecting sqlparse<1,>=0.4.0\u001b[0m\n",
      "\u001b[35mDownloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[35mCollecting querystring-parser<2\u001b[0m\n",
      "\u001b[35mDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting Flask<4\u001b[0m\n",
      "\u001b[35mDownloading flask-3.0.3-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[35mCollecting opentelemetry-sdk<3,>=1.0.0\u001b[0m\n",
      "\u001b[35mDownloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (5.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas<3 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.2.4)\u001b[0m\n",
      "\u001b[35mCollecting cachetools<6,>=5.0.0\u001b[0m\n",
      "\u001b[35mDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.18.1)\u001b[0m\n",
      "\u001b[35mCollecting entrypoints<1\u001b[0m\n",
      "\u001b[35mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting graphene<4\u001b[0m\n",
      "\u001b[35mDownloading graphene-3.4-py2.py3-none-any.whl (114 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (4.8.1)\u001b[0m\n",
      "\u001b[35mCollecting docker<8,>=4.0.0\u001b[0m\n",
      "\u001b[35mDownloading docker-7.1.0-py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[35mCollecting gitpython<4,>=3.1.9\u001b[0m\n",
      "\u001b[35mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown<4,>=3.3\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.7-py3-none-any.whl (106 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.4.3)\u001b[0m\n",
      "\u001b[35mCollecting alembic!=1.10.0,<2\u001b[0m\n",
      "\u001b[35mDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz<2025 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (0.24.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging<25 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[35mCollecting gunicorn<23\u001b[0m\n",
      "\u001b[35mDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[35mCollecting opentelemetry-api<3,>=1.0.0\u001b[0m\n",
      "\u001b[35mDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[35mCollecting sqlalchemy<3,>=1.4.0\u001b[0m\n",
      "\u001b[35mDownloading SQLAlchemy-2.0.36-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting boto3>=1.34\u001b[0m\n",
      "\u001b[34mDownloading boto3-1.35.49-py3-none-any.whl (139 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 3)) (2021.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (5.6.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (1.26.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 7)) (0.37.0)\u001b[0m\n",
      "\u001b[35mCollecting boto3>=1.34\u001b[0m\n",
      "\u001b[35mDownloading boto3-1.35.49-py3-none-any.whl (139 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 3)) (2021.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (6.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (5.6.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (1.26.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 7)) (0.37.0)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources\u001b[0m\n",
      "\u001b[34mDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=4\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mCollecting Mako\u001b[0m\n",
      "\u001b[34mDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34mCollecting s3transfer<0.11.0,>=0.10.0\u001b[0m\n",
      "\u001b[34mDownloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\u001b[0m\n",
      "\u001b[35mCollecting importlib-resources\u001b[0m\n",
      "\u001b[35mDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[35mCollecting Mako\u001b[0m\n",
      "\u001b[35mDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[35mCollecting typing-extensions>=4\u001b[0m\n",
      "\u001b[35mDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[35mCollecting s3transfer<0.11.0,>=0.10.0\u001b[0m\n",
      "\u001b[35mDownloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.34->sagemaker-mlflow==0.1.0->-r requirements.txt (line 2)) (0.10.0)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.36.0,>=1.35.49\u001b[0m\n",
      "\u001b[34mDownloading botocore-1.35.49-py3-none-any.whl (12.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.34->sagemaker-mlflow==0.1.0->-r requirements.txt (line 2)) (0.10.0)\u001b[0m\n",
      "\u001b[34mCollecting Werkzeug>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[35mCollecting botocore<1.36.0,>=1.35.49\u001b[0m\n",
      "\u001b[35mDownloading botocore-1.35.49-py3-none-any.whl (12.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting blinker>=1.6.2\u001b[0m\n",
      "\u001b[34mDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting click<9,>=7.0\u001b[0m\n",
      "\u001b[34mDownloading click-8.1.7-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting Jinja2<4,>=2.11\u001b[0m\n",
      "\u001b[34mDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34mCollecting itsdangerous>=2.1.2\u001b[0m\n",
      "\u001b[34mDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphql-core<3.3,>=3.1\u001b[0m\n",
      "\u001b[34mDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphql-relay<3.3,>=3.1\u001b[0m\n",
      "\u001b[34mDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.13.2->-r requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2<4,>=2.11->mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (8.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (1.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata!=4.7.0,<8,>=3.7.0\u001b[0m\n",
      "\u001b[34mDownloading importlib_metadata-7.2.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting deprecated>=1.2.6\u001b[0m\n",
      "\u001b[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-semantic-conventions==0.48b0\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from querystring-parser<2->mlflow==2.13.2->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow==2.13.2->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow==2.13.2->-r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.13.2->-r requirements.txt (line 1)) (1.1.2)\u001b[0m\n",
      "\u001b[35mCollecting itsdangerous>=2.1.2\u001b[0m\n",
      "\u001b[35mDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[35mCollecting blinker>=1.6.2\u001b[0m\n",
      "\u001b[35mDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting click<9,>=7.0\u001b[0m\n",
      "\u001b[35mDownloading click-8.1.7-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[35mCollecting Werkzeug>=3.0.0\u001b[0m\n",
      "\u001b[35mDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[35mCollecting Jinja2<4,>=2.11\u001b[0m\n",
      "\u001b[35mDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[35mCollecting gitdb<5,>=4.0.1\u001b[0m\n",
      "\u001b[35mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[35mCollecting graphql-core<3.3,>=3.1\u001b[0m\n",
      "\u001b[35mDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\u001b[0m\n",
      "\u001b[35mCollecting graphql-relay<3.3,>=3.1\u001b[0m\n",
      "\u001b[35mDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.13.2->-r requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2<4,>=2.11->mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (1.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (8.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[35mCollecting deprecated>=1.2.6\u001b[0m\n",
      "\u001b[35mDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting importlib-metadata!=4.7.0,<8,>=3.7.0\u001b[0m\n",
      "\u001b[35mDownloading importlib_metadata-7.2.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[35mCollecting opentelemetry-semantic-conventions==0.48b0\u001b[0m\n",
      "\u001b[35mDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from querystring-parser<2->mlflow==2.13.2->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (2021.10.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow==2.13.2->-r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow==2.13.2->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.13.2->-r requirements.txt (line 1)) (1.1.2)\u001b[0m\n",
      "\u001b[34mCollecting wrapt<2,>=1.10\u001b[0m\n",
      "\u001b[34mDownloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-10-27 04:32:53,366 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-10-27 04:32:53,368 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-10-27 04:32:53,378 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:32:53,380 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:32:53,625 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/dask/dask-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/distributed/distributed-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/graphviz/graphviz-0.17-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/HeapDict/HeapDict-1.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lightgbm/lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/locket/locket-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/msgpack/msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/partd/partd-1.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sortedcontainers/sortedcontainers-2.4.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/toolz/toolz-0.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/zict/zict-2.2.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting mlflow==2.13.2\u001b[0m\n",
      "\u001b[34mDownloading mlflow-2.13.2-py3-none-any.whl (25.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker-mlflow==0.1.0\u001b[0m\n",
      "\u001b[34mDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.4.3)\u001b[0m\n",
      "\u001b[34mCollecting docker<8,>=4.0.0\u001b[0m\n",
      "\u001b[34mDownloading docker-7.1.0-py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (5.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy<3,>=1.4.0\u001b[0m\n",
      "\u001b[34mDownloading SQLAlchemy-2.0.36-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-api<3,>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mCollecting querystring-parser<2\u001b[0m\n",
      "\u001b[34mDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting sqlparse<1,>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphene<4\u001b[0m\n",
      "\u001b[34mDownloading graphene-3.4-py2.py3-none-any.whl (114 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<3 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.2.4)\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-10-27 04:32:53,239 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-10-27 04:32:53,241 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-10-27 04:32:53,251 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-10-27 04:32:53,253 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2024-10-27 04:32:53,516 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing ./lib/dask/dask-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/distributed/distributed-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/graphviz/graphviz-0.17-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/HeapDict/HeapDict-1.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/lightgbm/lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/locket/locket-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/msgpack/msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/partd/partd-1.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sortedcontainers/sortedcontainers-2.4.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/toolz/toolz-0.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/zict/zict-2.2.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mCollecting mlflow==2.13.2\u001b[0m\n",
      "\u001b[35mDownloading mlflow-2.13.2-py3-none-any.whl (25.0 MB)\u001b[0m\n",
      "\u001b[35mCollecting sagemaker-mlflow==0.1.0\u001b[0m\n",
      "\u001b[35mDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas<3 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.2.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.0.2)\u001b[0m\n",
      "\u001b[35mCollecting sqlparse<1,>=0.4.0\u001b[0m\n",
      "\u001b[35mDownloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (5.0.0)\u001b[0m\n",
      "\u001b[35mCollecting docker<8,>=4.0.0\u001b[0m\n",
      "\u001b[35mDownloading docker-7.1.0-py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown<4,>=3.3\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.7-py3-none-any.whl (106 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (0.24.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.18.1)\u001b[0m\n",
      "\u001b[35mCollecting opentelemetry-api<3,>=1.0.0\u001b[0m\n",
      "\u001b[35mDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[35mCollecting sqlalchemy<3,>=1.4.0\u001b[0m\n",
      "\u001b[35mDownloading SQLAlchemy-2.0.36-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001b[0m\n",
      "\u001b[35mCollecting Flask<4\u001b[0m\n",
      "\u001b[35mDownloading flask-3.0.3-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz<2025 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[35mCollecting opentelemetry-sdk<3,>=1.0.0\u001b[0m\n",
      "\u001b[35mDownloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.4.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging<25 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (4.8.1)\u001b[0m\n",
      "\u001b[35mCollecting entrypoints<1\u001b[0m\n",
      "\u001b[35mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[35mCollecting gitpython<4,>=3.1.9\u001b[0m\n",
      "\u001b[35mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[35mCollecting gunicorn<23\u001b[0m\n",
      "\u001b[35mDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[35mCollecting querystring-parser<2\u001b[0m\n",
      "\u001b[35mDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting alembic!=1.10.0,<2\u001b[0m\n",
      "\u001b[35mDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.7.1)\u001b[0m\n",
      "\u001b[35mCollecting cachetools<6,>=5.0.0\u001b[0m\n",
      "\u001b[35mDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting graphene<4\u001b[0m\n",
      "\u001b[35mDownloading graphene-3.4-py2.py3-none-any.whl (114 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown<4,>=3.3\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.7-py3-none-any.whl (106 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-sdk<3,>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging<25 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting gunicorn<23\u001b[0m\n",
      "\u001b[34mDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitpython<4,>=3.1.9\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.18.1)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints<1\u001b[0m\n",
      "\u001b[34mDownloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting Flask<4\u001b[0m\n",
      "\u001b[34mDownloading flask-3.0.3-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic!=1.10.0,<2\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (4.8.1)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6,>=5.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz<2025 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.8/site-packages (from mlflow==2.13.2->-r requirements.txt (line 1)) (3.0.2)\u001b[0m\n",
      "\u001b[36mCollecting wrapt<2,>=1.10\u001b[0m\n",
      "\u001b[36mDownloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\u001b[0m\n",
      "\u001b[36mCollecting smmap<6,>=3.0.1\u001b[0m\n",
      "\u001b[36mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting boto3>=1.34\u001b[0m\n",
      "\u001b[34mDownloading boto3-1.35.49-py3-none-any.whl (139 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 3)) (2021.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (1.26.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (5.6.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 7)) (0.37.0)\u001b[0m\n",
      "\u001b[34mCollecting Mako\u001b[0m\n",
      "\u001b[34mDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources\u001b[0m\n",
      "\u001b[34mDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[32mCollecting MarkupSafe>=2.0\u001b[0m\n",
      "\u001b[32mDownloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\u001b[0m\n",
      "\u001b[32mInstalling collected packages: wrapt, typing-extensions, importlib-metadata, deprecated, smmap, opentelemetry-api, MarkupSafe, graphql-core, Werkzeug, toolz, sqlalchemy, opentelemetry-semantic-conventions, Mako, locket, Jinja2, itsdangerous, importlib-resources, graphql-relay, gitdb, click, botocore, blinker, sqlparse, s3transfer, querystring-parser, partd, opentelemetry-sdk, markdown, HeapDict, gunicorn, graphene, gitpython, Flask, entrypoints, docker, cachetools, alembic, zict, tblib, sortedcontainers, msgpack, mlflow, dask, boto3, sagemaker-mlflow, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-prepack-script-utilities, lightgbm, graphviz, distributed\u001b[0m\n",
      "\u001b[32mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[32mFound existing installation: typing-extensions 3.10.0.2\u001b[0m\n",
      "\u001b[32mUninstalling typing-extensions-3.10.0.2:\u001b[0m\n",
      "\u001b[36mCollecting MarkupSafe>=2.0\u001b[0m\n",
      "\u001b[36mDownloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\u001b[0m\n",
      "\u001b[35mCollecting boto3>=1.34\u001b[0m\n",
      "\u001b[35mDownloading boto3-1.35.49-py3-none-any.whl (139 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 3)) (2021.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (6.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (1.26.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 4)) (5.6.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 7)) (0.37.0)\u001b[0m\n",
      "\u001b[35mCollecting importlib-resources\u001b[0m\n",
      "\u001b[35mDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[35mCollecting Mako\u001b[0m\n",
      "\u001b[35mDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[35mCollecting typing-extensions>=4\u001b[0m\n",
      "\u001b[35mDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=4\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mCollecting s3transfer<0.11.0,>=0.10.0\u001b[0m\n",
      "\u001b[34mDownloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled typing-extensions-3.10.0.2\u001b[0m\n",
      "\u001b[32mAttempting uninstall: importlib-metadata\u001b[0m\n",
      "\u001b[32mFound existing installation: importlib-metadata 4.8.1\u001b[0m\n",
      "\u001b[32mUninstalling importlib-metadata-4.8.1:\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled importlib-metadata-4.8.1\u001b[0m\n",
      "\u001b[32mAttempting uninstall: MarkupSafe\u001b[0m\n",
      "\u001b[32mFound existing installation: MarkupSafe 2.0.1\u001b[0m\n",
      "\u001b[32mUninstalling MarkupSafe-2.0.1:\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled MarkupSafe-2.0.1\u001b[0m\n",
      "\u001b[32mAttempting uninstall: Werkzeug\u001b[0m\n",
      "\u001b[32mFound existing installation: Werkzeug 2.0.2\u001b[0m\n",
      "\u001b[32mUninstalling Werkzeug-2.0.2:\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled Werkzeug-2.0.2\u001b[0m\n",
      "\u001b[36mInstalling collected packages: wrapt, typing-extensions, importlib-metadata, deprecated, smmap, opentelemetry-api, MarkupSafe, graphql-core, Werkzeug, toolz, sqlalchemy, opentelemetry-semantic-conventions, Mako, locket, Jinja2, itsdangerous, importlib-resources, graphql-relay, gitdb, click, botocore, blinker, sqlparse, s3transfer, querystring-parser, partd, opentelemetry-sdk, markdown, HeapDict, gunicorn, graphene, gitpython, Flask, entrypoints, docker, cachetools, alembic, zict, tblib, sortedcontainers, msgpack, mlflow, dask, boto3, sagemaker-mlflow, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-prepack-script-utilities, lightgbm, graphviz, distributed\u001b[0m\n",
      "\u001b[36mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[36mFound existing installation: typing-extensions 3.10.0.2\u001b[0m\n",
      "\u001b[36mUninstalling typing-extensions-3.10.0.2:\u001b[0m\n",
      "\u001b[36mSuccessfully uninstalled typing-extensions-3.10.0.2\u001b[0m\n",
      "\u001b[36mAttempting uninstall: importlib-metadata\u001b[0m\n",
      "\u001b[36mFound existing installation: importlib-metadata 4.8.1\u001b[0m\n",
      "\u001b[36mUninstalling importlib-metadata-4.8.1:\u001b[0m\n",
      "\u001b[36mSuccessfully uninstalled importlib-metadata-4.8.1\u001b[0m\n",
      "\u001b[36mAttempting uninstall: MarkupSafe\u001b[0m\n",
      "\u001b[36mFound existing installation: MarkupSafe 2.0.1\u001b[0m\n",
      "\u001b[36mUninstalling MarkupSafe-2.0.1:\u001b[0m\n",
      "\u001b[36mSuccessfully uninstalled MarkupSafe-2.0.1\u001b[0m\n",
      "\u001b[36mAttempting uninstall: Werkzeug\u001b[0m\n",
      "\u001b[36mFound existing installation: Werkzeug 2.0.2\u001b[0m\n",
      "\u001b[36mUninstalling Werkzeug-2.0.2:\u001b[0m\n",
      "\u001b[36mSuccessfully uninstalled Werkzeug-2.0.2\u001b[0m\n",
      "\u001b[35mCollecting botocore<1.36.0,>=1.35.49\u001b[0m\n",
      "\u001b[35mDownloading botocore-1.35.49-py3-none-any.whl (12.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.36.0,>=1.35.49\u001b[0m\n",
      "\u001b[34mDownloading botocore-1.35.49-py3-none-any.whl (12.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.34->sagemaker-mlflow==0.1.0->-r requirements.txt (line 2)) (0.10.0)\u001b[0m\n",
      "\u001b[34mCollecting Jinja2<4,>=2.11\u001b[0m\n",
      "\u001b[34mDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34mCollecting blinker>=1.6.2\u001b[0m\n",
      "\u001b[34mDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting itsdangerous>=2.1.2\u001b[0m\n",
      "\u001b[34mDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting Werkzeug>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[32mAttempting uninstall: Jinja2\u001b[0m\n",
      "\u001b[32mFound existing installation: Jinja2 3.0.2\u001b[0m\n",
      "\u001b[32mUninstalling Jinja2-3.0.2:\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled Jinja2-3.0.2\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.34->sagemaker-mlflow==0.1.0->-r requirements.txt (line 2)) (0.10.0)\u001b[0m\n",
      "\u001b[35mCollecting s3transfer<0.11.0,>=0.10.0\u001b[0m\n",
      "\u001b[35mDownloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\u001b[0m\n",
      "\u001b[35mCollecting click<9,>=7.0\u001b[0m\n",
      "\u001b[35mDownloading click-8.1.7-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[35mCollecting Jinja2<4,>=2.11\u001b[0m\n",
      "\u001b[35mDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[35mCollecting blinker>=1.6.2\u001b[0m\n",
      "\u001b[35mDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting itsdangerous>=2.1.2\u001b[0m\n",
      "\u001b[35mDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[35mCollecting Werkzeug>=3.0.0\u001b[0m\n",
      "\u001b[35mDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\u001b[0m\n",
      "\u001b[35mCollecting gitdb<5,>=4.0.1\u001b[0m\n",
      "\u001b[35mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[35mCollecting graphql-relay<3.3,>=3.1\u001b[0m\n",
      "\u001b[35mDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[35mCollecting graphql-core<3.3,>=3.1\u001b[0m\n",
      "\u001b[35mDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.13.2->-r requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2<4,>=2.11->mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (8.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (1.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[35mCollecting deprecated>=1.2.6\u001b[0m\n",
      "\u001b[35mDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting importlib-metadata!=4.7.0,<8,>=3.7.0\u001b[0m\n",
      "\u001b[35mDownloading importlib_metadata-7.2.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting click<9,>=7.0\u001b[0m\n",
      "\u001b[34mDownloading click-8.1.7-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphql-relay<3.3,>=3.1\u001b[0m\n",
      "\u001b[34mDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphql-core<3.3,>=3.1\u001b[0m\n",
      "\u001b[34mDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.13.2->-r requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2<4,>=2.11->mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (8.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow==2.13.2->-r requirements.txt (line 1)) (1.3.2)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata!=4.7.0,<8,>=3.7.0\u001b[0m\n",
      "\u001b[34mDownloading importlib_metadata-7.2.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting deprecated>=1.2.6\u001b[0m\n",
      "\u001b[34mDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-semantic-conventions==0.48b0\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from querystring-parser<2->mlflow==2.13.2->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow==2.13.2->-r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow==2.13.2->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.13.2->-r requirements.txt (line 1)) (1.1.2)\u001b[0m\n",
      "\u001b[32mAttempting uninstall: click\u001b[0m\n",
      "\u001b[32mFound existing installation: click 8.0.3\u001b[0m\n",
      "\u001b[32mUninstalling click-8.0.3:\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled click-8.0.3\u001b[0m\n",
      "\u001b[32mAttempting uninstall: botocore\u001b[0m\n",
      "\u001b[32mFound existing installation: botocore 1.21.60\u001b[0m\n",
      "\u001b[32mUninstalling botocore-1.21.60:\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled botocore-1.21.60\u001b[0m\n",
      "\u001b[36mAttempting uninstall: Jinja2\u001b[0m\n",
      "\u001b[36mFound existing installation: Jinja2 3.0.2\u001b[0m\n",
      "\u001b[36mUninstalling Jinja2-3.0.2:\u001b[0m\n",
      "\u001b[36mSuccessfully uninstalled Jinja2-3.0.2\u001b[0m\n",
      "\u001b[36mAttempting uninstall: click\u001b[0m\n",
      "\u001b[36mFound existing installation: click 8.0.3\u001b[0m\n",
      "\u001b[36mUninstalling click-8.0.3:\u001b[0m\n",
      "\u001b[36mSuccessfully uninstalled click-8.0.3\u001b[0m\n",
      "\u001b[36mAttempting uninstall: botocore\u001b[0m\n",
      "\u001b[36mFound existing installation: botocore 1.21.60\u001b[0m\n",
      "\u001b[36mUninstalling botocore-1.21.60:\u001b[0m\n",
      "\u001b[36mSuccessfully uninstalled botocore-1.21.60\u001b[0m\n",
      "\u001b[35mCollecting opentelemetry-semantic-conventions==0.48b0\u001b[0m\n",
      "\u001b[35mDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from querystring-parser<2->mlflow==2.13.2->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2->-r requirements.txt (line 1)) (2021.10.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow==2.13.2->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow==2.13.2->-r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.13.2->-r requirements.txt (line 1)) (1.1.2)\u001b[0m\n",
      "\u001b[35mCollecting wrapt<2,>=1.10\u001b[0m\n",
      "\u001b[35mDownloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\u001b[0m\n",
      "\u001b[35mCollecting smmap<6,>=3.0.1\u001b[0m\n",
      "\u001b[35mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting wrapt<2,>=1.10\u001b[0m\n",
      "\u001b[34mDownloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting MarkupSafe>=2.0\u001b[0m\n",
      "\u001b[34mDownloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\u001b[0m\n",
      "\u001b[32mAttempting uninstall: s3transfer\u001b[0m\n",
      "\u001b[32mFound existing installation: s3transfer 0.5.0\u001b[0m\n",
      "\u001b[32mUninstalling s3transfer-0.5.0:\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled s3transfer-0.5.0\u001b[0m\n",
      "\u001b[36mAttempting uninstall: s3transfer\u001b[0m\n",
      "\u001b[36mFound existing installation: s3transfer 0.5.0\u001b[0m\n",
      "\u001b[36mUninstalling s3transfer-0.5.0:\u001b[0m\n",
      "\u001b[36mSuccessfully uninstalled s3transfer-0.5.0\u001b[0m\n",
      "\u001b[35mCollecting MarkupSafe>=2.0\u001b[0m\n",
      "\u001b[35mDownloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: wrapt, typing-extensions, importlib-metadata, deprecated, smmap, opentelemetry-api, MarkupSafe, graphql-core, Werkzeug, toolz, sqlalchemy, opentelemetry-semantic-conventions, Mako, locket, Jinja2, itsdangerous, importlib-resources, graphql-relay, gitdb, click, botocore, blinker, sqlparse, s3transfer, querystring-parser, partd, opentelemetry-sdk, markdown, HeapDict, gunicorn, graphene, gitpython, Flask, entrypoints, docker, cachetools, alembic, zict, tblib, sortedcontainers, msgpack, mlflow, dask, boto3, sagemaker-mlflow, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-prepack-script-utilities, lightgbm, graphviz, distributed\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing-extensions 3.10.0.2\u001b[0m\n",
      "\u001b[34mUninstalling typing-extensions-3.10.0.2:\u001b[0m\n",
      "\u001b[35mInstalling collected packages: wrapt, typing-extensions, importlib-metadata, deprecated, smmap, opentelemetry-api, MarkupSafe, graphql-core, Werkzeug, toolz, sqlalchemy, opentelemetry-semantic-conventions, Mako, locket, Jinja2, itsdangerous, importlib-resources, graphql-relay, gitdb, click, botocore, blinker, sqlparse, s3transfer, querystring-parser, partd, opentelemetry-sdk, markdown, HeapDict, gunicorn, graphene, gitpython, Flask, entrypoints, docker, cachetools, alembic, zict, tblib, sortedcontainers, msgpack, mlflow, dask, boto3, sagemaker-mlflow, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-prepack-script-utilities, lightgbm, graphviz, distributed\u001b[0m\n",
      "\u001b[35mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[35mFound existing installation: typing-extensions 3.10.0.2\u001b[0m\n",
      "\u001b[35mUninstalling typing-extensions-3.10.0.2:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled typing-extensions-3.10.0.2\u001b[0m\n",
      "\u001b[35mAttempting uninstall: importlib-metadata\u001b[0m\n",
      "\u001b[35mFound existing installation: importlib-metadata 4.8.1\u001b[0m\n",
      "\u001b[35mUninstalling importlib-metadata-4.8.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled importlib-metadata-4.8.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: MarkupSafe\u001b[0m\n",
      "\u001b[35mFound existing installation: MarkupSafe 2.0.1\u001b[0m\n",
      "\u001b[35mUninstalling MarkupSafe-2.0.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled MarkupSafe-2.0.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: Werkzeug\u001b[0m\n",
      "\u001b[35mFound existing installation: Werkzeug 2.0.2\u001b[0m\n",
      "\u001b[35mUninstalling Werkzeug-2.0.2:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled Werkzeug-2.0.2\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing-extensions-3.10.0.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: importlib-metadata\u001b[0m\n",
      "\u001b[34mFound existing installation: importlib-metadata 4.8.1\u001b[0m\n",
      "\u001b[34mUninstalling importlib-metadata-4.8.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled importlib-metadata-4.8.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: MarkupSafe\u001b[0m\n",
      "\u001b[34mFound existing installation: MarkupSafe 2.0.1\u001b[0m\n",
      "\u001b[34mUninstalling MarkupSafe-2.0.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled MarkupSafe-2.0.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: Werkzeug\u001b[0m\n",
      "\u001b[34mFound existing installation: Werkzeug 2.0.2\u001b[0m\n",
      "\u001b[34mUninstalling Werkzeug-2.0.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled Werkzeug-2.0.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: Jinja2\u001b[0m\n",
      "\u001b[34mFound existing installation: Jinja2 3.0.2\u001b[0m\n",
      "\u001b[34mUninstalling Jinja2-3.0.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled Jinja2-3.0.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: click\u001b[0m\n",
      "\u001b[34mFound existing installation: click 8.0.3\u001b[0m\n",
      "\u001b[34mUninstalling click-8.0.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled click-8.0.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: botocore\u001b[0m\n",
      "\u001b[34mFound existing installation: botocore 1.21.60\u001b[0m\n",
      "\u001b[34mUninstalling botocore-1.21.60:\u001b[0m\n",
      "\u001b[32mAttempting uninstall: boto3\u001b[0m\n",
      "\u001b[32mFound existing installation: boto3 1.18.60\u001b[0m\n",
      "\u001b[32mUninstalling boto3-1.18.60:\u001b[0m\n",
      "\u001b[32mSuccessfully uninstalled boto3-1.18.60\u001b[0m\n",
      "\u001b[32mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[32mawscli 1.20.60 requires botocore==1.21.60, but you have botocore 1.35.49 which is incompatible.\u001b[0m\n",
      "\u001b[32mawscli 1.20.60 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.10.3 which is incompatible.\u001b[0m\n",
      "\u001b[32mSuccessfully installed Flask-3.0.3 HeapDict-1.0.1 Jinja2-3.1.4 Mako-1.3.6 MarkupSafe-2.1.5 Werkzeug-3.0.6 alembic-1.13.3 blinker-1.8.2 boto3-1.35.49 botocore-1.35.49 cachetools-5.5.0 click-8.1.7 dask-2022.12.1 deprecated-1.2.14 distributed-2022.12.1 docker-7.1.0 entrypoints-0.4 gitdb-4.0.11 gitpython-3.1.43 graphene-3.4 graphql-core-3.2.5 graphql-relay-3.2.0 graphviz-0.17 gunicorn-22.0.0 importlib-metadata-7.2.1 importlib-resources-6.4.5 itsdangerous-2.2.0 lightgbm-3.3.3 locket-1.0.0 markdown-3.7 mlflow-2.13.2 msgpack-1.0.4 opentelemetry-api-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 partd-1.3.0 querystring-parser-1.2.4 s3transfer-0.10.3 sagemaker-jumpstart-prepack-script-utilities-1.0.0 sagemaker-jumpstart-tabular-script-utilities-1.0.0 sagemaker-mlflow-0.1.0 smmap-5.0.1 sortedcontainers-2.4.0 sqlalchemy-2.0.36 sqlparse-0.5.1 tblib-1.7.0 toolz-0.12.0 typing-extensions-4.12.2 wrapt-1.16.0 zict-2.2.0\u001b[0m\n",
      "\u001b[32mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[36mAttempting uninstall: boto3\u001b[0m\n",
      "\u001b[36mFound existing installation: boto3 1.18.60\u001b[0m\n",
      "\u001b[36mUninstalling boto3-1.18.60:\u001b[0m\n",
      "\u001b[36mSuccessfully uninstalled boto3-1.18.60\u001b[0m\n",
      "\u001b[36mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[36mawscli 1.20.60 requires botocore==1.21.60, but you have botocore 1.35.49 which is incompatible.\u001b[0m\n",
      "\u001b[36mawscli 1.20.60 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.10.3 which is incompatible.\u001b[0m\n",
      "\u001b[36mSuccessfully installed Flask-3.0.3 HeapDict-1.0.1 Jinja2-3.1.4 Mako-1.3.6 MarkupSafe-2.1.5 Werkzeug-3.0.6 alembic-1.13.3 blinker-1.8.2 boto3-1.35.49 botocore-1.35.49 cachetools-5.5.0 click-8.1.7 dask-2022.12.1 deprecated-1.2.14 distributed-2022.12.1 docker-7.1.0 entrypoints-0.4 gitdb-4.0.11 gitpython-3.1.43 graphene-3.4 graphql-core-3.2.5 graphql-relay-3.2.0 graphviz-0.17 gunicorn-22.0.0 importlib-metadata-7.2.1 importlib-resources-6.4.5 itsdangerous-2.2.0 lightgbm-3.3.3 locket-1.0.0 markdown-3.7 mlflow-2.13.2 msgpack-1.0.4 opentelemetry-api-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 partd-1.3.0 querystring-parser-1.2.4 s3transfer-0.10.3 sagemaker-jumpstart-prepack-script-utilities-1.0.0 sagemaker-jumpstart-tabular-script-utilities-1.0.0 sagemaker-mlflow-0.1.0 smmap-5.0.1 sortedcontainers-2.4.0 sqlalchemy-2.0.36 sqlparse-0.5.1 tblib-1.7.0 toolz-0.12.0 typing-extensions-4.12.2 wrapt-1.16.0 zict-2.2.0\u001b[0m\n",
      "\u001b[36mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35mAttempting uninstall: Jinja2\u001b[0m\n",
      "\u001b[35mFound existing installation: Jinja2 3.0.2\u001b[0m\n",
      "\u001b[35mUninstalling Jinja2-3.0.2:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled Jinja2-3.0.2\u001b[0m\n",
      "\u001b[35mAttempting uninstall: click\u001b[0m\n",
      "\u001b[35mFound existing installation: click 8.0.3\u001b[0m\n",
      "\u001b[35mUninstalling click-8.0.3:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled click-8.0.3\u001b[0m\n",
      "\u001b[35mAttempting uninstall: botocore\u001b[0m\n",
      "\u001b[35mFound existing installation: botocore 1.21.60\u001b[0m\n",
      "\u001b[35mUninstalling botocore-1.21.60:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled botocore-1.21.60\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled botocore-1.21.60\u001b[0m\n",
      "\u001b[34mAttempting uninstall: s3transfer\u001b[0m\n",
      "\u001b[34mFound existing installation: s3transfer 0.5.0\u001b[0m\n",
      "\u001b[34mUninstalling s3transfer-0.5.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled s3transfer-0.5.0\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:05,819 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:05,833 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:05,844 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:05,853 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32mTraining Env:\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-3\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bagging_fraction\": \"0.53\",\n",
      "        \"bagging_freq\": \"5\",\n",
      "        \"boosting\": \"gbdt\",\n",
      "        \"feature_fraction\": \"0.74\",\n",
      "        \"feature_fraction_bynode\": \"1.0\",\n",
      "        \"is_unbalance\": \"False\",\n",
      "        \"lambda_l1\": \"0.0\",\n",
      "        \"lambda_l2\": \"0.0\",\n",
      "        \"learning_rate\": \"0.009\",\n",
      "        \"max_bin\": \"255\",\n",
      "        \"max_delta_step\": \"0.0\",\n",
      "        \"max_depth\": \"11\",\n",
      "        \"metric\": \"auc\",\n",
      "        \"min_data_in_leaf\": \"26\",\n",
      "        \"min_gain_to_split\": \"0.0\",\n",
      "        \"num_boost_round\": \"200\",\n",
      "        \"num_leaves\": \"67\",\n",
      "        \"num_threads\": \"0\",\n",
      "        \"scale_pos_weight\": \"1.0\",\n",
      "        \"tree_learner\": \"voting\",\n",
      "        \"use_dask\": \"False\",\n",
      "        \"verbosity\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"sagemaker-jumpstart-2024-10-27-04-30-43-804\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-3\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32mEnvironment variables:\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auc\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"200\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"voting\",\"use_dask\":\"False\",\"verbosity\":\"1\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-3\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-1\",\"algo-2\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[\"model\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-3\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-3\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"hyperparameters\":{\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auc\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"200\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"voting\",\"use_dask\":\"False\",\"verbosity\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"sagemaker-jumpstart-2024-10-27-04-30-43-804\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-3\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-1\",\"algo-2\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--bagging_fraction\",\"0.53\",\"--bagging_freq\",\"5\",\"--boosting\",\"gbdt\",\"--feature_fraction\",\"0.74\",\"--feature_fraction_bynode\",\"1.0\",\"--is_unbalance\",\"False\",\"--lambda_l1\",\"0.0\",\"--lambda_l2\",\"0.0\",\"--learning_rate\",\"0.009\",\"--max_bin\",\"255\",\"--max_delta_step\",\"0.0\",\"--max_depth\",\"11\",\"--metric\",\"auc\",\"--min_data_in_leaf\",\"26\",\"--min_gain_to_split\",\"0.0\",\"--num_boost_round\",\"200\",\"--num_leaves\",\"67\",\"--num_threads\",\"0\",\"--scale_pos_weight\",\"1.0\",\"--tree_learner\",\"voting\",\"--use_dask\",\"False\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[32mSM_HP_BAGGING_FRACTION=0.53\u001b[0m\n",
      "\u001b[32mSM_HP_BAGGING_FREQ=5\u001b[0m\n",
      "\u001b[32mSM_HP_BOOSTING=gbdt\u001b[0m\n",
      "\u001b[32mSM_HP_FEATURE_FRACTION=0.74\u001b[0m\n",
      "\u001b[32mSM_HP_FEATURE_FRACTION_BYNODE=1.0\u001b[0m\n",
      "\u001b[32mSM_HP_IS_UNBALANCE=False\u001b[0m\n",
      "\u001b[32mSM_HP_LAMBDA_L1=0.0\u001b[0m\n",
      "\u001b[32mSM_HP_LAMBDA_L2=0.0\u001b[0m\n",
      "\u001b[32mSM_HP_LEARNING_RATE=0.009\u001b[0m\n",
      "\u001b[32mSM_HP_MAX_BIN=255\u001b[0m\n",
      "\u001b[32mSM_HP_MAX_DELTA_STEP=0.0\u001b[0m\n",
      "\u001b[32mSM_HP_MAX_DEPTH=11\u001b[0m\n",
      "\u001b[32mSM_HP_METRIC=auc\u001b[0m\n",
      "\u001b[32mSM_HP_MIN_DATA_IN_LEAF=26\u001b[0m\n",
      "\u001b[32mSM_HP_MIN_GAIN_TO_SPLIT=0.0\u001b[0m\n",
      "\u001b[32mSM_HP_NUM_BOOST_ROUND=200\u001b[0m\n",
      "\u001b[32mSM_HP_NUM_LEAVES=67\u001b[0m\n",
      "\u001b[32mSM_HP_NUM_THREADS=0\u001b[0m\n",
      "\u001b[32mSM_HP_SCALE_POS_WEIGHT=1.0\u001b[0m\n",
      "\u001b[32mSM_HP_TREE_LEARNER=voting\u001b[0m\n",
      "\u001b[32mSM_HP_USE_DASK=False\u001b[0m\n",
      "\u001b[32mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.8 train.py --bagging_fraction 0.53 --bagging_freq 5 --boosting gbdt --feature_fraction 0.74 --feature_fraction_bynode 1.0 --is_unbalance False --lambda_l1 0.0 --lambda_l2 0.0 --learning_rate 0.009 --max_bin 255 --max_delta_step 0.0 --max_depth 11 --metric auc --min_data_in_leaf 26 --min_gain_to_split 0.0 --num_boost_round 200 --num_leaves 67 --num_threads 0 --scale_pos_weight 1.0 --tree_learner voting --use_dask False --verbosity 1\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:06,008 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:06,022 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:06,033 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:06,042 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[36mTraining Env:\u001b[0m\n",
      "\u001b[36m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-4\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bagging_fraction\": \"0.53\",\n",
      "        \"bagging_freq\": \"5\",\n",
      "        \"boosting\": \"gbdt\",\n",
      "        \"feature_fraction\": \"0.74\",\n",
      "        \"feature_fraction_bynode\": \"1.0\",\n",
      "        \"is_unbalance\": \"False\",\n",
      "        \"lambda_l1\": \"0.0\",\n",
      "        \"lambda_l2\": \"0.0\",\n",
      "        \"learning_rate\": \"0.009\",\n",
      "        \"max_bin\": \"255\",\n",
      "        \"max_delta_step\": \"0.0\",\n",
      "        \"max_depth\": \"11\",\n",
      "        \"metric\": \"auc\",\n",
      "        \"min_data_in_leaf\": \"26\",\n",
      "        \"min_gain_to_split\": \"0.0\",\n",
      "        \"num_boost_round\": \"200\",\n",
      "        \"num_leaves\": \"67\",\n",
      "        \"num_threads\": \"0\",\n",
      "        \"scale_pos_weight\": \"1.0\",\n",
      "        \"tree_learner\": \"voting\",\n",
      "        \"use_dask\": \"False\",\n",
      "        \"verbosity\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"sagemaker-jumpstart-2024-10-27-04-30-43-804\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-4\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[36m}\u001b[0m\n",
      "\u001b[36mEnvironment variables:\u001b[0m\n",
      "\u001b[36mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[36mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[36mSM_HPS={\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auc\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"200\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"voting\",\"use_dask\":\"False\",\"verbosity\":\"1\"}\u001b[0m\n",
      "\u001b[36mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[36mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[36mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-4\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-1\",\"algo-2\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[36mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[36mSM_CHANNELS=[\"model\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[36mSM_CURRENT_HOST=algo-4\u001b[0m\n",
      "\u001b[36mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[36mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[36mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[36mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[36mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[36mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[36mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[36mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[36mSM_MODULE_DIR=s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[36mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-4\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"hyperparameters\":{\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auc\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"200\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"voting\",\"use_dask\":\"False\",\"verbosity\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"sagemaker-jumpstart-2024-10-27-04-30-43-804\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-4\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-1\",\"algo-2\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[36mSM_USER_ARGS=[\"--bagging_fraction\",\"0.53\",\"--bagging_freq\",\"5\",\"--boosting\",\"gbdt\",\"--feature_fraction\",\"0.74\",\"--feature_fraction_bynode\",\"1.0\",\"--is_unbalance\",\"False\",\"--lambda_l1\",\"0.0\",\"--lambda_l2\",\"0.0\",\"--learning_rate\",\"0.009\",\"--max_bin\",\"255\",\"--max_delta_step\",\"0.0\",\"--max_depth\",\"11\",\"--metric\",\"auc\",\"--min_data_in_leaf\",\"26\",\"--min_gain_to_split\",\"0.0\",\"--num_boost_round\",\"200\",\"--num_leaves\",\"67\",\"--num_threads\",\"0\",\"--scale_pos_weight\",\"1.0\",\"--tree_learner\",\"voting\",\"--use_dask\",\"False\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[36mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[36mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[36mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[36mSM_HP_BAGGING_FRACTION=0.53\u001b[0m\n",
      "\u001b[36mSM_HP_BAGGING_FREQ=5\u001b[0m\n",
      "\u001b[36mSM_HP_BOOSTING=gbdt\u001b[0m\n",
      "\u001b[36mSM_HP_FEATURE_FRACTION=0.74\u001b[0m\n",
      "\u001b[36mSM_HP_FEATURE_FRACTION_BYNODE=1.0\u001b[0m\n",
      "\u001b[36mSM_HP_IS_UNBALANCE=False\u001b[0m\n",
      "\u001b[36mSM_HP_LAMBDA_L1=0.0\u001b[0m\n",
      "\u001b[36mSM_HP_LAMBDA_L2=0.0\u001b[0m\n",
      "\u001b[36mSM_HP_LEARNING_RATE=0.009\u001b[0m\n",
      "\u001b[36mSM_HP_MAX_BIN=255\u001b[0m\n",
      "\u001b[36mSM_HP_MAX_DELTA_STEP=0.0\u001b[0m\n",
      "\u001b[36mSM_HP_MAX_DEPTH=11\u001b[0m\n",
      "\u001b[36mSM_HP_METRIC=auc\u001b[0m\n",
      "\u001b[36mSM_HP_MIN_DATA_IN_LEAF=26\u001b[0m\n",
      "\u001b[36mSM_HP_MIN_GAIN_TO_SPLIT=0.0\u001b[0m\n",
      "\u001b[36mSM_HP_NUM_BOOST_ROUND=200\u001b[0m\n",
      "\u001b[36mSM_HP_NUM_LEAVES=67\u001b[0m\n",
      "\u001b[36mSM_HP_NUM_THREADS=0\u001b[0m\n",
      "\u001b[36mSM_HP_SCALE_POS_WEIGHT=1.0\u001b[0m\n",
      "\u001b[36mSM_HP_TREE_LEARNER=voting\u001b[0m\n",
      "\u001b[36mSM_HP_USE_DASK=False\u001b[0m\n",
      "\u001b[36mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[36mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[36mInvoking script with the following command:\u001b[0m\n",
      "\u001b[36m/opt/conda/bin/python3.8 train.py --bagging_fraction 0.53 --bagging_freq 5 --boosting gbdt --feature_fraction 0.74 --feature_fraction_bynode 1.0 --is_unbalance False --lambda_l1 0.0 --lambda_l2 0.0 --learning_rate 0.009 --max_bin 255 --max_delta_step 0.0 --max_depth 11 --metric auc --min_data_in_leaf 26 --min_gain_to_split 0.0 --num_boost_round 200 --num_leaves 67 --num_threads 0 --scale_pos_weight 1.0 --tree_learner voting --use_dask False --verbosity 1\u001b[0m\n",
      "\u001b[32mINFO:matplotlib.font_manager:generated new fontManager\u001b[0m\n",
      "\u001b[32mINFO:root:Initializing a Dask cluster\u001b[0m\n",
      "\u001b[32mINFO:root:Start Dask cluster in all nodes\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.8/site-packages/distributed/cli/dask_worker.py:266: FutureWarning: dask-worker is deprecated and will be removed in a future release; use `dask worker` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mAttempting uninstall: boto3\u001b[0m\n",
      "\u001b[34mFound existing installation: boto3 1.18.60\u001b[0m\n",
      "\u001b[34mUninstalling boto3-1.18.60:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled boto3-1.18.60\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mawscli 1.20.60 requires botocore==1.21.60, but you have botocore 1.35.49 which is incompatible.\u001b[0m\n",
      "\u001b[34mawscli 1.20.60 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.10.3 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Flask-3.0.3 HeapDict-1.0.1 Jinja2-3.1.4 Mako-1.3.6 MarkupSafe-2.1.5 Werkzeug-3.0.6 alembic-1.13.3 blinker-1.8.2 boto3-1.35.49 botocore-1.35.49 cachetools-5.5.0 click-8.1.7 dask-2022.12.1 deprecated-1.2.14 distributed-2022.12.1 docker-7.1.0 entrypoints-0.4 gitdb-4.0.11 gitpython-3.1.43 graphene-3.4 graphql-core-3.2.5 graphql-relay-3.2.0 graphviz-0.17 gunicorn-22.0.0 importlib-metadata-7.2.1 importlib-resources-6.4.5 itsdangerous-2.2.0 lightgbm-3.3.3 locket-1.0.0 markdown-3.7 mlflow-2.13.2 msgpack-1.0.4 opentelemetry-api-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 partd-1.3.0 querystring-parser-1.2.4 s3transfer-0.10.3 sagemaker-jumpstart-prepack-script-utilities-1.0.0 sagemaker-jumpstart-tabular-script-utilities-1.0.0 sagemaker-mlflow-0.1.0 smmap-5.0.1 sortedcontainers-2.4.0 sqlalchemy-2.0.36 sqlparse-0.5.1 tblib-1.7.0 toolz-0.12.0 typing-extensions-4.12.2 wrapt-1.16.0 zict-2.2.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:10,454 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:10,468 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:10,479 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:10,488 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bagging_fraction\": \"0.53\",\n",
      "        \"bagging_freq\": \"5\",\n",
      "        \"boosting\": \"gbdt\",\n",
      "        \"feature_fraction\": \"0.74\",\n",
      "        \"feature_fraction_bynode\": \"1.0\",\n",
      "        \"is_unbalance\": \"False\",\n",
      "        \"lambda_l1\": \"0.0\",\n",
      "        \"lambda_l2\": \"0.0\",\n",
      "        \"learning_rate\": \"0.009\",\n",
      "        \"max_bin\": \"255\",\n",
      "        \"max_delta_step\": \"0.0\",\n",
      "        \"max_depth\": \"11\",\n",
      "        \"metric\": \"auc\",\n",
      "        \"min_data_in_leaf\": \"26\",\n",
      "        \"min_gain_to_split\": \"0.0\",\n",
      "        \"num_boost_round\": \"200\",\n",
      "        \"num_leaves\": \"67\",\n",
      "        \"num_threads\": \"0\",\n",
      "        \"scale_pos_weight\": \"1.0\",\n",
      "        \"tree_learner\": \"voting\",\n",
      "        \"use_dask\": \"False\",\n",
      "        \"verbosity\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-jumpstart-2024-10-27-04-30-43-804\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auc\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"200\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"voting\",\"use_dask\":\"False\",\"verbosity\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-1\",\"algo-2\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"hyperparameters\":{\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auc\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"200\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"voting\",\"use_dask\":\"False\",\"verbosity\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-jumpstart-2024-10-27-04-30-43-804\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-1\",\"algo-2\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bagging_fraction\",\"0.53\",\"--bagging_freq\",\"5\",\"--boosting\",\"gbdt\",\"--feature_fraction\",\"0.74\",\"--feature_fraction_bynode\",\"1.0\",\"--is_unbalance\",\"False\",\"--lambda_l1\",\"0.0\",\"--lambda_l2\",\"0.0\",\"--learning_rate\",\"0.009\",\"--max_bin\",\"255\",\"--max_delta_step\",\"0.0\",\"--max_depth\",\"11\",\"--metric\",\"auc\",\"--min_data_in_leaf\",\"26\",\"--min_gain_to_split\",\"0.0\",\"--num_boost_round\",\"200\",\"--num_leaves\",\"67\",\"--num_threads\",\"0\",\"--scale_pos_weight\",\"1.0\",\"--tree_learner\",\"voting\",\"--use_dask\",\"False\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_BAGGING_FRACTION=0.53\u001b[0m\n",
      "\u001b[34mSM_HP_BAGGING_FREQ=5\u001b[0m\n",
      "\u001b[34mSM_HP_BOOSTING=gbdt\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_FRACTION=0.74\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_FRACTION_BYNODE=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_IS_UNBALANCE=False\u001b[0m\n",
      "\u001b[34mSM_HP_LAMBDA_L1=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_LAMBDA_L2=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.009\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_BIN=255\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DELTA_STEP=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=11\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC=auc\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_DATA_IN_LEAF=26\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_GAIN_TO_SPLIT=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_BOOST_ROUND=200\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LEAVES=67\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_THREADS=0\u001b[0m\n",
      "\u001b[34mSM_HP_SCALE_POS_WEIGHT=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_TREE_LEARNER=voting\u001b[0m\n",
      "\u001b[34mSM_HP_USE_DASK=False\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --bagging_fraction 0.53 --bagging_freq 5 --boosting gbdt --feature_fraction 0.74 --feature_fraction_bynode 1.0 --is_unbalance False --lambda_l1 0.0 --lambda_l2 0.0 --learning_rate 0.009 --max_bin 255 --max_delta_step 0.0 --max_depth 11 --metric auc --min_data_in_leaf 26 --min_gain_to_split 0.0 --num_boost_round 200 --num_leaves 67 --num_threads 0 --scale_pos_weight 1.0 --tree_learner voting --use_dask False --verbosity 1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: s3transfer\u001b[0m\n",
      "\u001b[35mFound existing installation: s3transfer 0.5.0\u001b[0m\n",
      "\u001b[35mUninstalling s3transfer-0.5.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled s3transfer-0.5.0\u001b[0m\n",
      "\u001b[36mINFO:matplotlib.font_manager:generated new fontManager\u001b[0m\n",
      "\u001b[36mINFO:root:Initializing a Dask cluster\u001b[0m\n",
      "\u001b[36mINFO:root:Start Dask cluster in all nodes\u001b[0m\n",
      "\u001b[36m/opt/conda/lib/python3.8/site-packages/distributed/cli/dask_worker.py:266: FutureWarning: dask-worker is deprecated and will be removed in a future release; use `dask worker` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mAttempting uninstall: boto3\u001b[0m\n",
      "\u001b[35mFound existing installation: boto3 1.18.60\u001b[0m\n",
      "\u001b[35mUninstalling boto3-1.18.60:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled boto3-1.18.60\u001b[0m\n",
      "\u001b[32mINFO:root:Received a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[36mINFO:root:Received a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[35mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[35mawscli 1.20.60 requires botocore==1.21.60, but you have botocore 1.35.49 which is incompatible.\u001b[0m\n",
      "\u001b[35mawscli 1.20.60 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.10.3 which is incompatible.\u001b[0m\n",
      "\u001b[35mSuccessfully installed Flask-3.0.3 HeapDict-1.0.1 Jinja2-3.1.4 Mako-1.3.6 MarkupSafe-2.1.5 Werkzeug-3.0.6 alembic-1.13.3 blinker-1.8.2 boto3-1.35.49 botocore-1.35.49 cachetools-5.5.0 click-8.1.7 dask-2022.12.1 deprecated-1.2.14 distributed-2022.12.1 docker-7.1.0 entrypoints-0.4 gitdb-4.0.11 gitpython-3.1.43 graphene-3.4 graphql-core-3.2.5 graphql-relay-3.2.0 graphviz-0.17 gunicorn-22.0.0 importlib-metadata-7.2.1 importlib-resources-6.4.5 itsdangerous-2.2.0 lightgbm-3.3.3 locket-1.0.0 markdown-3.7 mlflow-2.13.2 msgpack-1.0.4 opentelemetry-api-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 partd-1.3.0 querystring-parser-1.2.4 s3transfer-0.10.3 sagemaker-jumpstart-prepack-script-utilities-1.0.0 sagemaker-jumpstart-tabular-script-utilities-1.0.0 sagemaker-mlflow-0.1.0 smmap-5.0.1 sortedcontainers-2.4.0 sqlalchemy-2.0.36 sqlparse-0.5.1 tblib-1.7.0 toolz-0.12.0 typing-extensions-4.12.2 wrapt-1.16.0 zict-2.2.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:10,645 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:10,659 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:10,671 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:10,680 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bagging_fraction\": \"0.53\",\n",
      "        \"bagging_freq\": \"5\",\n",
      "        \"boosting\": \"gbdt\",\n",
      "        \"feature_fraction\": \"0.74\",\n",
      "        \"feature_fraction_bynode\": \"1.0\",\n",
      "        \"is_unbalance\": \"False\",\n",
      "        \"lambda_l1\": \"0.0\",\n",
      "        \"lambda_l2\": \"0.0\",\n",
      "        \"learning_rate\": \"0.009\",\n",
      "        \"max_bin\": \"255\",\n",
      "        \"max_delta_step\": \"0.0\",\n",
      "        \"max_depth\": \"11\",\n",
      "        \"metric\": \"auc\",\n",
      "        \"min_data_in_leaf\": \"26\",\n",
      "        \"min_gain_to_split\": \"0.0\",\n",
      "        \"num_boost_round\": \"200\",\n",
      "        \"num_leaves\": \"67\",\n",
      "        \"num_threads\": \"0\",\n",
      "        \"scale_pos_weight\": \"1.0\",\n",
      "        \"tree_learner\": \"voting\",\n",
      "        \"use_dask\": \"False\",\n",
      "        \"verbosity\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"sagemaker-jumpstart-2024-10-27-04-30-43-804\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auc\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"200\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"voting\",\"use_dask\":\"False\",\"verbosity\":\"1\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-1\",\"algo-2\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"model\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"hyperparameters\":{\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auc\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"200\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"voting\",\"use_dask\":\"False\",\"verbosity\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"sagemaker-jumpstart-2024-10-27-04-30-43-804\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-04-30-43-804/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"instance_groups\":[{\"hosts\":[\"algo-3\",\"algo-1\",\"algo-2\",\"algo-4\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--bagging_fraction\",\"0.53\",\"--bagging_freq\",\"5\",\"--boosting\",\"gbdt\",\"--feature_fraction\",\"0.74\",\"--feature_fraction_bynode\",\"1.0\",\"--is_unbalance\",\"False\",\"--lambda_l1\",\"0.0\",\"--lambda_l2\",\"0.0\",\"--learning_rate\",\"0.009\",\"--max_bin\",\"255\",\"--max_delta_step\",\"0.0\",\"--max_depth\",\"11\",\"--metric\",\"auc\",\"--min_data_in_leaf\",\"26\",\"--min_gain_to_split\",\"0.0\",\"--num_boost_round\",\"200\",\"--num_leaves\",\"67\",\"--num_threads\",\"0\",\"--scale_pos_weight\",\"1.0\",\"--tree_learner\",\"voting\",\"--use_dask\",\"False\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[35mSM_HP_BAGGING_FRACTION=0.53\u001b[0m\n",
      "\u001b[35mSM_HP_BAGGING_FREQ=5\u001b[0m\n",
      "\u001b[35mSM_HP_BOOSTING=gbdt\u001b[0m\n",
      "\u001b[35mSM_HP_FEATURE_FRACTION=0.74\u001b[0m\n",
      "\u001b[35mSM_HP_FEATURE_FRACTION_BYNODE=1.0\u001b[0m\n",
      "\u001b[35mSM_HP_IS_UNBALANCE=False\u001b[0m\n",
      "\u001b[35mSM_HP_LAMBDA_L1=0.0\u001b[0m\n",
      "\u001b[35mSM_HP_LAMBDA_L2=0.0\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING_RATE=0.009\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_BIN=255\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_DELTA_STEP=0.0\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_DEPTH=11\u001b[0m\n",
      "\u001b[35mSM_HP_METRIC=auc\u001b[0m\n",
      "\u001b[35mSM_HP_MIN_DATA_IN_LEAF=26\u001b[0m\n",
      "\u001b[35mSM_HP_MIN_GAIN_TO_SPLIT=0.0\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_BOOST_ROUND=200\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_LEAVES=67\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_THREADS=0\u001b[0m\n",
      "\u001b[35mSM_HP_SCALE_POS_WEIGHT=1.0\u001b[0m\n",
      "\u001b[35mSM_HP_TREE_LEARNER=voting\u001b[0m\n",
      "\u001b[35mSM_HP_USE_DASK=False\u001b[0m\n",
      "\u001b[35mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 train.py --bagging_fraction 0.53 --bagging_freq 5 --boosting gbdt --feature_fraction 0.74 --feature_fraction_bynode 1.0 --is_unbalance False --lambda_l1 0.0 --lambda_l2 0.0 --learning_rate 0.009 --max_bin 255 --max_delta_step 0.0 --max_depth 11 --metric auc --min_data_in_leaf 26 --min_gain_to_split 0.0 --num_boost_round 200 --num_leaves 67 --num_threads 0 --scale_pos_weight 1.0 --tree_learner voting --use_dask False --verbosity 1\u001b[0m\n",
      "\u001b[35mINFO:matplotlib.font_manager:generated new fontManager\u001b[0m\n",
      "\u001b[34mINFO:matplotlib.font_manager:generated new fontManager\u001b[0m\n",
      "\u001b[34mINFO:root:Initializing a Dask cluster\u001b[0m\n",
      "\u001b[34mINFO:root:Start Dask cluster in all nodes\u001b[0m\n",
      "\u001b[35mINFO:root:Initializing a Dask cluster\u001b[0m\n",
      "\u001b[35mINFO:root:Start Dask cluster in all nodes\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/distributed/cli/dask_worker.py:266: FutureWarning: dask-worker is deprecated and will be removed in a future release; use `dask worker` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/distributed/cli/dask_worker.py:266: FutureWarning: dask-worker is deprecated and will be removed in a future release; use `dask worker` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:13,004 - distributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/distributed/dashboard/core.py:20: UserWarning: \u001b[0m\n",
      "\u001b[34mDask needs bokeh >= 2.4.2, < 3 for the dashboard.\u001b[0m\n",
      "\u001b[34mYou have bokeh==2.4.0.\u001b[0m\n",
      "\u001b[34mContinuing without the dashboard.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:13,366 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:13,369 - distributed.scheduler - INFO - State start\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:13,371 - distributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:13,372 - distributed.scheduler - INFO -   Scheduler at:      tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:13,372 - distributed.scheduler - INFO -   dashboard at:                     :8787\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:13,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.2.84.151:33667'\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/distributed/dashboard/core.py:20: UserWarning: \u001b[0m\n",
      "\u001b[35mDask needs bokeh >= 2.4.2, < 3 for the dashboard.\u001b[0m\n",
      "\u001b[35mYou have bokeh==2.4.0.\u001b[0m\n",
      "\u001b[35mContinuing without the dashboard.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,232 - distributed.worker - INFO -       Start worker at:     tcp://10.2.84.151:9000\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,232 - distributed.worker - INFO -          Listening to:     tcp://10.2.84.151:9000\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,232 - distributed.worker - INFO -          dashboard at:          10.2.84.151:33263\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,232 - distributed.worker - INFO - Waiting to connect to:       tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,232 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,232 - distributed.worker - INFO -               Threads:                          8\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,232 - distributed.worker - INFO -                Memory:                  28.32 GiB\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,232 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-em28vw5k\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,232 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,240 - distributed.worker - INFO -         Registered to:       tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,240 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35m2024-10-27 04:33:14,241 - distributed.core - INFO - Starting established connection to tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:13,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.2.82.8:36973'\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,237 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.2.84.151:9000', status: init, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,240 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.2.84.151:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,240 - distributed.core - INFO - Starting established connection to tcp://10.2.84.151:38700\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,556 - distributed.scheduler - INFO - Receive client connection: Client-94a2e8ef-941c-11ef-8022-023318eb7faa\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,556 - distributed.core - INFO - Starting established connection to tcp://10.2.82.8:34114\u001b[0m\n",
      "\u001b[34mINFO:root:Client summary: <Client: 'tcp://10.2.82.8:8786' processes=1 threads=8, memory=28.32 GiB>.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading data\u001b[0m\n",
      "\u001b[34mINFO:root:'ContentType' is not identified in either training or validation data channel. Default ContentType 'text/csv' is used to read the train and validation data.\u001b[0m\n",
      "\u001b[34mINFO:root:Found data in the validation channel. Reading the train and validation data from the training and validation channel, respectively.\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:14,271 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.2.96.89:33217'\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:14,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.2.91.117:40795'\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/distributed/dashboard/core.py:20: UserWarning: \u001b[0m\n",
      "\u001b[34mDask needs bokeh >= 2.4.2, < 3 for the dashboard.\u001b[0m\n",
      "\u001b[34mYou have bokeh==2.4.0.\u001b[0m\n",
      "\u001b[34mContinuing without the dashboard.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,746 - distributed.worker - INFO -       Start worker at:       tcp://10.2.82.8:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,746 - distributed.worker - INFO -          Listening to:       tcp://10.2.82.8:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,746 - distributed.worker - INFO -          dashboard at:            10.2.82.8:35551\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,746 - distributed.worker - INFO - Waiting to connect to:       tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,746 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,746 - distributed.worker - INFO -               Threads:                          8\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,746 - distributed.worker - INFO -                Memory:                  28.32 GiB\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,746 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-juiwxlhd\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,746 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,751 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.2.82.8:9000', status: init, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,751 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.2.82.8:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,752 - distributed.core - INFO - Starting established connection to tcp://10.2.82.8:34140\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,752 - distributed.worker - INFO -         Registered to:       tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,752 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,753 - distributed.core - INFO - Starting established connection to tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:14,774 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:38706 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:15,107 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.2.96.89:9000', status: init, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:15,108 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.2.96.89:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:15,108 - distributed.core - INFO - Starting established connection to tcp://10.2.96.89:55634\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:15,552 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.2.91.117:9000', status: init, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:15,553 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.2.91.117:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:15,553 - distributed.core - INFO - Starting established connection to tcp://10.2.91.117:57158\u001b[0m\n",
      "\u001b[36m/opt/conda/lib/python3.8/site-packages/distributed/dashboard/core.py:20: UserWarning: \u001b[0m\n",
      "\u001b[36mDask needs bokeh >= 2.4.2, < 3 for the dashboard.\u001b[0m\n",
      "\u001b[36mYou have bokeh==2.4.0.\u001b[0m\n",
      "\u001b[36mContinuing without the dashboard.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,546 - distributed.worker - INFO -       Start worker at:     tcp://10.2.91.117:9000\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,547 - distributed.worker - INFO -          Listening to:     tcp://10.2.91.117:9000\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,547 - distributed.worker - INFO -          dashboard at:          10.2.91.117:33183\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,547 - distributed.worker - INFO - Waiting to connect to:       tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,547 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,547 - distributed.worker - INFO -               Threads:                          8\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,547 - distributed.worker - INFO -                Memory:                  28.32 GiB\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1z22ucb9\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,547 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,553 - distributed.worker - INFO -         Registered to:       tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,553 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36m2024-10-27 04:33:15,554 - distributed.core - INFO - Starting established connection to tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.8/site-packages/distributed/dashboard/core.py:20: UserWarning: \u001b[0m\n",
      "\u001b[32mDask needs bokeh >= 2.4.2, < 3 for the dashboard.\u001b[0m\n",
      "\u001b[32mYou have bokeh==2.4.0.\u001b[0m\n",
      "\u001b[32mContinuing without the dashboard.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,101 - distributed.worker - INFO -       Start worker at:      tcp://10.2.96.89:9000\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,101 - distributed.worker - INFO -          Listening to:      tcp://10.2.96.89:9000\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,101 - distributed.worker - INFO -          dashboard at:           10.2.96.89:38245\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,102 - distributed.worker - INFO - Waiting to connect to:       tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,102 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,102 - distributed.worker - INFO -               Threads:                          8\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,102 - distributed.worker - INFO -                Memory:                  28.30 GiB\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,102 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xu9amimp\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,102 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,108 - distributed.worker - INFO -         Registered to:       tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,108 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32m2024-10-27 04:33:15,109 - distributed.core - INFO - Starting established connection to tcp://10.2.82.8:8786\u001b[0m\n",
      "\u001b[34mINFO:root:'_input_model_extracted/__models_info__.json' file could not be found.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:16,776 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:38726 closed before handshake completed\u001b[0m\n",
      "\u001b[34mWARNING:root:Disable early stopping feature in multi-instance dask training due to an issue in the open sourced LightGBM repository. For details, see https://github.com/microsoft/SynapseML/issues/728#issuecomment-1221599961\u001b[0m\n",
      "\u001b[34mINFO:root:Beginning training\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:525: UserWarning: Parameter n_jobs will be ignored.\n",
      "  _log_warning(f\"Parameter {param_alias} will be ignored.\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:525: UserWarning: Parameter num_threads will be ignored.\n",
      "  _log_warning(f\"Parameter {param_alias} will be ignored.\")\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:18,777 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:52092 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:20,778 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:52106 closed before handshake completed\u001b[0m\n",
      "\u001b[34mFinding random open ports for workers\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Trying to bind port 34513...\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Binding port 34513 succeeded\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Listening...\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Trying to bind port 52925...\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Binding port 52925 succeeded\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Listening...\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] Connecting to rank 2 failed, waiting for 200 milliseconds\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] Connecting to rank 2 failed, waiting for 260 milliseconds\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Connected to rank 1\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Connected to rank 2\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Connected to rank 3\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Local rank: 0, total number of machines: 4\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Trying to bind port 39841...\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Binding port 39841 succeeded\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Listening...\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] Connecting to rank 2 failed, waiting for 200 milliseconds\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Connected to rank 0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Connected to rank 2\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Connected to rank 3\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Local rank: 1, total number of machines: 4\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Trying to bind port 58197...\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Binding port 58197 succeeded\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Listening...\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Connected to rank 0\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Connected to rank 1\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Connected to rank 3\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Local rank: 2, total number of machines: 4\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Connected to rank 0\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Connected to rank 1\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Connected to rank 2\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Local rank: 3, total number of machines: 4\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Number of positive: 2622900, number of negative: 2627100\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021001 seconds.\u001b[0m\n",
      "\u001b[35mYou can set `force_row_wise=true` to remove the overhead.\u001b[0m\n",
      "\u001b[35mAnd if memory is not enough, you can set `force_col_wise=true`.\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Total Bins 2407\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Number of data points in the train set: 843739, number of used features: 19\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:22,781 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:52120 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Number of positive: 2622900, number of negative: 2627100\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148844 seconds.\u001b[0m\n",
      "\u001b[34mYou can set `force_col_wise=true` to remove the overhead.\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Total Bins 2407\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Number of data points in the train set: 1875025, number of used features: 19\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Number of positive: 2622900, number of negative: 2627100\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154566 seconds.\u001b[0m\n",
      "\u001b[32mYou can set `force_col_wise=true` to remove the overhead.\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Total Bins 2407\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Number of data points in the train set: 1968743, number of used features: 19\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Number of positive: 2622900, number of negative: 2627100\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012981 seconds.\u001b[0m\n",
      "\u001b[36mYou can set `force_row_wise=true` to remove the overhead.\u001b[0m\n",
      "\u001b[36mAnd if memory is not enough, you can set `force_col_wise=true`.\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Total Bins 2407\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Number of data points in the train set: 562493, number of used features: 19\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[36m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499600 -> initscore=-0.001600\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[35mStart training from score -0.001600\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499600 -> initscore=-0.001600\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[34mStart training from score -0.001600\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[32m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499600 -> initscore=-0.001600\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Start training from score -0.001600\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499600 -> initscore=-0.001600\u001b[0m\n",
      "\u001b[32m[1]#011train's auc: 0.95541#011train's binary_logloss: 0.686634\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[36mStart training from score -0.001600\u001b[0m\n",
      "\u001b[36m[1]#011train's auc: 0.956215#011train's binary_logloss: 0.686617\u001b[0m\n",
      "\u001b[35m[1]#011val's auc: 0.95563#011val's binary_logloss: 0.686627#011train's auc: 0.955913#011train's binary_logloss: 0.686625\u001b[0m\n",
      "\u001b[35m[2]#011val's auc: 0.967891#011val's binary_logloss: 0.681019#011train's auc: 0.968149#011train's binary_logloss: 0.681012\u001b[0m\n",
      "\u001b[34m[1]#011train's auc: 0.955508#011train's binary_logloss: 0.686629\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:24,781 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:52134 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[2]#011train's auc: 0.967892#011train's binary_logloss: 0.681024\u001b[0m\n",
      "\u001b[34m[3]#011train's auc: 0.975801#011train's binary_logloss: 0.674419\u001b[0m\n",
      "\u001b[36m[2]#011train's auc: 0.968475#011train's binary_logloss: 0.680998\u001b[0m\n",
      "\u001b[36m[3]#011train's auc: 0.976166#011train's binary_logloss: 0.674386\u001b[0m\n",
      "\u001b[36m[4]#011train's auc: 0.980911#011train's binary_logloss: 0.668069\u001b[0m\n",
      "\u001b[32m[2]#011train's auc: 0.96776#011train's binary_logloss: 0.681027\u001b[0m\n",
      "\u001b[32m[3]#011train's auc: 0.975682#011train's binary_logloss: 0.674421\u001b[0m\n",
      "\u001b[35m[3]#011val's auc: 0.975811#011val's binary_logloss: 0.674406#011train's auc: 0.975951#011train's binary_logloss: 0.674403\u001b[0m\n",
      "\u001b[35m[4]#011val's auc: 0.980662#011val's binary_logloss: 0.668093#011train's auc: 0.980791#011train's binary_logloss: 0.668089\u001b[0m\n",
      "\u001b[34m[4]#011train's auc: 0.98066#011train's binary_logloss: 0.668103\u001b[0m\n",
      "\u001b[32m[4]#011train's auc: 0.98057#011train's binary_logloss: 0.668107\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:26,783 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:52148 closed before handshake completed\u001b[0m\n",
      "\u001b[35m[5]#011val's auc: 0.98066#011val's binary_logloss: 0.661673#011train's auc: 0.980806#011train's binary_logloss: 0.661667\u001b[0m\n",
      "\u001b[34m[5]#011train's auc: 0.980673#011train's binary_logloss: 0.661689\u001b[0m\n",
      "\u001b[36m[5]#011train's auc: 0.980924#011train's binary_logloss: 0.661646\u001b[0m\n",
      "\u001b[32m[5]#011train's auc: 0.980588#011train's binary_logloss: 0.661692\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:28,785 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:55210 closed before handshake completed\u001b[0m\n",
      "\u001b[35m[6]#011val's auc: 0.983131#011val's binary_logloss: 0.655284#011train's auc: 0.983334#011train's binary_logloss: 0.655282\u001b[0m\n",
      "\u001b[34m[6]#011train's auc: 0.98317#011train's binary_logloss: 0.655305\u001b[0m\n",
      "\u001b[36m[6]#011train's auc: 0.983414#011train's binary_logloss: 0.655255\u001b[0m\n",
      "\u001b[32m[6]#011train's auc: 0.983114#011train's binary_logloss: 0.655305\u001b[0m\n",
      "\u001b[35m[7]#011val's auc: 0.98248#011val's binary_logloss: 0.649376#011train's auc: 0.98268#011train's binary_logloss: 0.649379\u001b[0m\n",
      "\u001b[35m[8]#011val's auc: 0.983437#011val's binary_logloss: 0.643242#011train's auc: 0.983675#011train's binary_logloss: 0.643242\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:30,787 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:55220 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[7]#011train's auc: 0.982522#011train's binary_logloss: 0.649403\u001b[0m\n",
      "\u001b[34m[8]#011train's auc: 0.983494#011train's binary_logloss: 0.643266\u001b[0m\n",
      "\u001b[34m[9]#011train's auc: 0.983286#011train's binary_logloss: 0.637412\u001b[0m\n",
      "\u001b[36m[7]#011train's auc: 0.982766#011train's binary_logloss: 0.649345\u001b[0m\n",
      "\u001b[36m[8]#011train's auc: 0.983741#011train's binary_logloss: 0.643208\u001b[0m\n",
      "\u001b[36m[9]#011train's auc: 0.983525#011train's binary_logloss: 0.637354\u001b[0m\n",
      "\u001b[32m[7]#011train's auc: 0.982463#011train's binary_logloss: 0.649403\u001b[0m\n",
      "\u001b[32m[8]#011train's auc: 0.98345#011train's binary_logloss: 0.643266\u001b[0m\n",
      "\u001b[32m[9]#011train's auc: 0.983241#011train's binary_logloss: 0.637415\u001b[0m\n",
      "\u001b[35m[9]#011val's auc: 0.983227#011val's binary_logloss: 0.637386#011train's auc: 0.983449#011train's binary_logloss: 0.637393\u001b[0m\n",
      "\u001b[35m[10]#011val's auc: 0.983519#011val's binary_logloss: 0.631393#011train's auc: 0.983736#011train's binary_logloss: 0.631404\u001b[0m\n",
      "\u001b[35m[11]#011val's auc: 0.983018#011val's binary_logloss: 0.625897#011train's auc: 0.983234#011train's binary_logloss: 0.625912\u001b[0m\n",
      "\u001b[34m[10]#011train's auc: 0.983583#011train's binary_logloss: 0.631421\u001b[0m\n",
      "\u001b[34m[11]#011train's auc: 0.983087#011train's binary_logloss: 0.625928\u001b[0m\n",
      "\u001b[36m[10]#011train's auc: 0.983822#011train's binary_logloss: 0.631363\u001b[0m\n",
      "\u001b[36m[11]#011train's auc: 0.983325#011train's binary_logloss: 0.625869\u001b[0m\n",
      "\u001b[36m[12]#011train's auc: 0.984263#011train's binary_logloss: 0.620266\u001b[0m\n",
      "\u001b[32m[10]#011train's auc: 0.983552#011train's binary_logloss: 0.631424\u001b[0m\n",
      "\u001b[32m[11]#011train's auc: 0.983043#011train's binary_logloss: 0.625934\u001b[0m\n",
      "\u001b[35m[12]#011val's auc: 0.983949#011val's binary_logloss: 0.620296#011train's auc: 0.98414#011train's binary_logloss: 0.620311\u001b[0m\n",
      "\u001b[35m[13]#011val's auc: 0.984582#011val's binary_logloss: 0.614507#011train's auc: 0.984752#011train's binary_logloss: 0.614524\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:32,789 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:55224 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[12]#011train's auc: 0.984022#011train's binary_logloss: 0.620327\u001b[0m\n",
      "\u001b[34m[13]#011train's auc: 0.984625#011train's binary_logloss: 0.614541\u001b[0m\n",
      "\u001b[36m[13]#011train's auc: 0.984836#011train's binary_logloss: 0.614482\u001b[0m\n",
      "\u001b[36m[14]#011train's auc: 0.986584#011train's binary_logloss: 0.608843\u001b[0m\n",
      "\u001b[32m[12]#011train's auc: 0.983951#011train's binary_logloss: 0.620339\u001b[0m\n",
      "\u001b[32m[13]#011train's auc: 0.984561#011train's binary_logloss: 0.614555\u001b[0m\n",
      "\u001b[32m[14]#011train's auc: 0.986313#011train's binary_logloss: 0.608924\u001b[0m\n",
      "\u001b[35m[14]#011val's auc: 0.986341#011val's binary_logloss: 0.608876#011train's auc: 0.986492#011train's binary_logloss: 0.608889\u001b[0m\n",
      "\u001b[35m[15]#011val's auc: 0.986114#011val's binary_logloss: 0.60407#011train's auc: 0.986267#011train's binary_logloss: 0.604081\u001b[0m\n",
      "\u001b[34m[14]#011train's auc: 0.986377#011train's binary_logloss: 0.60891\u001b[0m\n",
      "\u001b[34m[15]#011train's auc: 0.986149#011train's binary_logloss: 0.604104\u001b[0m\n",
      "\u001b[36m[15]#011train's auc: 0.986354#011train's binary_logloss: 0.604029\u001b[0m\n",
      "\u001b[36m[16]#011train's auc: 0.986709#011train's binary_logloss: 0.598556\u001b[0m\n",
      "\u001b[32m[15]#011train's auc: 0.986084#011train's binary_logloss: 0.604119\u001b[0m\n",
      "\u001b[32m[16]#011train's auc: 0.986435#011train's binary_logloss: 0.598648\u001b[0m\n",
      "\u001b[35m[16]#011val's auc: 0.986475#011val's binary_logloss: 0.598598#011train's auc: 0.986616#011train's binary_logloss: 0.598611\u001b[0m\n",
      "\u001b[35m[17]#011val's auc: 0.986956#011val's binary_logloss: 0.593141#011train's auc: 0.987042#011train's binary_logloss: 0.593154\u001b[0m\n",
      "\u001b[34m[16]#011train's auc: 0.986502#011train's binary_logloss: 0.598635\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:34,791 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:55226 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[17]#011train's auc: 0.986934#011train's binary_logloss: 0.593178\u001b[0m\n",
      "\u001b[34m[18]#011train's auc: 0.986781#011train's binary_logloss: 0.588257\u001b[0m\n",
      "\u001b[36m[17]#011train's auc: 0.987133#011train's binary_logloss: 0.593103\u001b[0m\n",
      "\u001b[36m[18]#011train's auc: 0.986984#011train's binary_logloss: 0.588173\u001b[0m\n",
      "\u001b[32m[17]#011train's auc: 0.986874#011train's binary_logloss: 0.593198\u001b[0m\n",
      "\u001b[32m[18]#011train's auc: 0.986722#011train's binary_logloss: 0.588281\u001b[0m\n",
      "\u001b[35m[18]#011val's auc: 0.986788#011val's binary_logloss: 0.588221#011train's auc: 0.986909#011train's binary_logloss: 0.58823\u001b[0m\n",
      "\u001b[35m[19]#011val's auc: 0.987086#011val's binary_logloss: 0.583118#011train's auc: 0.987208#011train's binary_logloss: 0.58313\u001b[0m\n",
      "\u001b[35m[20]#011val's auc: 0.987438#011val's binary_logloss: 0.577897#011train's auc: 0.987573#011train's binary_logloss: 0.577907\u001b[0m\n",
      "\u001b[34m[19]#011train's auc: 0.987092#011train's binary_logloss: 0.583156\u001b[0m\n",
      "\u001b[34m[20]#011train's auc: 0.987449#011train's binary_logloss: 0.577936\u001b[0m\n",
      "\u001b[36m[19]#011train's auc: 0.987311#011train's binary_logloss: 0.583066\u001b[0m\n",
      "\u001b[36m[20]#011train's auc: 0.987667#011train's binary_logloss: 0.577838\u001b[0m\n",
      "\u001b[36m[21]#011train's auc: 0.987664#011train's binary_logloss: 0.57308\u001b[0m\n",
      "\u001b[32m[19]#011train's auc: 0.987027#011train's binary_logloss: 0.58318\u001b[0m\n",
      "\u001b[32m[20]#011train's auc: 0.987384#011train's binary_logloss: 0.577959\u001b[0m\n",
      "\u001b[35m[21]#011val's auc: 0.987416#011val's binary_logloss: 0.573151#011train's auc: 0.987567#011train's binary_logloss: 0.573151\u001b[0m\n",
      "\u001b[35m[22]#011val's auc: 0.987524#011val's binary_logloss: 0.568101#011train's auc: 0.987676#011train's binary_logloss: 0.568105\u001b[0m\n",
      "\u001b[36m[22]#011train's auc: 0.987765#011train's binary_logloss: 0.568034\u001b[0m\n",
      "\u001b[36m[23]#011train's auc: 0.987713#011train's binary_logloss: 0.563418\u001b[0m\n",
      "\u001b[36m[24]#011train's auc: 0.987647#011train's binary_logloss: 0.558468\u001b[0m\n",
      "\u001b[32m[21]#011train's auc: 0.987373#011train's binary_logloss: 0.573207\u001b[0m\n",
      "\u001b[32m[22]#011train's auc: 0.987476#011train's binary_logloss: 0.568163\u001b[0m\n",
      "\u001b[32m[23]#011train's auc: 0.987421#011train's binary_logloss: 0.563551\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:36,794 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:55234 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[21]#011train's auc: 0.98744#011train's binary_logloss: 0.573184\u001b[0m\n",
      "\u001b[34m[22]#011train's auc: 0.987549#011train's binary_logloss: 0.568137\u001b[0m\n",
      "\u001b[34m[23]#011train's auc: 0.987495#011train's binary_logloss: 0.563527\u001b[0m\n",
      "\u001b[35m[23]#011val's auc: 0.987464#011val's binary_logloss: 0.563491#011train's auc: 0.987626#011train's binary_logloss: 0.563489\u001b[0m\n",
      "\u001b[35m[24]#011val's auc: 0.987398#011val's binary_logloss: 0.55854#011train's auc: 0.987569#011train's binary_logloss: 0.558536\u001b[0m\n",
      "\u001b[35m[25]#011val's auc: 0.987346#011val's binary_logloss: 0.554256#011train's auc: 0.987524#011train's binary_logloss: 0.554255\u001b[0m\n",
      "\u001b[34m[24]#011train's auc: 0.987432#011train's binary_logloss: 0.558576\u001b[0m\n",
      "\u001b[34m[25]#011train's auc: 0.987388#011train's binary_logloss: 0.554294\u001b[0m\n",
      "\u001b[34m[26]#011train's auc: 0.987415#011train's binary_logloss: 0.549885\u001b[0m\n",
      "\u001b[36m[25]#011train's auc: 0.987601#011train's binary_logloss: 0.554175\u001b[0m\n",
      "\u001b[36m[26]#011train's auc: 0.987628#011train's binary_logloss: 0.549755\u001b[0m\n",
      "\u001b[32m[24]#011train's auc: 0.987359#011train's binary_logloss: 0.558601\u001b[0m\n",
      "\u001b[32m[25]#011train's auc: 0.987308#011train's binary_logloss: 0.554321\u001b[0m\n",
      "\u001b[32m[26]#011train's auc: 0.987337#011train's binary_logloss: 0.549913\u001b[0m\n",
      "\u001b[35m[26]#011val's auc: 0.987372#011val's binary_logloss: 0.549846#011train's auc: 0.987549#011train's binary_logloss: 0.549841\u001b[0m\n",
      "\u001b[35m[27]#011val's auc: 0.987435#011val's binary_logloss: 0.545508#011train's auc: 0.987609#011train's binary_logloss: 0.5455\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:38,796 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:57746 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[27]#011train's auc: 0.987477#011train's binary_logloss: 0.54555\u001b[0m\n",
      "\u001b[34m[28]#011train's auc: 0.987507#011train's binary_logloss: 0.541025\u001b[0m\n",
      "\u001b[36m[27]#011train's auc: 0.987675#011train's binary_logloss: 0.545411\u001b[0m\n",
      "\u001b[36m[28]#011train's auc: 0.987701#011train's binary_logloss: 0.540882\u001b[0m\n",
      "\u001b[32m[27]#011train's auc: 0.987393#011train's binary_logloss: 0.545578\u001b[0m\n",
      "\u001b[32m[28]#011train's auc: 0.987419#011train's binary_logloss: 0.541053\u001b[0m\n",
      "\u001b[35m[28]#011val's auc: 0.987456#011val's binary_logloss: 0.540986#011train's auc: 0.987645#011train's binary_logloss: 0.540971\u001b[0m\n",
      "\u001b[35m[29]#011val's auc: 0.987423#011val's binary_logloss: 0.536554#011train's auc: 0.987608#011train's binary_logloss: 0.536539\u001b[0m\n",
      "\u001b[34m[29]#011train's auc: 0.987473#011train's binary_logloss: 0.536592\u001b[0m\n",
      "\u001b[34m[30]#011train's auc: 0.987478#011train's binary_logloss: 0.532107\u001b[0m\n",
      "\u001b[36m[29]#011train's auc: 0.987666#011train's binary_logloss: 0.536445\u001b[0m\n",
      "\u001b[36m[30]#011train's auc: 0.987674#011train's binary_logloss: 0.531952\u001b[0m\n",
      "\u001b[36m[31]#011train's auc: 0.988041#011train's binary_logloss: 0.527641\u001b[0m\n",
      "\u001b[32m[29]#011train's auc: 0.987383#011train's binary_logloss: 0.536623\u001b[0m\n",
      "\u001b[32m[30]#011train's auc: 0.987388#011train's binary_logloss: 0.532136\u001b[0m\n",
      "\u001b[35m[30]#011val's auc: 0.987422#011val's binary_logloss: 0.532064#011train's auc: 0.987617#011train's binary_logloss: 0.532047\u001b[0m\n",
      "\u001b[35m[31]#011val's auc: 0.987801#011val's binary_logloss: 0.527755#011train's auc: 0.987988#011train's binary_logloss: 0.527736\u001b[0m\n",
      "\u001b[35m[32]#011val's auc: 0.987698#011val's binary_logloss: 0.523665#011train's auc: 0.987885#011train's binary_logloss: 0.523649\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:40,797 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:57754 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[31]#011train's auc: 0.987866#011train's binary_logloss: 0.527797\u001b[0m\n",
      "\u001b[34m[32]#011train's auc: 0.987764#011train's binary_logloss: 0.523713\u001b[0m\n",
      "\u001b[34m[33]#011train's auc: 0.987785#011train's binary_logloss: 0.519939\u001b[0m\n",
      "\u001b[36m[32]#011train's auc: 0.987942#011train's binary_logloss: 0.523549\u001b[0m\n",
      "\u001b[36m[33]#011train's auc: 0.987962#011train's binary_logloss: 0.519764\u001b[0m\n",
      "\u001b[32m[31]#011train's auc: 0.987777#011train's binary_logloss: 0.527828\u001b[0m\n",
      "\u001b[32m[32]#011train's auc: 0.987673#011train's binary_logloss: 0.523743\u001b[0m\n",
      "\u001b[32m[33]#011train's auc: 0.987695#011train's binary_logloss: 0.519973\u001b[0m\n",
      "\u001b[35m[33]#011val's auc: 0.987722#011val's binary_logloss: 0.519887#011train's auc: 0.987906#011train's binary_logloss: 0.519874\u001b[0m\n",
      "\u001b[35m[34]#011val's auc: 0.987851#011val's binary_logloss: 0.516009#011train's auc: 0.988031#011train's binary_logloss: 0.515992\u001b[0m\n",
      "\u001b[35m[35]#011val's auc: 0.98825#011val's binary_logloss: 0.51172#011train's auc: 0.988415#011train's binary_logloss: 0.511699\u001b[0m\n",
      "\u001b[34m[34]#011train's auc: 0.987909#011train's binary_logloss: 0.516061\u001b[0m\n",
      "\u001b[34m[35]#011train's auc: 0.988301#011train's binary_logloss: 0.51177\u001b[0m\n",
      "\u001b[32m[34]#011train's auc: 0.987827#011train's binary_logloss: 0.516095\u001b[0m\n",
      "\u001b[32m[35]#011train's auc: 0.988211#011train's binary_logloss: 0.511808\u001b[0m\n",
      "\u001b[32m[36]#011train's auc: 0.988207#011train's binary_logloss: 0.5084\u001b[0m\n",
      "\u001b[36m[34]#011train's auc: 0.988081#011train's binary_logloss: 0.515883\u001b[0m\n",
      "\u001b[36m[35]#011train's auc: 0.988472#011train's binary_logloss: 0.511589\u001b[0m\n",
      "\u001b[36m[36]#011train's auc: 0.988462#011train's binary_logloss: 0.508174\u001b[0m\n",
      "\u001b[35m[36]#011val's auc: 0.988247#011val's binary_logloss: 0.508312#011train's auc: 0.988407#011train's binary_logloss: 0.508287\u001b[0m\n",
      "\u001b[35m[37]#011val's auc: 0.988208#011val's binary_logloss: 0.504463#011train's auc: 0.988376#011train's binary_logloss: 0.504438\u001b[0m\n",
      "\u001b[34m[36]#011train's auc: 0.988297#011train's binary_logloss: 0.508359\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:42,799 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:57760 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[37]#011train's auc: 0.988261#011train's binary_logloss: 0.504511\u001b[0m\n",
      "\u001b[34m[38]#011train's auc: 0.988536#011train's binary_logloss: 0.500661\u001b[0m\n",
      "\u001b[32m[37]#011train's auc: 0.988171#011train's binary_logloss: 0.504551\u001b[0m\n",
      "\u001b[32m[38]#011train's auc: 0.988449#011train's binary_logloss: 0.500704\u001b[0m\n",
      "\u001b[36m[37]#011train's auc: 0.98843#011train's binary_logloss: 0.504316\u001b[0m\n",
      "\u001b[36m[38]#011train's auc: 0.988701#011train's binary_logloss: 0.500462\u001b[0m\n",
      "\u001b[35m[38]#011val's auc: 0.988453#011val's binary_logloss: 0.500612#011train's auc: 0.988625#011train's binary_logloss: 0.500591\u001b[0m\n",
      "\u001b[35m[39]#011val's auc: 0.988367#011val's binary_logloss: 0.496796#011train's auc: 0.98854#011train's binary_logloss: 0.496777\u001b[0m\n",
      "\u001b[35m[40]#011val's auc: 0.988844#011val's binary_logloss: 0.493208#011train's auc: 0.989011#011train's binary_logloss: 0.493184\u001b[0m\n",
      "\u001b[34m[39]#011train's auc: 0.988452#011train's binary_logloss: 0.496849\u001b[0m\n",
      "\u001b[34m[40]#011train's auc: 0.988924#011train's binary_logloss: 0.493263\u001b[0m\n",
      "\u001b[36m[39]#011train's auc: 0.988618#011train's binary_logloss: 0.496647\u001b[0m\n",
      "\u001b[36m[40]#011train's auc: 0.989075#011train's binary_logloss: 0.493048\u001b[0m\n",
      "\u001b[36m[41]#011train's auc: 0.989088#011train's binary_logloss: 0.489062\u001b[0m\n",
      "\u001b[32m[39]#011train's auc: 0.988363#011train's binary_logloss: 0.496892\u001b[0m\n",
      "\u001b[32m[40]#011train's auc: 0.988848#011train's binary_logloss: 0.493305\u001b[0m\n",
      "\u001b[32m[41]#011train's auc: 0.988862#011train's binary_logloss: 0.489323\u001b[0m\n",
      "\u001b[35m[41]#011val's auc: 0.988865#011val's binary_logloss: 0.48922#011train's auc: 0.989024#011train's binary_logloss: 0.489199\u001b[0m\n",
      "\u001b[35m[42]#011val's auc: 0.989002#011val's binary_logloss: 0.485191#011train's auc: 0.989157#011train's binary_logloss: 0.485166\u001b[0m\n",
      "\u001b[34m[41]#011train's auc: 0.988939#011train's binary_logloss: 0.489277\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:44,802 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:57770 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[42]#011train's auc: 0.989076#011train's binary_logloss: 0.485247\u001b[0m\n",
      "\u001b[34m[43]#011train's auc: 0.989167#011train's binary_logloss: 0.48123\u001b[0m\n",
      "\u001b[36m[42]#011train's auc: 0.989227#011train's binary_logloss: 0.48503\u001b[0m\n",
      "\u001b[36m[43]#011train's auc: 0.989321#011train's binary_logloss: 0.481011\u001b[0m\n",
      "\u001b[32m[42]#011train's auc: 0.988998#011train's binary_logloss: 0.485292\u001b[0m\n",
      "\u001b[32m[43]#011train's auc: 0.98909#011train's binary_logloss: 0.481278\u001b[0m\n",
      "\u001b[35m[43]#011val's auc: 0.989095#011val's binary_logloss: 0.481173#011train's auc: 0.989246#011train's binary_logloss: 0.481149\u001b[0m\n",
      "\u001b[35m[44]#011val's auc: 0.989325#011val's binary_logloss: 0.477531#011train's auc: 0.989463#011train's binary_logloss: 0.477507\u001b[0m\n",
      "\u001b[35m[45]#011val's auc: 0.989465#011val's binary_logloss: 0.474605#011train's auc: 0.989623#011train's binary_logloss: 0.474577\u001b[0m\n",
      "\u001b[34m[44]#011train's auc: 0.9894#011train's binary_logloss: 0.477588\u001b[0m\n",
      "\u001b[34m[45]#011train's auc: 0.989555#011train's binary_logloss: 0.474665\u001b[0m\n",
      "\u001b[34m[46]#011train's auc: 0.989624#011train's binary_logloss: 0.471374\u001b[0m\n",
      "\u001b[36m[44]#011train's auc: 0.989545#011train's binary_logloss: 0.47736\u001b[0m\n",
      "\u001b[36m[45]#011train's auc: 0.989711#011train's binary_logloss: 0.474422\u001b[0m\n",
      "\u001b[36m[46]#011train's auc: 0.989778#011train's binary_logloss: 0.471119\u001b[0m\n",
      "\u001b[32m[44]#011train's auc: 0.989323#011train's binary_logloss: 0.477637\u001b[0m\n",
      "\u001b[32m[45]#011train's auc: 0.989472#011train's binary_logloss: 0.474714\u001b[0m\n",
      "\u001b[32m[46]#011train's auc: 0.98954#011train's binary_logloss: 0.471428\u001b[0m\n",
      "\u001b[35m[46]#011val's auc: 0.989542#011val's binary_logloss: 0.471313#011train's auc: 0.989687#011train's binary_logloss: 0.471285\u001b[0m\n",
      "\u001b[35m[47]#011val's auc: 0.989604#011val's binary_logloss: 0.467959#011train's auc: 0.989747#011train's binary_logloss: 0.467928\u001b[0m\n",
      "\u001b[35m[48]#011val's auc: 0.989752#011val's binary_logloss: 0.464368#011train's auc: 0.989896#011train's binary_logloss: 0.464339\u001b[0m\n",
      "\u001b[36m[47]#011train's auc: 0.989831#011train's binary_logloss: 0.467766\u001b[0m\n",
      "\u001b[36m[48]#011train's auc: 0.989978#011train's binary_logloss: 0.464178\u001b[0m\n",
      "\u001b[36m[49]#011train's auc: 0.9901#011train's binary_logloss: 0.46061\u001b[0m\n",
      "\u001b[32m[47]#011train's auc: 0.989601#011train's binary_logloss: 0.468074\u001b[0m\n",
      "\u001b[32m[48]#011train's auc: 0.989744#011train's binary_logloss: 0.464488\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:46,804 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:57774 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[47]#011train's auc: 0.989682#011train's binary_logloss: 0.468021\u001b[0m\n",
      "\u001b[34m[48]#011train's auc: 0.98983#011train's binary_logloss: 0.464432\u001b[0m\n",
      "\u001b[35m[49]#011val's auc: 0.989879#011val's binary_logloss: 0.460799#011train's auc: 0.99002#011train's binary_logloss: 0.460771\u001b[0m\n",
      "\u001b[35m[50]#011val's auc: 0.990187#011val's binary_logloss: 0.457441#011train's auc: 0.990322#011train's binary_logloss: 0.457411\u001b[0m\n",
      "\u001b[34m[49]#011train's auc: 0.989955#011train's binary_logloss: 0.460864\u001b[0m\n",
      "\u001b[34m[50]#011train's auc: 0.990271#011train's binary_logloss: 0.457501\u001b[0m\n",
      "\u001b[34m[51]#011train's auc: 0.990236#011train's binary_logloss: 0.454082\u001b[0m\n",
      "\u001b[36m[50]#011train's auc: 0.990421#011train's binary_logloss: 0.457241\u001b[0m\n",
      "\u001b[36m[51]#011train's auc: 0.990387#011train's binary_logloss: 0.453819\u001b[0m\n",
      "\u001b[32m[49]#011train's auc: 0.989874#011train's binary_logloss: 0.460919\u001b[0m\n",
      "\u001b[32m[50]#011train's auc: 0.990188#011train's binary_logloss: 0.45756\u001b[0m\n",
      "\u001b[32m[51]#011train's auc: 0.990155#011train's binary_logloss: 0.454141\u001b[0m\n",
      "\u001b[35m[51]#011val's auc: 0.990156#011val's binary_logloss: 0.45402#011train's auc: 0.99029#011train's binary_logloss: 0.453989\u001b[0m\n",
      "\u001b[35m[52]#011val's auc: 0.990258#011val's binary_logloss: 0.450896#011train's auc: 0.990392#011train's binary_logloss: 0.45086\u001b[0m\n",
      "\u001b[35m[53]#011val's auc: 0.990296#011val's binary_logloss: 0.447552#011train's auc: 0.99043#011train's binary_logloss: 0.447511\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:48,805 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:39782 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[52]#011train's auc: 0.990346#011train's binary_logloss: 0.450952\u001b[0m\n",
      "\u001b[34m[53]#011train's auc: 0.990384#011train's binary_logloss: 0.447605\u001b[0m\n",
      "\u001b[36m[52]#011train's auc: 0.990499#011train's binary_logloss: 0.450682\u001b[0m\n",
      "\u001b[36m[53]#011train's auc: 0.990538#011train's binary_logloss: 0.447332\u001b[0m\n",
      "\u001b[36m[54]#011train's auc: 0.990619#011train's binary_logloss: 0.443897\u001b[0m\n",
      "\u001b[32m[52]#011train's auc: 0.990264#011train's binary_logloss: 0.451016\u001b[0m\n",
      "\u001b[32m[53]#011train's auc: 0.990301#011train's binary_logloss: 0.44767\u001b[0m\n",
      "\u001b[35m[54]#011val's auc: 0.990386#011val's binary_logloss: 0.444119#011train's auc: 0.990516#011train's binary_logloss: 0.444078\u001b[0m\n",
      "\u001b[35m[55]#011val's auc: 0.990408#011val's binary_logloss: 0.440884#011train's auc: 0.990535#011train's binary_logloss: 0.440843\u001b[0m\n",
      "\u001b[34m[54]#011train's auc: 0.990467#011train's binary_logloss: 0.444172\u001b[0m\n",
      "\u001b[34m[55]#011train's auc: 0.990485#011train's binary_logloss: 0.440936\u001b[0m\n",
      "\u001b[34m[56]#011train's auc: 0.990564#011train's binary_logloss: 0.437606\u001b[0m\n",
      "\u001b[36m[55]#011train's auc: 0.99064#011train's binary_logloss: 0.440658\u001b[0m\n",
      "\u001b[36m[56]#011train's auc: 0.990718#011train's binary_logloss: 0.437328\u001b[0m\n",
      "\u001b[32m[54]#011train's auc: 0.990388#011train's binary_logloss: 0.444235\u001b[0m\n",
      "\u001b[32m[55]#011train's auc: 0.99041#011train's binary_logloss: 0.440997\u001b[0m\n",
      "\u001b[32m[56]#011train's auc: 0.990491#011train's binary_logloss: 0.437667\u001b[0m\n",
      "\u001b[35m[56]#011val's auc: 0.990495#011val's binary_logloss: 0.437551#011train's auc: 0.990616#011train's binary_logloss: 0.43751\u001b[0m\n",
      "\u001b[35m[57]#011val's auc: 0.990532#011val's binary_logloss: 0.434312#011train's auc: 0.990658#011train's binary_logloss: 0.434264\u001b[0m\n",
      "\u001b[35m[58]#011val's auc: 0.990743#011val's binary_logloss: 0.430996#011train's auc: 0.99086#011train's binary_logloss: 0.430949\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:50,808 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:39796 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[57]#011train's auc: 0.990606#011train's binary_logloss: 0.434359\u001b[0m\n",
      "\u001b[34m[58]#011train's auc: 0.990812#011train's binary_logloss: 0.431043\u001b[0m\n",
      "\u001b[34m[59]#011train's auc: 0.990885#011train's binary_logloss: 0.42781\u001b[0m\n",
      "\u001b[36m[57]#011train's auc: 0.990758#011train's binary_logloss: 0.43408\u001b[0m\n",
      "\u001b[36m[58]#011train's auc: 0.990966#011train's binary_logloss: 0.430766\u001b[0m\n",
      "\u001b[36m[59]#011train's auc: 0.991037#011train's binary_logloss: 0.42753\u001b[0m\n",
      "\u001b[32m[57]#011train's auc: 0.990536#011train's binary_logloss: 0.434423\u001b[0m\n",
      "\u001b[32m[58]#011train's auc: 0.99074#011train's binary_logloss: 0.43111\u001b[0m\n",
      "\u001b[32m[59]#011train's auc: 0.990814#011train's binary_logloss: 0.427875\u001b[0m\n",
      "\u001b[35m[59]#011val's auc: 0.990819#011val's binary_logloss: 0.427764#011train's auc: 0.990934#011train's binary_logloss: 0.427716\u001b[0m\n",
      "\u001b[35m[60]#011val's auc: 0.990876#011val's binary_logloss: 0.424708#011train's auc: 0.990985#011train's binary_logloss: 0.42466\u001b[0m\n",
      "\u001b[34m[60]#011train's auc: 0.990935#011train's binary_logloss: 0.424754\u001b[0m\n",
      "\u001b[34m[61]#011train's auc: 0.991105#011train's binary_logloss: 0.421657\u001b[0m\n",
      "\u001b[36m[60]#011train's auc: 0.991087#011train's binary_logloss: 0.42447\u001b[0m\n",
      "\u001b[36m[61]#011train's auc: 0.991258#011train's binary_logloss: 0.421369\u001b[0m\n",
      "\u001b[36m[62]#011train's auc: 0.991234#011train's binary_logloss: 0.418386\u001b[0m\n",
      "\u001b[32m[60]#011train's auc: 0.990864#011train's binary_logloss: 0.42482\u001b[0m\n",
      "\u001b[32m[61]#011train's auc: 0.991032#011train's binary_logloss: 0.421722\u001b[0m\n",
      "\u001b[35m[61]#011val's auc: 0.991047#011val's binary_logloss: 0.42161#011train's auc: 0.991154#011train's binary_logloss: 0.421563\u001b[0m\n",
      "\u001b[35m[62]#011val's auc: 0.991021#011val's binary_logloss: 0.418628#011train's auc: 0.99113#011train's binary_logloss: 0.41858\u001b[0m\n",
      "\u001b[35m[63]#011val's auc: 0.991131#011val's binary_logloss: 0.415871#011train's auc: 0.99125#011train's binary_logloss: 0.415818\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:52,810 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:39810 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[62]#011train's auc: 0.99108#011train's binary_logloss: 0.418674\u001b[0m\n",
      "\u001b[34m[63]#011train's auc: 0.9912#011train's binary_logloss: 0.415914\u001b[0m\n",
      "\u001b[34m[64]#011train's auc: 0.991299#011train's binary_logloss: 0.412953\u001b[0m\n",
      "\u001b[36m[63]#011train's auc: 0.99135#011train's binary_logloss: 0.41562\u001b[0m\n",
      "\u001b[36m[64]#011train's auc: 0.991443#011train's binary_logloss: 0.412654\u001b[0m\n",
      "\u001b[32m[62]#011train's auc: 0.991006#011train's binary_logloss: 0.418741\u001b[0m\n",
      "\u001b[32m[63]#011train's auc: 0.991128#011train's binary_logloss: 0.415982\u001b[0m\n",
      "\u001b[32m[64]#011train's auc: 0.991229#011train's binary_logloss: 0.413018\u001b[0m\n",
      "\u001b[35m[64]#011val's auc: 0.991236#011val's binary_logloss: 0.412914#011train's auc: 0.991349#011train's binary_logloss: 0.412853\u001b[0m\n",
      "\u001b[35m[65]#011val's auc: 0.99138#011val's binary_logloss: 0.409929#011train's auc: 0.991497#011train's binary_logloss: 0.409868\u001b[0m\n",
      "\u001b[34m[65]#011train's auc: 0.99145#011train's binary_logloss: 0.409965\u001b[0m\n",
      "\u001b[34m[66]#011train's auc: 0.991593#011train's binary_logloss: 0.40713\u001b[0m\n",
      "\u001b[36m[65]#011train's auc: 0.991596#011train's binary_logloss: 0.409663\u001b[0m\n",
      "\u001b[36m[66]#011train's auc: 0.991747#011train's binary_logloss: 0.406823\u001b[0m\n",
      "\u001b[36m[67]#011train's auc: 0.991833#011train's binary_logloss: 0.404232\u001b[0m\n",
      "\u001b[32m[65]#011train's auc: 0.991383#011train's binary_logloss: 0.410033\u001b[0m\n",
      "\u001b[32m[66]#011train's auc: 0.991534#011train's binary_logloss: 0.407196\u001b[0m\n",
      "\u001b[35m[66]#011val's auc: 0.99154#011val's binary_logloss: 0.407095#011train's auc: 0.991648#011train's binary_logloss: 0.407034\u001b[0m\n",
      "\u001b[35m[67]#011val's auc: 0.991616#011val's binary_logloss: 0.404503#011train's auc: 0.991726#011train's binary_logloss: 0.404445\u001b[0m\n",
      "\u001b[35m[68]#011val's auc: 0.991683#011val's binary_logloss: 0.401566#011train's auc: 0.991789#011train's binary_logloss: 0.401509\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:54,812 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:39812 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[67]#011train's auc: 0.991673#011train's binary_logloss: 0.404539\u001b[0m\n",
      "\u001b[34m[68]#011train's auc: 0.991735#011train's binary_logloss: 0.401605\u001b[0m\n",
      "\u001b[34m[69]#011train's auc: 0.991919#011train's binary_logloss: 0.398981\u001b[0m\n",
      "\u001b[36m[68]#011train's auc: 0.991898#011train's binary_logloss: 0.401294\u001b[0m\n",
      "\u001b[36m[69]#011train's auc: 0.992078#011train's binary_logloss: 0.398665\u001b[0m\n",
      "\u001b[32m[67]#011train's auc: 0.991612#011train's binary_logloss: 0.404607\u001b[0m\n",
      "\u001b[32m[68]#011train's auc: 0.991678#011train's binary_logloss: 0.401672\u001b[0m\n",
      "\u001b[32m[69]#011train's auc: 0.991867#011train's binary_logloss: 0.399046\u001b[0m\n",
      "\u001b[35m[69]#011val's auc: 0.991865#011val's binary_logloss: 0.39894#011train's auc: 0.991974#011train's binary_logloss: 0.398879\u001b[0m\n",
      "\u001b[35m[70]#011val's auc: 0.991949#011val's binary_logloss: 0.395952#011train's auc: 0.992056#011train's binary_logloss: 0.395891\u001b[0m\n",
      "\u001b[35m[71]#011val's auc: 0.99195#011val's binary_logloss: 0.393149#011train's auc: 0.992059#011train's binary_logloss: 0.393081\u001b[0m\n",
      "\u001b[34m[70]#011train's auc: 0.992004#011train's binary_logloss: 0.395991\u001b[0m\n",
      "\u001b[34m[71]#011train's auc: 0.992008#011train's binary_logloss: 0.393181\u001b[0m\n",
      "\u001b[34m[72]#011train's auc: 0.992031#011train's binary_logloss: 0.390761\u001b[0m\n",
      "\u001b[36m[70]#011train's auc: 0.992159#011train's binary_logloss: 0.395675\u001b[0m\n",
      "\u001b[36m[71]#011train's auc: 0.992163#011train's binary_logloss: 0.392858\u001b[0m\n",
      "\u001b[36m[72]#011train's auc: 0.992189#011train's binary_logloss: 0.390432\u001b[0m\n",
      "\u001b[32m[70]#011train's auc: 0.991951#011train's binary_logloss: 0.39606\u001b[0m\n",
      "\u001b[32m[71]#011train's auc: 0.991953#011train's binary_logloss: 0.393254\u001b[0m\n",
      "\u001b[32m[72]#011train's auc: 0.991976#011train's binary_logloss: 0.390838\u001b[0m\n",
      "\u001b[35m[72]#011val's auc: 0.991974#011val's binary_logloss: 0.390729#011train's auc: 0.992084#011train's binary_logloss: 0.390658\u001b[0m\n",
      "\u001b[35m[73]#011val's auc: 0.99206#011val's binary_logloss: 0.387908#011train's auc: 0.99217#011train's binary_logloss: 0.387837\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:56,813 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:39818 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[73]#011train's auc: 0.992118#011train's binary_logloss: 0.387938\u001b[0m\n",
      "\u001b[34m[74]#011train's auc: 0.992145#011train's binary_logloss: 0.385176\u001b[0m\n",
      "\u001b[36m[73]#011train's auc: 0.992277#011train's binary_logloss: 0.387606\u001b[0m\n",
      "\u001b[36m[74]#011train's auc: 0.992307#011train's binary_logloss: 0.384838\u001b[0m\n",
      "\u001b[36m[75]#011train's auc: 0.99234#011train's binary_logloss: 0.382174\u001b[0m\n",
      "\u001b[32m[73]#011train's auc: 0.992064#011train's binary_logloss: 0.388015\u001b[0m\n",
      "\u001b[32m[74]#011train's auc: 0.99209#011train's binary_logloss: 0.385257\u001b[0m\n",
      "\u001b[34m[75]#011train's auc: 0.99218#011train's binary_logloss: 0.382514\u001b[0m\n",
      "\u001b[34m[76]#011train's auc: 0.992228#011train's binary_logloss: 0.379829\u001b[0m\n",
      "\u001b[34m[77]#011train's auc: 0.992259#011train's binary_logloss: 0.377062\u001b[0m\n",
      "\u001b[34m2024-10-27 04:33:58,816 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:59160 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[78]#011train's auc: 0.992311#011train's binary_logloss: 0.374468\u001b[0m\n",
      "\u001b[34m[79]#011train's auc: 0.99233#011train's binary_logloss: 0.371936\u001b[0m\n",
      "\u001b[34m[80]#011train's auc: 0.992396#011train's binary_logloss: 0.369413\u001b[0m\n",
      "\u001b[34m[81]#011train's auc: 0.992449#011train's binary_logloss: 0.367046\u001b[0m\n",
      "\u001b[34m[82]#011train's auc: 0.992525#011train's binary_logloss: 0.364605\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:00,817 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:59162 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[83]#011train's auc: 0.992571#011train's binary_logloss: 0.362271\u001b[0m\n",
      "\u001b[34m[84]#011train's auc: 0.992592#011train's binary_logloss: 0.359889\u001b[0m\n",
      "\u001b[35m[74]#011val's auc: 0.992086#011val's binary_logloss: 0.38515#011train's auc: 0.992196#011train's binary_logloss: 0.385074\u001b[0m\n",
      "\u001b[35m[75]#011val's auc: 0.992122#011val's binary_logloss: 0.382489#011train's auc: 0.992232#011train's binary_logloss: 0.382411\u001b[0m\n",
      "\u001b[35m[76]#011val's auc: 0.992169#011val's binary_logloss: 0.379811#011train's auc: 0.992275#011train's binary_logloss: 0.37973\u001b[0m\n",
      "\u001b[36m[76]#011train's auc: 0.992389#011train's binary_logloss: 0.37949\u001b[0m\n",
      "\u001b[36m[77]#011train's auc: 0.992419#011train's binary_logloss: 0.376722\u001b[0m\n",
      "\u001b[32m[75]#011train's auc: 0.992126#011train's binary_logloss: 0.382594\u001b[0m\n",
      "\u001b[32m[76]#011train's auc: 0.992174#011train's binary_logloss: 0.379911\u001b[0m\n",
      "\u001b[32m[77]#011train's auc: 0.992204#011train's binary_logloss: 0.377145\u001b[0m\n",
      "\u001b[35m[77]#011val's auc: 0.9922#011val's binary_logloss: 0.377044#011train's auc: 0.992307#011train's binary_logloss: 0.376962\u001b[0m\n",
      "\u001b[35m[78]#011val's auc: 0.992249#011val's binary_logloss: 0.374453#011train's auc: 0.992355#011train's binary_logloss: 0.374371\u001b[0m\n",
      "\u001b[35m[79]#011val's auc: 0.992267#011val's binary_logloss: 0.371916#011train's auc: 0.992377#011train's binary_logloss: 0.371834\u001b[0m\n",
      "\u001b[36m[78]#011train's auc: 0.992469#011train's binary_logloss: 0.374126\u001b[0m\n",
      "\u001b[36m[79]#011train's auc: 0.99249#011train's binary_logloss: 0.371583\u001b[0m\n",
      "\u001b[36m[80]#011train's auc: 0.992554#011train's binary_logloss: 0.369064\u001b[0m\n",
      "\u001b[32m[78]#011train's auc: 0.992253#011train's binary_logloss: 0.374555\u001b[0m\n",
      "\u001b[32m[79]#011train's auc: 0.992273#011train's binary_logloss: 0.37202\u001b[0m\n",
      "\u001b[32m[80]#011train's auc: 0.992341#011train's binary_logloss: 0.369499\u001b[0m\n",
      "\u001b[35m[80]#011val's auc: 0.992334#011val's binary_logloss: 0.369393#011train's auc: 0.992441#011train's binary_logloss: 0.369311\u001b[0m\n",
      "\u001b[35m[81]#011val's auc: 0.992384#011val's binary_logloss: 0.367029#011train's auc: 0.992493#011train's binary_logloss: 0.366947\u001b[0m\n",
      "\u001b[36m[81]#011train's auc: 0.992607#011train's binary_logloss: 0.366689\u001b[0m\n",
      "\u001b[36m[82]#011train's auc: 0.992678#011train's binary_logloss: 0.364248\u001b[0m\n",
      "\u001b[32m[81]#011train's auc: 0.992394#011train's binary_logloss: 0.367136\u001b[0m\n",
      "\u001b[32m[82]#011train's auc: 0.992469#011train's binary_logloss: 0.364701\u001b[0m\n",
      "\u001b[35m[82]#011val's auc: 0.992454#011val's binary_logloss: 0.364594#011train's auc: 0.992569#011train's binary_logloss: 0.364511\u001b[0m\n",
      "\u001b[35m[83]#011val's auc: 0.992502#011val's binary_logloss: 0.36226#011train's auc: 0.992613#011train's binary_logloss: 0.362175\u001b[0m\n",
      "\u001b[36m[83]#011train's auc: 0.992722#011train's binary_logloss: 0.361912\u001b[0m\n",
      "\u001b[36m[84]#011train's auc: 0.992741#011train's binary_logloss: 0.359529\u001b[0m\n",
      "\u001b[32m[83]#011train's auc: 0.992515#011train's binary_logloss: 0.36237\u001b[0m\n",
      "\u001b[32m[84]#011train's auc: 0.992536#011train's binary_logloss: 0.359989\u001b[0m\n",
      "\u001b[35m[84]#011val's auc: 0.992521#011val's binary_logloss: 0.35988#011train's auc: 0.992632#011train's binary_logloss: 0.359795\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:02,819 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:59178 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[85]#011train's auc: 0.992591#011train's binary_logloss: 0.357629\u001b[0m\n",
      "\u001b[36m[85]#011train's auc: 0.992738#011train's binary_logloss: 0.357269\u001b[0m\n",
      "\u001b[32m[85]#011train's auc: 0.992535#011train's binary_logloss: 0.357732\u001b[0m\n",
      "\u001b[35m[85]#011val's auc: 0.992516#011val's binary_logloss: 0.357627#011train's auc: 0.99263#011train's binary_logloss: 0.357536\u001b[0m\n",
      "\u001b[35m[86]#011val's auc: 0.99263#011val's binary_logloss: 0.355103#011train's auc: 0.99273#011train's binary_logloss: 0.355008\u001b[0m\n",
      "\u001b[34m[86]#011train's auc: 0.992693#011train's binary_logloss: 0.355101\u001b[0m\n",
      "\u001b[36m[86]#011train's auc: 0.99284#011train's binary_logloss: 0.354741\u001b[0m\n",
      "\u001b[36m[87]#011train's auc: 0.992929#011train's binary_logloss: 0.352346\u001b[0m\n",
      "\u001b[32m[86]#011train's auc: 0.992638#011train's binary_logloss: 0.355206\u001b[0m\n",
      "\u001b[35m[87]#011val's auc: 0.992715#011val's binary_logloss: 0.352713#011train's auc: 0.992816#011train's binary_logloss: 0.352616\u001b[0m\n",
      "\u001b[35m[88]#011val's auc: 0.992799#011val's binary_logloss: 0.350359#011train's auc: 0.992897#011train's binary_logloss: 0.350262\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:04,821 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:59180 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[87]#011train's auc: 0.992781#011train's binary_logloss: 0.352709\u001b[0m\n",
      "\u001b[34m[88]#011train's auc: 0.992863#011train's binary_logloss: 0.350357\u001b[0m\n",
      "\u001b[34m[89]#011train's auc: 0.992943#011train's binary_logloss: 0.348071\u001b[0m\n",
      "\u001b[36m[88]#011train's auc: 0.993013#011train's binary_logloss: 0.34999\u001b[0m\n",
      "\u001b[36m[89]#011train's auc: 0.993094#011train's binary_logloss: 0.347697\u001b[0m\n",
      "\u001b[32m[87]#011train's auc: 0.992724#011train's binary_logloss: 0.352815\u001b[0m\n",
      "\u001b[32m[88]#011train's auc: 0.992804#011train's binary_logloss: 0.350464\u001b[0m\n",
      "\u001b[32m[89]#011train's auc: 0.992885#011train's binary_logloss: 0.348178\u001b[0m\n",
      "\u001b[35m[89]#011val's auc: 0.99288#011val's binary_logloss: 0.348074#011train's auc: 0.992976#011train's binary_logloss: 0.347975\u001b[0m\n",
      "\u001b[35m[90]#011val's auc: 0.992914#011val's binary_logloss: 0.345758#011train's auc: 0.993009#011train's binary_logloss: 0.345653\u001b[0m\n",
      "\u001b[36m[90]#011train's auc: 0.993128#011train's binary_logloss: 0.345378\u001b[0m\n",
      "\u001b[36m[91]#011train's auc: 0.993182#011train's binary_logloss: 0.343094\u001b[0m\n",
      "\u001b[36m[92]#011train's auc: 0.993188#011train's binary_logloss: 0.340867\u001b[0m\n",
      "\u001b[34m[90]#011train's auc: 0.992974#011train's binary_logloss: 0.345754\u001b[0m\n",
      "\u001b[34m[91]#011train's auc: 0.99303#011train's binary_logloss: 0.343472\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:06,823 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:59182 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[90]#011train's auc: 0.992919#011train's binary_logloss: 0.345858\u001b[0m\n",
      "\u001b[32m[91]#011train's auc: 0.992973#011train's binary_logloss: 0.34358\u001b[0m\n",
      "\u001b[35m[91]#011val's auc: 0.992968#011val's binary_logloss: 0.343481#011train's auc: 0.993064#011train's binary_logloss: 0.343372\u001b[0m\n",
      "\u001b[35m[92]#011val's auc: 0.992973#011val's binary_logloss: 0.341259#011train's auc: 0.993073#011train's binary_logloss: 0.341147\u001b[0m\n",
      "\u001b[35m[93]#011val's auc: 0.992985#011val's binary_logloss: 0.339152#011train's auc: 0.993084#011train's binary_logloss: 0.339038\u001b[0m\n",
      "\u001b[36m[93]#011train's auc: 0.993201#011train's binary_logloss: 0.338755\u001b[0m\n",
      "\u001b[36m[94]#011train's auc: 0.99327#011train's binary_logloss: 0.33639\u001b[0m\n",
      "\u001b[34m[92]#011train's auc: 0.993037#011train's binary_logloss: 0.341246\u001b[0m\n",
      "\u001b[34m[93]#011train's auc: 0.993048#011train's binary_logloss: 0.339138\u001b[0m\n",
      "\u001b[34m[94]#011train's auc: 0.993119#011train's binary_logloss: 0.336775\u001b[0m\n",
      "\u001b[32m[92]#011train's auc: 0.992981#011train's binary_logloss: 0.341354\u001b[0m\n",
      "\u001b[32m[93]#011train's auc: 0.992992#011train's binary_logloss: 0.339248\u001b[0m\n",
      "\u001b[32m[94]#011train's auc: 0.993064#011train's binary_logloss: 0.336885\u001b[0m\n",
      "\u001b[35m[94]#011val's auc: 0.993056#011val's binary_logloss: 0.336791#011train's auc: 0.993155#011train's binary_logloss: 0.336675\u001b[0m\n",
      "\u001b[35m[95]#011val's auc: 0.993116#011val's binary_logloss: 0.334705#011train's auc: 0.993215#011train's binary_logloss: 0.334584\u001b[0m\n",
      "\u001b[36m[95]#011train's auc: 0.993335#011train's binary_logloss: 0.334299\u001b[0m\n",
      "\u001b[36m[96]#011train's auc: 0.993374#011train's binary_logloss: 0.332086\u001b[0m\n",
      "\u001b[36m[97]#011train's auc: 0.993457#011train's binary_logloss: 0.330098\u001b[0m\n",
      "\u001b[34m[95]#011train's auc: 0.99318#011train's binary_logloss: 0.334689\u001b[0m\n",
      "\u001b[34m[96]#011train's auc: 0.993217#011train's binary_logloss: 0.332483\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:08,825 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:54226 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[95]#011train's auc: 0.993125#011train's binary_logloss: 0.334797\u001b[0m\n",
      "\u001b[32m[96]#011train's auc: 0.99316#011train's binary_logloss: 0.332591\u001b[0m\n",
      "\u001b[32m[97]#011train's auc: 0.993244#011train's binary_logloss: 0.33061\u001b[0m\n",
      "\u001b[35m[96]#011val's auc: 0.993152#011val's binary_logloss: 0.332499#011train's auc: 0.993252#011train's binary_logloss: 0.332376\u001b[0m\n",
      "\u001b[35m[97]#011val's auc: 0.99323#011val's binary_logloss: 0.330521#011train's auc: 0.993334#011train's binary_logloss: 0.330396\u001b[0m\n",
      "\u001b[35m[98]#011val's auc: 0.993275#011val's binary_logloss: 0.328624#011train's auc: 0.993373#011train's binary_logloss: 0.328502\u001b[0m\n",
      "\u001b[36m[98]#011train's auc: 0.993497#011train's binary_logloss: 0.328202\u001b[0m\n",
      "\u001b[36m[99]#011train's auc: 0.993575#011train's binary_logloss: 0.326184\u001b[0m\n",
      "\u001b[34m[97]#011train's auc: 0.9933#011train's binary_logloss: 0.330501\u001b[0m\n",
      "\u001b[34m[98]#011train's auc: 0.99334#011train's binary_logloss: 0.328608\u001b[0m\n",
      "\u001b[34m[99]#011train's auc: 0.993418#011train's binary_logloss: 0.326596\u001b[0m\n",
      "\u001b[32m[98]#011train's auc: 0.993285#011train's binary_logloss: 0.328716\u001b[0m\n",
      "\u001b[32m[99]#011train's auc: 0.993362#011train's binary_logloss: 0.326707\u001b[0m\n",
      "\u001b[35m[99]#011val's auc: 0.993352#011val's binary_logloss: 0.326615#011train's auc: 0.993451#011train's binary_logloss: 0.326492\u001b[0m\n",
      "\u001b[35m[100]#011val's auc: 0.993416#011val's binary_logloss: 0.324387#011train's auc: 0.993515#011train's binary_logloss: 0.324257\u001b[0m\n",
      "\u001b[35m[101]#011val's auc: 0.993439#011val's binary_logloss: 0.322315#011train's auc: 0.993537#011train's binary_logloss: 0.322185\u001b[0m\n",
      "\u001b[36m[100]#011train's auc: 0.993643#011train's binary_logloss: 0.32395\u001b[0m\n",
      "\u001b[36m[101]#011train's auc: 0.993666#011train's binary_logloss: 0.321879\u001b[0m\n",
      "\u001b[36m[102]#011train's auc: 0.99371#011train's binary_logloss: 0.320026\u001b[0m\n",
      "\u001b[34m[100]#011train's auc: 0.993485#011train's binary_logloss: 0.324363\u001b[0m\n",
      "\u001b[34m[101]#011train's auc: 0.993507#011train's binary_logloss: 0.322295\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:10,828 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:54230 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[102]#011train's auc: 0.993554#011train's binary_logloss: 0.320445\u001b[0m\n",
      "\u001b[32m[100]#011train's auc: 0.99343#011train's binary_logloss: 0.324473\u001b[0m\n",
      "\u001b[32m[101]#011train's auc: 0.993453#011train's binary_logloss: 0.322401\u001b[0m\n",
      "\u001b[32m[102]#011train's auc: 0.993498#011train's binary_logloss: 0.320553\u001b[0m\n",
      "\u001b[35m[102]#011val's auc: 0.993483#011val's binary_logloss: 0.320469#011train's auc: 0.993581#011train's binary_logloss: 0.320336\u001b[0m\n",
      "\u001b[35m[103]#011val's auc: 0.993551#011val's binary_logloss: 0.31834#011train's auc: 0.99365#011train's binary_logloss: 0.318206\u001b[0m\n",
      "\u001b[36m[103]#011train's auc: 0.993778#011train's binary_logloss: 0.317892\u001b[0m\n",
      "\u001b[36m[104]#011train's auc: 0.993799#011train's binary_logloss: 0.315843\u001b[0m\n",
      "\u001b[36m[105]#011train's auc: 0.993834#011train's binary_logloss: 0.31373\u001b[0m\n",
      "\u001b[34m[103]#011train's auc: 0.993624#011train's binary_logloss: 0.318315\u001b[0m\n",
      "\u001b[34m[104]#011train's auc: 0.993644#011train's binary_logloss: 0.316271\u001b[0m\n",
      "\u001b[32m[103]#011train's auc: 0.993567#011train's binary_logloss: 0.318424\u001b[0m\n",
      "\u001b[32m[104]#011train's auc: 0.993587#011train's binary_logloss: 0.316379\u001b[0m\n",
      "\u001b[35m[104]#011val's auc: 0.99357#011val's binary_logloss: 0.316298#011train's auc: 0.99367#011train's binary_logloss: 0.316163\u001b[0m\n",
      "\u001b[35m[105]#011val's auc: 0.993611#011val's binary_logloss: 0.314184#011train's auc: 0.993709#011train's binary_logloss: 0.314045\u001b[0m\n",
      "\u001b[35m[106]#011val's auc: 0.993664#011val's binary_logloss: 0.312147#011train's auc: 0.993762#011train's binary_logloss: 0.31201\u001b[0m\n",
      "\u001b[36m[106]#011train's auc: 0.993888#011train's binary_logloss: 0.311693\u001b[0m\n",
      "\u001b[36m[107]#011train's auc: 0.993969#011train's binary_logloss: 0.309847\u001b[0m\n",
      "\u001b[34m[105]#011train's auc: 0.993683#011train's binary_logloss: 0.314156\u001b[0m\n",
      "\u001b[34m[106]#011train's auc: 0.993736#011train's binary_logloss: 0.312121\u001b[0m\n",
      "\u001b[34m[107]#011train's auc: 0.993818#011train's binary_logloss: 0.310278\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:12,829 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:54234 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[105]#011train's auc: 0.993627#011train's binary_logloss: 0.314264\u001b[0m\n",
      "\u001b[32m[106]#011train's auc: 0.993679#011train's binary_logloss: 0.312229\u001b[0m\n",
      "\u001b[32m[107]#011train's auc: 0.993759#011train's binary_logloss: 0.310389\u001b[0m\n",
      "\u001b[35m[107]#011val's auc: 0.993746#011val's binary_logloss: 0.310304#011train's auc: 0.993841#011train's binary_logloss: 0.310168\u001b[0m\n",
      "\u001b[35m[108]#011val's auc: 0.993803#011val's binary_logloss: 0.308286#011train's auc: 0.993896#011train's binary_logloss: 0.308149\u001b[0m\n",
      "\u001b[35m[109]#011val's auc: 0.99382#011val's binary_logloss: 0.306512#011train's auc: 0.993911#011train's binary_logloss: 0.306376\u001b[0m\n",
      "\u001b[36m[108]#011train's auc: 0.994024#011train's binary_logloss: 0.307827\u001b[0m\n",
      "\u001b[36m[109]#011train's auc: 0.994036#011train's binary_logloss: 0.306053\u001b[0m\n",
      "\u001b[36m[110]#011train's auc: 0.994095#011train's binary_logloss: 0.304142\u001b[0m\n",
      "\u001b[34m[108]#011train's auc: 0.993876#011train's binary_logloss: 0.308261\u001b[0m\n",
      "\u001b[34m[109]#011train's auc: 0.99389#011train's binary_logloss: 0.306486\u001b[0m\n",
      "\u001b[34m[110]#011train's auc: 0.99395#011train's binary_logloss: 0.304576\u001b[0m\n",
      "\u001b[32m[108]#011train's auc: 0.993814#011train's binary_logloss: 0.308372\u001b[0m\n",
      "\u001b[32m[109]#011train's auc: 0.993832#011train's binary_logloss: 0.306597\u001b[0m\n",
      "\u001b[32m[110]#011train's auc: 0.993893#011train's binary_logloss: 0.304687\u001b[0m\n",
      "\u001b[35m[110]#011val's auc: 0.99388#011val's binary_logloss: 0.304601#011train's auc: 0.993971#011train's binary_logloss: 0.304466\u001b[0m\n",
      "\u001b[35m[111]#011val's auc: 0.993934#011val's binary_logloss: 0.30278#011train's auc: 0.99402#011train's binary_logloss: 0.302649\u001b[0m\n",
      "\u001b[36m[111]#011train's auc: 0.994146#011train's binary_logloss: 0.302316\u001b[0m\n",
      "\u001b[36m[112]#011train's auc: 0.994205#011train's binary_logloss: 0.300457\u001b[0m\n",
      "\u001b[34m[111]#011train's auc: 0.994003#011train's binary_logloss: 0.302754\u001b[0m\n",
      "\u001b[34m[112]#011train's auc: 0.994063#011train's binary_logloss: 0.300893\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:14,831 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:54238 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[111]#011train's auc: 0.993945#011train's binary_logloss: 0.302867\u001b[0m\n",
      "\u001b[32m[112]#011train's auc: 0.994006#011train's binary_logloss: 0.301007\u001b[0m\n",
      "\u001b[35m[112]#011val's auc: 0.993989#011val's binary_logloss: 0.300921#011train's auc: 0.994077#011train's binary_logloss: 0.300786\u001b[0m\n",
      "\u001b[35m[113]#011val's auc: 0.99403#011val's binary_logloss: 0.299217#011train's auc: 0.994121#011train's binary_logloss: 0.299078\u001b[0m\n",
      "\u001b[35m[114]#011val's auc: 0.994048#011val's binary_logloss: 0.29733#011train's auc: 0.994139#011train's binary_logloss: 0.297191\u001b[0m\n",
      "\u001b[36m[113]#011train's auc: 0.994249#011train's binary_logloss: 0.298747\u001b[0m\n",
      "\u001b[36m[114]#011train's auc: 0.994269#011train's binary_logloss: 0.296855\u001b[0m\n",
      "\u001b[36m[115]#011train's auc: 0.994292#011train's binary_logloss: 0.295198\u001b[0m\n",
      "\u001b[34m[113]#011train's auc: 0.994108#011train's binary_logloss: 0.299182\u001b[0m\n",
      "\u001b[34m[114]#011train's auc: 0.994127#011train's binary_logloss: 0.297294\u001b[0m\n",
      "\u001b[34m[115]#011train's auc: 0.994152#011train's binary_logloss: 0.295639\u001b[0m\n",
      "\u001b[32m[113]#011train's auc: 0.994049#011train's binary_logloss: 0.299302\u001b[0m\n",
      "\u001b[32m[114]#011train's auc: 0.994069#011train's binary_logloss: 0.297415\u001b[0m\n",
      "\u001b[32m[115]#011train's auc: 0.994093#011train's binary_logloss: 0.295765\u001b[0m\n",
      "\u001b[35m[115]#011val's auc: 0.994071#011val's binary_logloss: 0.295678#011train's auc: 0.994163#011train's binary_logloss: 0.295539\u001b[0m\n",
      "\u001b[35m[116]#011val's auc: 0.994129#011val's binary_logloss: 0.293935#011train's auc: 0.994215#011train's binary_logloss: 0.293797\u001b[0m\n",
      "\u001b[35m[117]#011val's auc: 0.994173#011val's binary_logloss: 0.292198#011train's auc: 0.994255#011train's binary_logloss: 0.292064\u001b[0m\n",
      "\u001b[36m[116]#011train's auc: 0.994348#011train's binary_logloss: 0.29345\u001b[0m\n",
      "\u001b[36m[117]#011train's auc: 0.994387#011train's binary_logloss: 0.291715\u001b[0m\n",
      "\u001b[36m[118]#011train's auc: 0.994417#011train's binary_logloss: 0.28985\u001b[0m\n",
      "\u001b[34m[116]#011train's auc: 0.994205#011train's binary_logloss: 0.293896\u001b[0m\n",
      "\u001b[34m[117]#011train's auc: 0.994247#011train's binary_logloss: 0.29216\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:16,834 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:54252 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[118]#011train's auc: 0.994277#011train's binary_logloss: 0.290297\u001b[0m\n",
      "\u001b[32m[116]#011train's auc: 0.994146#011train's binary_logloss: 0.294024\u001b[0m\n",
      "\u001b[32m[117]#011train's auc: 0.994189#011train's binary_logloss: 0.292288\u001b[0m\n",
      "\u001b[32m[118]#011train's auc: 0.994218#011train's binary_logloss: 0.290427\u001b[0m\n",
      "\u001b[35m[118]#011val's auc: 0.994203#011val's binary_logloss: 0.290334#011train's auc: 0.994285#011train's binary_logloss: 0.290199\u001b[0m\n",
      "\u001b[35m[119]#011val's auc: 0.994246#011val's binary_logloss: 0.288807#011train's auc: 0.994326#011train's binary_logloss: 0.288673\u001b[0m\n",
      "\u001b[36m[119]#011train's auc: 0.994459#011train's binary_logloss: 0.288321\u001b[0m\n",
      "\u001b[36m[120]#011train's auc: 0.994525#011train's binary_logloss: 0.286424\u001b[0m\n",
      "\u001b[34m[119]#011train's auc: 0.994319#011train's binary_logloss: 0.28877\u001b[0m\n",
      "\u001b[34m[120]#011train's auc: 0.994384#011train's binary_logloss: 0.286877\u001b[0m\n",
      "\u001b[32m[119]#011train's auc: 0.994259#011train's binary_logloss: 0.288903\u001b[0m\n",
      "\u001b[32m[120]#011train's auc: 0.994325#011train's binary_logloss: 0.287009\u001b[0m\n",
      "\u001b[35m[120]#011val's auc: 0.994313#011val's binary_logloss: 0.286913#011train's auc: 0.994391#011train's binary_logloss: 0.28678\u001b[0m\n",
      "\u001b[35m[121]#011val's auc: 0.994354#011val's binary_logloss: 0.285128#011train's auc: 0.994432#011train's binary_logloss: 0.28499\u001b[0m\n",
      "\u001b[35m[122]#011val's auc: 0.994399#011val's binary_logloss: 0.283388#011train's auc: 0.994475#011train's binary_logloss: 0.283252\u001b[0m\n",
      "\u001b[36m[121]#011train's auc: 0.994568#011train's binary_logloss: 0.284632\u001b[0m\n",
      "\u001b[36m[122]#011train's auc: 0.994612#011train's binary_logloss: 0.282889\u001b[0m\n",
      "\u001b[36m[123]#011train's auc: 0.994661#011train's binary_logloss: 0.281291\u001b[0m\n",
      "\u001b[34m[121]#011train's auc: 0.994427#011train's binary_logloss: 0.285089\u001b[0m\n",
      "\u001b[34m[122]#011train's auc: 0.99447#011train's binary_logloss: 0.283349\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:18,836 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:42934 closed before handshake completed\u001b[0m\n",
      "\u001b[36m[124]#011train's auc: 0.994731#011train's binary_logloss: 0.279718\u001b[0m\n",
      "\u001b[36m[125]#011train's auc: 0.99476#011train's binary_logloss: 0.278347\u001b[0m\n",
      "\u001b[34m[123]#011train's auc: 0.994519#011train's binary_logloss: 0.281753\u001b[0m\n",
      "\u001b[34m[124]#011train's auc: 0.994593#011train's binary_logloss: 0.280184\u001b[0m\n",
      "\u001b[34m[125]#011train's auc: 0.994622#011train's binary_logloss: 0.278819\u001b[0m\n",
      "\u001b[36m[126]#011train's auc: 0.994801#011train's binary_logloss: 0.276706\u001b[0m\n",
      "\u001b[36m[127]#011train's auc: 0.994831#011train's binary_logloss: 0.275014\u001b[0m\n",
      "\u001b[34m[126]#011train's auc: 0.994664#011train's binary_logloss: 0.27718\u001b[0m\n",
      "\u001b[34m[127]#011train's auc: 0.994694#011train's binary_logloss: 0.275489\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:20,837 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:42936 closed before handshake completed\u001b[0m\n",
      "\u001b[36m[128]#011train's auc: 0.994863#011train's binary_logloss: 0.273416\u001b[0m\n",
      "\u001b[36m[129]#011train's auc: 0.994885#011train's binary_logloss: 0.271794\u001b[0m\n",
      "\u001b[36m[130]#011train's auc: 0.994906#011train's binary_logloss: 0.270168\u001b[0m\n",
      "\u001b[34m[128]#011train's auc: 0.994725#011train's binary_logloss: 0.273895\u001b[0m\n",
      "\u001b[34m[129]#011train's auc: 0.994749#011train's binary_logloss: 0.272269\u001b[0m\n",
      "\u001b[34m[130]#011train's auc: 0.994772#011train's binary_logloss: 0.270643\u001b[0m\n",
      "\u001b[36m[131]#011train's auc: 0.994929#011train's binary_logloss: 0.268474\u001b[0m\n",
      "\u001b[36m[132]#011train's auc: 0.994972#011train's binary_logloss: 0.26693\u001b[0m\n",
      "\u001b[36m[133]#011train's auc: 0.995037#011train's binary_logloss: 0.265256\u001b[0m\n",
      "\u001b[34m[131]#011train's auc: 0.994795#011train's binary_logloss: 0.268947\u001b[0m\n",
      "\u001b[34m[132]#011train's auc: 0.994837#011train's binary_logloss: 0.267408\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:22,838 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:42940 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[121]#011train's auc: 0.994369#011train's binary_logloss: 0.285219\u001b[0m\n",
      "\u001b[32m[122]#011train's auc: 0.994413#011train's binary_logloss: 0.283479\u001b[0m\n",
      "\u001b[32m[123]#011train's auc: 0.994463#011train's binary_logloss: 0.281886\u001b[0m\n",
      "\u001b[35m[123]#011val's auc: 0.994449#011val's binary_logloss: 0.281793#011train's auc: 0.994523#011train's binary_logloss: 0.281659\u001b[0m\n",
      "\u001b[35m[124]#011val's auc: 0.994519#011val's binary_logloss: 0.280223#011train's auc: 0.994592#011train's binary_logloss: 0.280092\u001b[0m\n",
      "\u001b[32m[124]#011train's auc: 0.994538#011train's binary_logloss: 0.280319\u001b[0m\n",
      "\u001b[32m[125]#011train's auc: 0.994567#011train's binary_logloss: 0.278957\u001b[0m\n",
      "\u001b[35m[125]#011val's auc: 0.994549#011val's binary_logloss: 0.278858#011train's auc: 0.99462#011train's binary_logloss: 0.27873\u001b[0m\n",
      "\u001b[35m[126]#011val's auc: 0.994596#011val's binary_logloss: 0.277213#011train's auc: 0.994661#011train's binary_logloss: 0.277092\u001b[0m\n",
      "\u001b[32m[126]#011train's auc: 0.994609#011train's binary_logloss: 0.277317\u001b[0m\n",
      "\u001b[32m[127]#011train's auc: 0.994638#011train's binary_logloss: 0.275626\u001b[0m\n",
      "\u001b[35m[127]#011val's auc: 0.994625#011val's binary_logloss: 0.275518#011train's auc: 0.994691#011train's binary_logloss: 0.275401\u001b[0m\n",
      "\u001b[35m[128]#011val's auc: 0.994658#011val's binary_logloss: 0.273924#011train's auc: 0.994721#011train's binary_logloss: 0.273811\u001b[0m\n",
      "\u001b[35m[129]#011val's auc: 0.99468#011val's binary_logloss: 0.272305#011train's auc: 0.994745#011train's binary_logloss: 0.272186\u001b[0m\n",
      "\u001b[32m[128]#011train's auc: 0.994669#011train's binary_logloss: 0.274034\u001b[0m\n",
      "\u001b[32m[129]#011train's auc: 0.994692#011train's binary_logloss: 0.272412\u001b[0m\n",
      "\u001b[32m[130]#011train's auc: 0.994716#011train's binary_logloss: 0.270786\u001b[0m\n",
      "\u001b[35m[130]#011val's auc: 0.994703#011val's binary_logloss: 0.270683#011train's auc: 0.994768#011train's binary_logloss: 0.27056\u001b[0m\n",
      "\u001b[35m[131]#011val's auc: 0.994727#011val's binary_logloss: 0.268985#011train's auc: 0.994792#011train's binary_logloss: 0.268861\u001b[0m\n",
      "\u001b[35m[132]#011val's auc: 0.994768#011val's binary_logloss: 0.267447#011train's auc: 0.994832#011train's binary_logloss: 0.267323\u001b[0m\n",
      "\u001b[32m[131]#011train's auc: 0.994739#011train's binary_logloss: 0.26909\u001b[0m\n",
      "\u001b[32m[132]#011train's auc: 0.994781#011train's binary_logloss: 0.267552\u001b[0m\n",
      "\u001b[32m[133]#011train's auc: 0.994848#011train's binary_logloss: 0.26588\u001b[0m\n",
      "\u001b[35m[133]#011val's auc: 0.994834#011val's binary_logloss: 0.265775#011train's auc: 0.994896#011train's binary_logloss: 0.265653\u001b[0m\n",
      "\u001b[35m[134]#011val's auc: 0.99489#011val's binary_logloss: 0.264215#011train's auc: 0.994953#011train's binary_logloss: 0.264093\u001b[0m\n",
      "\u001b[36m[134]#011train's auc: 0.995097#011train's binary_logloss: 0.26369\u001b[0m\n",
      "\u001b[36m[135]#011train's auc: 0.995176#011train's binary_logloss: 0.262054\u001b[0m\n",
      "\u001b[34m[133]#011train's auc: 0.994902#011train's binary_logloss: 0.265736\u001b[0m\n",
      "\u001b[34m[134]#011train's auc: 0.994961#011train's binary_logloss: 0.264172\u001b[0m\n",
      "\u001b[34m[135]#011train's auc: 0.995042#011train's binary_logloss: 0.262536\u001b[0m\n",
      "\u001b[32m[134]#011train's auc: 0.994905#011train's binary_logloss: 0.26432\u001b[0m\n",
      "\u001b[32m[135]#011train's auc: 0.994986#011train's binary_logloss: 0.262686\u001b[0m\n",
      "\u001b[35m[135]#011val's auc: 0.994973#011val's binary_logloss: 0.262579#011train's auc: 0.995034#011train's binary_logloss: 0.262459\u001b[0m\n",
      "\u001b[35m[136]#011val's auc: 0.995014#011val's binary_logloss: 0.260945#011train's auc: 0.995076#011train's binary_logloss: 0.260826\u001b[0m\n",
      "\u001b[36m[136]#011train's auc: 0.99522#011train's binary_logloss: 0.260413\u001b[0m\n",
      "\u001b[36m[137]#011train's auc: 0.995258#011train's binary_logloss: 0.258935\u001b[0m\n",
      "\u001b[34m[136]#011train's auc: 0.995086#011train's binary_logloss: 0.2609\u001b[0m\n",
      "\u001b[34m[137]#011train's auc: 0.995125#011train's binary_logloss: 0.259426\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:24,841 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:42944 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[136]#011train's auc: 0.99503#011train's binary_logloss: 0.261049\u001b[0m\n",
      "\u001b[32m[137]#011train's auc: 0.995068#011train's binary_logloss: 0.259577\u001b[0m\n",
      "\u001b[32m[138]#011train's auc: 0.995108#011train's binary_logloss: 0.258108\u001b[0m\n",
      "\u001b[35m[137]#011val's auc: 0.995052#011val's binary_logloss: 0.259472#011train's auc: 0.995114#011train's binary_logloss: 0.259351\u001b[0m\n",
      "\u001b[35m[138]#011val's auc: 0.995093#011val's binary_logloss: 0.258003#011train's auc: 0.995154#011train's binary_logloss: 0.257881\u001b[0m\n",
      "\u001b[35m[139]#011val's auc: 0.995142#011val's binary_logloss: 0.256432#011train's auc: 0.995202#011train's binary_logloss: 0.25631\u001b[0m\n",
      "\u001b[36m[138]#011train's auc: 0.995296#011train's binary_logloss: 0.257466\u001b[0m\n",
      "\u001b[36m[139]#011train's auc: 0.995342#011train's binary_logloss: 0.255893\u001b[0m\n",
      "\u001b[36m[140]#011train's auc: 0.995405#011train's binary_logloss: 0.254427\u001b[0m\n",
      "\u001b[34m[138]#011train's auc: 0.995165#011train's binary_logloss: 0.257957\u001b[0m\n",
      "\u001b[34m[139]#011train's auc: 0.995213#011train's binary_logloss: 0.256385\u001b[0m\n",
      "\u001b[34m[140]#011train's auc: 0.99527#011train's binary_logloss: 0.254925\u001b[0m\n",
      "\u001b[32m[139]#011train's auc: 0.995157#011train's binary_logloss: 0.256537\u001b[0m\n",
      "\u001b[32m[140]#011train's auc: 0.995219#011train's binary_logloss: 0.255077\u001b[0m\n",
      "\u001b[35m[140]#011val's auc: 0.995208#011val's binary_logloss: 0.254972#011train's auc: 0.995265#011train's binary_logloss: 0.25485\u001b[0m\n",
      "\u001b[35m[141]#011val's auc: 0.995228#011val's binary_logloss: 0.2536#011train's auc: 0.995285#011train's binary_logloss: 0.253477\u001b[0m\n",
      "\u001b[35m[142]#011val's auc: 0.995271#011val's binary_logloss: 0.252216#011train's auc: 0.995327#011train's binary_logloss: 0.252093\u001b[0m\n",
      "\u001b[36m[141]#011train's auc: 0.995425#011train's binary_logloss: 0.253052\u001b[0m\n",
      "\u001b[36m[142]#011train's auc: 0.995468#011train's binary_logloss: 0.251663\u001b[0m\n",
      "\u001b[36m[143]#011train's auc: 0.995502#011train's binary_logloss: 0.250175\u001b[0m\n",
      "\u001b[34m[141]#011train's auc: 0.995291#011train's binary_logloss: 0.253554\u001b[0m\n",
      "\u001b[34m[142]#011train's auc: 0.99533#011train's binary_logloss: 0.25217\u001b[0m\n",
      "\u001b[34m[143]#011train's auc: 0.995365#011train's binary_logloss: 0.250681\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:26,841 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:42952 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[141]#011train's auc: 0.99524#011train's binary_logloss: 0.253704\u001b[0m\n",
      "\u001b[32m[142]#011train's auc: 0.995282#011train's binary_logloss: 0.252319\u001b[0m\n",
      "\u001b[32m[143]#011train's auc: 0.995315#011train's binary_logloss: 0.250831\u001b[0m\n",
      "\u001b[35m[143]#011val's auc: 0.995303#011val's binary_logloss: 0.250727#011train's auc: 0.995362#011train's binary_logloss: 0.250605\u001b[0m\n",
      "\u001b[35m[144]#011val's auc: 0.995331#011val's binary_logloss: 0.249517#011train's auc: 0.99539#011train's binary_logloss: 0.249389\u001b[0m\n",
      "\u001b[35m[145]#011val's auc: 0.995375#011val's binary_logloss: 0.248216#011train's auc: 0.995433#011train's binary_logloss: 0.248089\u001b[0m\n",
      "\u001b[36m[144]#011train's auc: 0.995532#011train's binary_logloss: 0.248959\u001b[0m\n",
      "\u001b[36m[145]#011train's auc: 0.995576#011train's binary_logloss: 0.247655\u001b[0m\n",
      "\u001b[36m[146]#011train's auc: 0.995632#011train's binary_logloss: 0.246069\u001b[0m\n",
      "\u001b[34m[144]#011train's auc: 0.995397#011train's binary_logloss: 0.249464\u001b[0m\n",
      "\u001b[34m[145]#011train's auc: 0.99544#011train's binary_logloss: 0.248163\u001b[0m\n",
      "\u001b[34m[146]#011train's auc: 0.995498#011train's binary_logloss: 0.246577\u001b[0m\n",
      "\u001b[32m[144]#011train's auc: 0.995347#011train's binary_logloss: 0.249618\u001b[0m\n",
      "\u001b[32m[145]#011train's auc: 0.99539#011train's binary_logloss: 0.248318\u001b[0m\n",
      "\u001b[32m[146]#011train's auc: 0.995448#011train's binary_logloss: 0.246731\u001b[0m\n",
      "\u001b[35m[146]#011val's auc: 0.995435#011val's binary_logloss: 0.246627#011train's auc: 0.99549#011train's binary_logloss: 0.246501\u001b[0m\n",
      "\u001b[35m[147]#011val's auc: 0.995465#011val's binary_logloss: 0.245295#011train's auc: 0.995519#011train's binary_logloss: 0.245171\u001b[0m\n",
      "\u001b[36m[147]#011train's auc: 0.995661#011train's binary_logloss: 0.244736\u001b[0m\n",
      "\u001b[36m[148]#011train's auc: 0.995709#011train's binary_logloss: 0.243202\u001b[0m\n",
      "\u001b[34m[147]#011train's auc: 0.995528#011train's binary_logloss: 0.245246\u001b[0m\n",
      "\u001b[34m[148]#011train's auc: 0.995575#011train's binary_logloss: 0.243711\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:28,843 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:36030 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[147]#011train's auc: 0.995478#011train's binary_logloss: 0.245399\u001b[0m\n",
      "\u001b[32m[148]#011train's auc: 0.995526#011train's binary_logloss: 0.243864\u001b[0m\n",
      "\u001b[32m[149]#011train's auc: 0.995562#011train's binary_logloss: 0.242434\u001b[0m\n",
      "\u001b[36m[149]#011train's auc: 0.995744#011train's binary_logloss: 0.241774\u001b[0m\n",
      "\u001b[36m[150]#011train's auc: 0.995776#011train's binary_logloss: 0.240629\u001b[0m\n",
      "\u001b[36m[151]#011train's auc: 0.995792#011train's binary_logloss: 0.239423\u001b[0m\n",
      "\u001b[34m[149]#011train's auc: 0.995612#011train's binary_logloss: 0.242281\u001b[0m\n",
      "\u001b[34m[150]#011train's auc: 0.995646#011train's binary_logloss: 0.241133\u001b[0m\n",
      "\u001b[34m[151]#011train's auc: 0.995663#011train's binary_logloss: 0.239929\u001b[0m\n",
      "\u001b[32m[150]#011train's auc: 0.995597#011train's binary_logloss: 0.24129\u001b[0m\n",
      "\u001b[32m[151]#011train's auc: 0.995613#011train's binary_logloss: 0.240087\u001b[0m\n",
      "\u001b[36m[152]#011train's auc: 0.995815#011train's binary_logloss: 0.238088\u001b[0m\n",
      "\u001b[34m[152]#011train's auc: 0.995687#011train's binary_logloss: 0.238596\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:30,845 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:36034 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[152]#011train's auc: 0.995639#011train's binary_logloss: 0.238752\u001b[0m\n",
      "\u001b[32m[153]#011train's auc: 0.995666#011train's binary_logloss: 0.237401\u001b[0m\n",
      "\u001b[36m[153]#011train's auc: 0.995841#011train's binary_logloss: 0.236736\u001b[0m\n",
      "\u001b[36m[154]#011train's auc: 0.995906#011train's binary_logloss: 0.235644\u001b[0m\n",
      "\u001b[36m[155]#011train's auc: 0.995924#011train's binary_logloss: 0.234283\u001b[0m\n",
      "\u001b[34m[153]#011train's auc: 0.995713#011train's binary_logloss: 0.237247\u001b[0m\n",
      "\u001b[34m[154]#011train's auc: 0.995781#011train's binary_logloss: 0.236157\u001b[0m\n",
      "\u001b[34m[155]#011train's auc: 0.995799#011train's binary_logloss: 0.234799\u001b[0m\n",
      "\u001b[32m[154]#011train's auc: 0.995731#011train's binary_logloss: 0.236313\u001b[0m\n",
      "\u001b[32m[155]#011train's auc: 0.995749#011train's binary_logloss: 0.234955\u001b[0m\n",
      "\u001b[36m[156]#011train's auc: 0.995949#011train's binary_logloss: 0.233136\u001b[0m\n",
      "\u001b[36m[157]#011train's auc: 0.995976#011train's binary_logloss: 0.231973\u001b[0m\n",
      "\u001b[36m[158]#011train's auc: 0.99604#011train's binary_logloss: 0.230822\u001b[0m\n",
      "\u001b[34m[156]#011train's auc: 0.995825#011train's binary_logloss: 0.233651\u001b[0m\n",
      "\u001b[34m[157]#011train's auc: 0.995852#011train's binary_logloss: 0.232489\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:32,847 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:36050 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[156]#011train's auc: 0.995775#011train's binary_logloss: 0.233807\u001b[0m\n",
      "\u001b[32m[157]#011train's auc: 0.995802#011train's binary_logloss: 0.232644\u001b[0m\n",
      "\u001b[32m[158]#011train's auc: 0.995873#011train's binary_logloss: 0.23149\u001b[0m\n",
      "\u001b[35m[148]#011val's auc: 0.995516#011val's binary_logloss: 0.243755#011train's auc: 0.995566#011train's binary_logloss: 0.243633\u001b[0m\n",
      "\u001b[35m[149]#011val's auc: 0.995551#011val's binary_logloss: 0.242325#011train's auc: 0.995602#011train's binary_logloss: 0.242203\u001b[0m\n",
      "\u001b[35m[150]#011val's auc: 0.995586#011val's binary_logloss: 0.241183#011train's auc: 0.995636#011train's binary_logloss: 0.24106\u001b[0m\n",
      "\u001b[35m[151]#011val's auc: 0.995603#011val's binary_logloss: 0.239979#011train's auc: 0.995653#011train's binary_logloss: 0.239855\u001b[0m\n",
      "\u001b[35m[152]#011val's auc: 0.995628#011val's binary_logloss: 0.238644#011train's auc: 0.995678#011train's binary_logloss: 0.23852\u001b[0m\n",
      "\u001b[35m[153]#011val's auc: 0.995652#011val's binary_logloss: 0.237295#011train's auc: 0.995704#011train's binary_logloss: 0.237173\u001b[0m\n",
      "\u001b[35m[154]#011val's auc: 0.995721#011val's binary_logloss: 0.236208#011train's auc: 0.995777#011train's binary_logloss: 0.23608\u001b[0m\n",
      "\u001b[35m[155]#011val's auc: 0.995739#011val's binary_logloss: 0.23485#011train's auc: 0.995796#011train's binary_logloss: 0.234723\u001b[0m\n",
      "\u001b[35m[156]#011val's auc: 0.995763#011val's binary_logloss: 0.233705#011train's auc: 0.995821#011train's binary_logloss: 0.233576\u001b[0m\n",
      "\u001b[35m[157]#011val's auc: 0.99579#011val's binary_logloss: 0.232545#011train's auc: 0.995848#011train's binary_logloss: 0.23241\u001b[0m\n",
      "\u001b[35m[158]#011val's auc: 0.995858#011val's binary_logloss: 0.231393#011train's auc: 0.995915#011train's binary_logloss: 0.231258\u001b[0m\n",
      "\u001b[35m[159]#011val's auc: 0.995888#011val's binary_logloss: 0.230155#011train's auc: 0.995944#011train's binary_logloss: 0.23002\u001b[0m\n",
      "\u001b[36m[159]#011train's auc: 0.996071#011train's binary_logloss: 0.229582\u001b[0m\n",
      "\u001b[36m[160]#011train's auc: 0.996133#011train's binary_logloss: 0.228155\u001b[0m\n",
      "\u001b[34m[158]#011train's auc: 0.995922#011train's binary_logloss: 0.231334\u001b[0m\n",
      "\u001b[34m[159]#011train's auc: 0.995952#011train's binary_logloss: 0.230096\u001b[0m\n",
      "\u001b[34m[160]#011train's auc: 0.996017#011train's binary_logloss: 0.228671\u001b[0m\n",
      "\u001b[32m[159]#011train's auc: 0.995905#011train's binary_logloss: 0.230252\u001b[0m\n",
      "\u001b[32m[160]#011train's auc: 0.99597#011train's binary_logloss: 0.228826\u001b[0m\n",
      "\u001b[35m[160]#011val's auc: 0.995953#011val's binary_logloss: 0.228728#011train's auc: 0.996006#011train's binary_logloss: 0.228594\u001b[0m\n",
      "\u001b[35m[161]#011val's auc: 0.99599#011val's binary_logloss: 0.227461#011train's auc: 0.996042#011train's binary_logloss: 0.227328\u001b[0m\n",
      "\u001b[35m[162]#011val's auc: 0.99602#011val's binary_logloss: 0.226157#011train's auc: 0.996073#011train's binary_logloss: 0.226024\u001b[0m\n",
      "\u001b[36m[161]#011train's auc: 0.996171#011train's binary_logloss: 0.226891\u001b[0m\n",
      "\u001b[36m[162]#011train's auc: 0.996201#011train's binary_logloss: 0.225588\u001b[0m\n",
      "\u001b[34m[161]#011train's auc: 0.996055#011train's binary_logloss: 0.227404\u001b[0m\n",
      "\u001b[34m[162]#011train's auc: 0.996086#011train's binary_logloss: 0.226099\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:34,849 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:36056 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[161]#011train's auc: 0.996009#011train's binary_logloss: 0.227558\u001b[0m\n",
      "\u001b[32m[162]#011train's auc: 0.99604#011train's binary_logloss: 0.226251\u001b[0m\n",
      "\u001b[32m[163]#011train's auc: 0.996137#011train's binary_logloss: 0.225112\u001b[0m\n",
      "\u001b[35m[163]#011val's auc: 0.996113#011val's binary_logloss: 0.225019#011train's auc: 0.996167#011train's binary_logloss: 0.224883\u001b[0m\n",
      "\u001b[35m[164]#011val's auc: 0.996156#011val's binary_logloss: 0.223812#011train's auc: 0.99621#011train's binary_logloss: 0.223676\u001b[0m\n",
      "\u001b[36m[163]#011train's auc: 0.99629#011train's binary_logloss: 0.224449\u001b[0m\n",
      "\u001b[36m[164]#011train's auc: 0.996333#011train's binary_logloss: 0.223245\u001b[0m\n",
      "\u001b[36m[165]#011train's auc: 0.996353#011train's binary_logloss: 0.221986\u001b[0m\n",
      "\u001b[34m[163]#011train's auc: 0.996184#011train's binary_logloss: 0.224959\u001b[0m\n",
      "\u001b[34m[164]#011train's auc: 0.996228#011train's binary_logloss: 0.223754\u001b[0m\n",
      "\u001b[32m[164]#011train's auc: 0.996181#011train's binary_logloss: 0.223909\u001b[0m\n",
      "\u001b[32m[165]#011train's auc: 0.996202#011train's binary_logloss: 0.222651\u001b[0m\n",
      "\u001b[35m[165]#011val's auc: 0.996175#011val's binary_logloss: 0.222557#011train's auc: 0.99623#011train's binary_logloss: 0.222419\u001b[0m\n",
      "\u001b[35m[166]#011val's auc: 0.996227#011val's binary_logloss: 0.221248#011train's auc: 0.996279#011train's binary_logloss: 0.221112\u001b[0m\n",
      "\u001b[36m[166]#011train's auc: 0.996403#011train's binary_logloss: 0.220681\u001b[0m\n",
      "\u001b[36m[167]#011train's auc: 0.996437#011train's binary_logloss: 0.219538\u001b[0m\n",
      "\u001b[34m[165]#011train's auc: 0.996249#011train's binary_logloss: 0.222495\u001b[0m\n",
      "\u001b[34m[166]#011train's auc: 0.996299#011train's binary_logloss: 0.221189\u001b[0m\n",
      "\u001b[34m[167]#011train's auc: 0.996334#011train's binary_logloss: 0.220046\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:36,852 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:36062 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[166]#011train's auc: 0.996252#011train's binary_logloss: 0.221345\u001b[0m\n",
      "\u001b[32m[167]#011train's auc: 0.996287#011train's binary_logloss: 0.220205\u001b[0m\n",
      "\u001b[35m[167]#011val's auc: 0.996264#011val's binary_logloss: 0.220102#011train's auc: 0.996315#011train's binary_logloss: 0.219968\u001b[0m\n",
      "\u001b[35m[168]#011val's auc: 0.996279#011val's binary_logloss: 0.21892#011train's auc: 0.996329#011train's binary_logloss: 0.218788\u001b[0m\n",
      "\u001b[35m[169]#011val's auc: 0.996334#011val's binary_logloss: 0.217698#011train's auc: 0.996382#011train's binary_logloss: 0.217567\u001b[0m\n",
      "\u001b[36m[168]#011train's auc: 0.996451#011train's binary_logloss: 0.218355\u001b[0m\n",
      "\u001b[36m[169]#011train's auc: 0.996503#011train's binary_logloss: 0.217134\u001b[0m\n",
      "\u001b[36m[170]#011train's auc: 0.996525#011train's binary_logloss: 0.216116\u001b[0m\n",
      "\u001b[34m[168]#011train's auc: 0.996349#011train's binary_logloss: 0.218863\u001b[0m\n",
      "\u001b[34m[169]#011train's auc: 0.996399#011train's binary_logloss: 0.217644\u001b[0m\n",
      "\u001b[34m[170]#011train's auc: 0.996421#011train's binary_logloss: 0.216627\u001b[0m\n",
      "\u001b[32m[168]#011train's auc: 0.996302#011train's binary_logloss: 0.219022\u001b[0m\n",
      "\u001b[32m[169]#011train's auc: 0.996355#011train's binary_logloss: 0.217802\u001b[0m\n",
      "\u001b[32m[170]#011train's auc: 0.996376#011train's binary_logloss: 0.216787\u001b[0m\n",
      "\u001b[35m[170]#011val's auc: 0.996355#011val's binary_logloss: 0.216682#011train's auc: 0.996404#011train's binary_logloss: 0.216549\u001b[0m\n",
      "\u001b[35m[171]#011val's auc: 0.996395#011val's binary_logloss: 0.215588#011train's auc: 0.996446#011train's binary_logloss: 0.215449\u001b[0m\n",
      "\u001b[36m[171]#011train's auc: 0.996564#011train's binary_logloss: 0.215019\u001b[0m\n",
      "\u001b[36m[172]#011train's auc: 0.996581#011train's binary_logloss: 0.213849\u001b[0m\n",
      "\u001b[34m[171]#011train's auc: 0.996463#011train's binary_logloss: 0.21553\u001b[0m\n",
      "\u001b[34m[172]#011train's auc: 0.996481#011train's binary_logloss: 0.214363\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:38,855 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:42896 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[171]#011train's auc: 0.996417#011train's binary_logloss: 0.215691\u001b[0m\n",
      "\u001b[32m[172]#011train's auc: 0.996434#011train's binary_logloss: 0.214522\u001b[0m\n",
      "\u001b[32m[173]#011train's auc: 0.996493#011train's binary_logloss: 0.213243\u001b[0m\n",
      "\u001b[35m[172]#011val's auc: 0.996413#011val's binary_logloss: 0.214421#011train's auc: 0.996463#011train's binary_logloss: 0.214283\u001b[0m\n",
      "\u001b[35m[173]#011val's auc: 0.99647#011val's binary_logloss: 0.213143#011train's auc: 0.996519#011train's binary_logloss: 0.213005\u001b[0m\n",
      "\u001b[35m[174]#011val's auc: 0.996515#011val's binary_logloss: 0.212107#011train's auc: 0.996566#011train's binary_logloss: 0.211969\u001b[0m\n",
      "\u001b[36m[173]#011train's auc: 0.996637#011train's binary_logloss: 0.212569\u001b[0m\n",
      "\u001b[36m[174]#011train's auc: 0.996685#011train's binary_logloss: 0.211531\u001b[0m\n",
      "\u001b[36m[175]#011train's auc: 0.996706#011train's binary_logloss: 0.210434\u001b[0m\n",
      "\u001b[34m[173]#011train's auc: 0.996538#011train's binary_logloss: 0.213085\u001b[0m\n",
      "\u001b[34m[174]#011train's auc: 0.996587#011train's binary_logloss: 0.212044\u001b[0m\n",
      "\u001b[34m[175]#011train's auc: 0.996608#011train's binary_logloss: 0.210949\u001b[0m\n",
      "\u001b[32m[174]#011train's auc: 0.996542#011train's binary_logloss: 0.212205\u001b[0m\n",
      "\u001b[32m[175]#011train's auc: 0.996563#011train's binary_logloss: 0.21111\u001b[0m\n",
      "\u001b[35m[175]#011val's auc: 0.996536#011val's binary_logloss: 0.211013#011train's auc: 0.996586#011train's binary_logloss: 0.210877\u001b[0m\n",
      "\u001b[35m[176]#011val's auc: 0.996567#011val's binary_logloss: 0.209925#011train's auc: 0.996617#011train's binary_logloss: 0.209786\u001b[0m\n",
      "\u001b[36m[176]#011train's auc: 0.996738#011train's binary_logloss: 0.20934\u001b[0m\n",
      "\u001b[36m[177]#011train's auc: 0.996748#011train's binary_logloss: 0.208189\u001b[0m\n",
      "\u001b[34m[176]#011train's auc: 0.996639#011train's binary_logloss: 0.209857\u001b[0m\n",
      "\u001b[34m[177]#011train's auc: 0.99665#011train's binary_logloss: 0.208704\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:40,855 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:42900 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[176]#011train's auc: 0.996594#011train's binary_logloss: 0.210019\u001b[0m\n",
      "\u001b[32m[177]#011train's auc: 0.996605#011train's binary_logloss: 0.208866\u001b[0m\n",
      "\u001b[32m[178]#011train's auc: 0.996628#011train's binary_logloss: 0.207811\u001b[0m\n",
      "\u001b[35m[177]#011val's auc: 0.996578#011val's binary_logloss: 0.208775#011train's auc: 0.996628#011train's binary_logloss: 0.208631\u001b[0m\n",
      "\u001b[35m[178]#011val's auc: 0.996601#011val's binary_logloss: 0.207721#011train's auc: 0.99665#011train's binary_logloss: 0.207577\u001b[0m\n",
      "\u001b[35m[179]#011val's auc: 0.996655#011val's binary_logloss: 0.206612#011train's auc: 0.996705#011train's binary_logloss: 0.206468\u001b[0m\n",
      "\u001b[36m[178]#011train's auc: 0.996769#011train's binary_logloss: 0.207137\u001b[0m\n",
      "\u001b[36m[179]#011train's auc: 0.996823#011train's binary_logloss: 0.206028\u001b[0m\n",
      "\u001b[36m[180]#011train's auc: 0.996869#011train's binary_logloss: 0.20496\u001b[0m\n",
      "\u001b[34m[178]#011train's auc: 0.996671#011train's binary_logloss: 0.207651\u001b[0m\n",
      "\u001b[34m[179]#011train's auc: 0.996728#011train's binary_logloss: 0.20654\u001b[0m\n",
      "\u001b[34m[180]#011train's auc: 0.996777#011train's binary_logloss: 0.205471\u001b[0m\n",
      "\u001b[32m[179]#011train's auc: 0.996684#011train's binary_logloss: 0.206703\u001b[0m\n",
      "\u001b[32m[180]#011train's auc: 0.996733#011train's binary_logloss: 0.205633\u001b[0m\n",
      "\u001b[35m[180]#011val's auc: 0.996706#011val's binary_logloss: 0.205537#011train's auc: 0.996752#011train's binary_logloss: 0.2054\u001b[0m\n",
      "\u001b[35m[181]#011val's auc: 0.996749#011val's binary_logloss: 0.204307#011train's auc: 0.996793#011train's binary_logloss: 0.20417\u001b[0m\n",
      "\u001b[35m[182]#011val's auc: 0.996791#011val's binary_logloss: 0.203232#011train's auc: 0.996834#011train's binary_logloss: 0.203094\u001b[0m\n",
      "\u001b[36m[181]#011train's auc: 0.99691#011train's binary_logloss: 0.20373\u001b[0m\n",
      "\u001b[36m[182]#011train's auc: 0.996951#011train's binary_logloss: 0.202653\u001b[0m\n",
      "\u001b[34m[181]#011train's auc: 0.996818#011train's binary_logloss: 0.204242\u001b[0m\n",
      "\u001b[34m[182]#011train's auc: 0.996861#011train's binary_logloss: 0.203166\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:42,857 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:42914 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[181]#011train's auc: 0.996775#011train's binary_logloss: 0.204404\u001b[0m\n",
      "\u001b[32m[182]#011train's auc: 0.996817#011train's binary_logloss: 0.20333\u001b[0m\n",
      "\u001b[32m[183]#011train's auc: 0.996883#011train's binary_logloss: 0.202445\u001b[0m\n",
      "\u001b[35m[183]#011val's auc: 0.996867#011val's binary_logloss: 0.202345#011train's auc: 0.996902#011train's binary_logloss: 0.20221\u001b[0m\n",
      "\u001b[35m[184]#011val's auc: 0.996881#011val's binary_logloss: 0.201281#011train's auc: 0.996916#011train's binary_logloss: 0.201148\u001b[0m\n",
      "\u001b[36m[183]#011train's auc: 0.997017#011train's binary_logloss: 0.201771\u001b[0m\n",
      "\u001b[36m[184]#011train's auc: 0.997032#011train's binary_logloss: 0.200705\u001b[0m\n",
      "\u001b[36m[185]#011train's auc: 0.997048#011train's binary_logloss: 0.199641\u001b[0m\n",
      "\u001b[34m[183]#011train's auc: 0.996919#011train's binary_logloss: 0.202284\u001b[0m\n",
      "\u001b[34m[184]#011train's auc: 0.996933#011train's binary_logloss: 0.201222\u001b[0m\n",
      "\u001b[34m[185]#011train's auc: 0.996949#011train's binary_logloss: 0.200159\u001b[0m\n",
      "\u001b[32m[184]#011train's auc: 0.996897#011train's binary_logloss: 0.201382\u001b[0m\n",
      "\u001b[32m[185]#011train's auc: 0.996913#011train's binary_logloss: 0.200319\u001b[0m\n",
      "\u001b[35m[185]#011val's auc: 0.996897#011val's binary_logloss: 0.200217#011train's auc: 0.996931#011train's binary_logloss: 0.200086\u001b[0m\n",
      "\u001b[35m[186]#011val's auc: 0.996923#011val's binary_logloss: 0.199226#011train's auc: 0.996956#011train's binary_logloss: 0.199094\u001b[0m\n",
      "\u001b[35m[187]#011val's auc: 0.996944#011val's binary_logloss: 0.198232#011train's auc: 0.996977#011train's binary_logloss: 0.198099\u001b[0m\n",
      "\u001b[36m[186]#011train's auc: 0.997073#011train's binary_logloss: 0.19865\u001b[0m\n",
      "\u001b[36m[187]#011train's auc: 0.997094#011train's binary_logloss: 0.197652\u001b[0m\n",
      "\u001b[36m[188]#011train's auc: 0.997122#011train's binary_logloss: 0.196605\u001b[0m\n",
      "\u001b[34m[186]#011train's auc: 0.996973#011train's binary_logloss: 0.199168\u001b[0m\n",
      "\u001b[34m[187]#011train's auc: 0.996994#011train's binary_logloss: 0.198172\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:44,860 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:42930 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[186]#011train's auc: 0.996938#011train's binary_logloss: 0.199328\u001b[0m\n",
      "\u001b[32m[187]#011train's auc: 0.996959#011train's binary_logloss: 0.198333\u001b[0m\n",
      "\u001b[32m[188]#011train's auc: 0.996988#011train's binary_logloss: 0.197288\u001b[0m\n",
      "\u001b[35m[188]#011val's auc: 0.996971#011val's binary_logloss: 0.197188#011train's auc: 0.997003#011train's binary_logloss: 0.197058\u001b[0m\n",
      "\u001b[35m[189]#011val's auc: 0.997003#011val's binary_logloss: 0.19618#011train's auc: 0.997037#011train's binary_logloss: 0.19605\u001b[0m\n",
      "\u001b[36m[189]#011train's auc: 0.997153#011train's binary_logloss: 0.195598\u001b[0m\n",
      "\u001b[36m[190]#011train's auc: 0.997171#011train's binary_logloss: 0.194551\u001b[0m\n",
      "\u001b[34m[188]#011train's auc: 0.997022#011train's binary_logloss: 0.197127\u001b[0m\n",
      "\u001b[34m[189]#011train's auc: 0.997056#011train's binary_logloss: 0.196118\u001b[0m\n",
      "\u001b[34m[190]#011train's auc: 0.997074#011train's binary_logloss: 0.195073\u001b[0m\n",
      "\u001b[32m[189]#011train's auc: 0.997021#011train's binary_logloss: 0.196281\u001b[0m\n",
      "\u001b[32m[190]#011train's auc: 0.997039#011train's binary_logloss: 0.195237\u001b[0m\n",
      "\u001b[35m[190]#011val's auc: 0.997021#011val's binary_logloss: 0.195137#011train's auc: 0.997055#011train's binary_logloss: 0.195008\u001b[0m\n",
      "\u001b[35m[191]#011val's auc: 0.997035#011val's binary_logloss: 0.194147#011train's auc: 0.997069#011train's binary_logloss: 0.194014\u001b[0m\n",
      "\u001b[35m[192]#011val's auc: 0.99708#011val's binary_logloss: 0.193012#011train's auc: 0.997113#011train's binary_logloss: 0.192878\u001b[0m\n",
      "\u001b[36m[191]#011train's auc: 0.997186#011train's binary_logloss: 0.193555\u001b[0m\n",
      "\u001b[36m[192]#011train's auc: 0.997229#011train's binary_logloss: 0.192422\u001b[0m\n",
      "\u001b[36m[193]#011train's auc: 0.997247#011train's binary_logloss: 0.191464\u001b[0m\n",
      "\u001b[34m[191]#011train's auc: 0.997089#011train's binary_logloss: 0.194079\u001b[0m\n",
      "\u001b[34m[192]#011train's auc: 0.997133#011train's binary_logloss: 0.192945\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:46,861 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:42938 closed before handshake completed\u001b[0m\n",
      "\u001b[34m[193]#011train's auc: 0.99715#011train's binary_logloss: 0.191991\u001b[0m\n",
      "\u001b[32m[191]#011train's auc: 0.997054#011train's binary_logloss: 0.194243\u001b[0m\n",
      "\u001b[32m[192]#011train's auc: 0.997098#011train's binary_logloss: 0.19311\u001b[0m\n",
      "\u001b[32m[193]#011train's auc: 0.997117#011train's binary_logloss: 0.192155\u001b[0m\n",
      "\u001b[35m[193]#011val's auc: 0.997099#011val's binary_logloss: 0.192058#011train's auc: 0.997132#011train's binary_logloss: 0.191923\u001b[0m\n",
      "\u001b[35m[194]#011val's auc: 0.997112#011val's binary_logloss: 0.191126#011train's auc: 0.997146#011train's binary_logloss: 0.190992\u001b[0m\n",
      "\u001b[35m[195]#011val's auc: 0.997128#011val's binary_logloss: 0.190141#011train's auc: 0.997161#011train's binary_logloss: 0.190005\u001b[0m\n",
      "\u001b[36m[194]#011train's auc: 0.997261#011train's binary_logloss: 0.190533\u001b[0m\n",
      "\u001b[36m[195]#011train's auc: 0.997278#011train's binary_logloss: 0.189542\u001b[0m\n",
      "\u001b[34m[194]#011train's auc: 0.997165#011train's binary_logloss: 0.191059\u001b[0m\n",
      "\u001b[34m[195]#011train's auc: 0.997182#011train's binary_logloss: 0.190072\u001b[0m\n",
      "\u001b[32m[194]#011train's auc: 0.997132#011train's binary_logloss: 0.191223\u001b[0m\n",
      "\u001b[32m[195]#011train's auc: 0.997147#011train's binary_logloss: 0.190236\u001b[0m\n",
      "\u001b[32m[196]#011train's auc: 0.997176#011train's binary_logloss: 0.189145\u001b[0m\n",
      "\u001b[35m[196]#011val's auc: 0.997158#011val's binary_logloss: 0.189047#011train's auc: 0.997189#011train's binary_logloss: 0.188911\u001b[0m\n",
      "\u001b[35m[197]#011val's auc: 0.997178#011val's binary_logloss: 0.188043#011train's auc: 0.997211#011train's binary_logloss: 0.187907\u001b[0m\n",
      "\u001b[36m[196]#011train's auc: 0.997305#011train's binary_logloss: 0.188451\u001b[0m\n",
      "\u001b[36m[197]#011train's auc: 0.997326#011train's binary_logloss: 0.187445\u001b[0m\n",
      "\u001b[36m[198]#011train's auc: 0.997358#011train's binary_logloss: 0.186493\u001b[0m\n",
      "\u001b[34m[196]#011train's auc: 0.997209#011train's binary_logloss: 0.188981\u001b[0m\n",
      "\u001b[34m[197]#011train's auc: 0.997232#011train's binary_logloss: 0.187974\u001b[0m\n",
      "\u001b[34m[198]#011train's auc: 0.997264#011train's binary_logloss: 0.187019\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:48,864 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:43602 closed before handshake completed\u001b[0m\n",
      "\u001b[32m[197]#011train's auc: 0.997198#011train's binary_logloss: 0.18814\u001b[0m\n",
      "\u001b[32m[198]#011train's auc: 0.997231#011train's binary_logloss: 0.187185\u001b[0m\n",
      "\u001b[35m[198]#011val's auc: 0.997212#011val's binary_logloss: 0.187087#011train's auc: 0.997245#011train's binary_logloss: 0.186952\u001b[0m\n",
      "\u001b[35m[199]#011val's auc: 0.997262#011val's binary_logloss: 0.18616#011train's auc: 0.997294#011train's binary_logloss: 0.186027\u001b[0m\n",
      "\u001b[36m[199]#011train's auc: 0.997406#011train's binary_logloss: 0.185569\u001b[0m\n",
      "\u001b[36m[200]#011train's auc: 0.997438#011train's binary_logloss: 0.184618\u001b[0m\n",
      "\u001b[36m[LightGBM] [Info] Finished linking network in 69.007473 seconds\u001b[0m\n",
      "\u001b[34m[199]#011train's auc: 0.997317#011train's binary_logloss: 0.186092\u001b[0m\n",
      "\u001b[34m[200]#011train's auc: 0.997352#011train's binary_logloss: 0.185137\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Finished linking network in 38.330932 seconds\u001b[0m\n",
      "\u001b[32m[199]#011train's auc: 0.997284#011train's binary_logloss: 0.186258\u001b[0m\n",
      "\u001b[32m[200]#011train's auc: 0.997319#011train's binary_logloss: 0.185303\u001b[0m\n",
      "\u001b[32m[LightGBM] [Info] Finished linking network in 34.152906 seconds\u001b[0m\n",
      "\u001b[35m[200]#011val's auc: 0.997296#011val's binary_logloss: 0.185205#011train's auc: 0.997329#011train's binary_logloss: 0.185072\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Finished linking network in 53.474405 seconds\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:50,865 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:43604 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:52,867 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:43608 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:54,869 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:43612 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:56,871 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:43614 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:34:58,873 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:56496 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:00,875 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:56506 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:02,877 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:56512 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:04,879 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:56514 closed before handshake completed\u001b[0m\n",
      "\u001b[34mINFO:root:Done training\u001b[0m\n",
      "\u001b[34mINFO:root:Saving model...\u001b[0m\n",
      "\u001b[34mINFO:root:Info file not found at '_input_model_extracted/__models_info__.json'.\u001b[0m\n",
      "\u001b[34m{'1': 172.0, '2': 192.0, '3': 28.0, '4': 14.0, '5': 77.0, '6': 549.0, '7': 759.0, '8': 916.0, '9': 1004.0, '10': 1303.0, '11': 298.0, '12': 1419.0, '13': 1316.0, '14': 1209.0, '15': 975.0, '16': 185.0, '17': 865.0, '18': 878.0, '19': 1041.0}\u001b[0m\n",
      "\u001b[34mINFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\u001b[0m\n",
      "\u001b[34mINFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:06,881 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:56522 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:08,881 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:46784 closed before handshake completed\u001b[0m\n",
      "\u001b[36m2024-10-27 04:35:09,699 - distributed.worker - INFO - Stopping worker at tcp://10.2.91.117:9000. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[36m2024-10-27 04:35:09,702 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.2.91.117:40795'. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[36m2024-10-27 04:35:09,703 - distributed.core - INFO - Connection to tcp://10.2.82.8:8786 has been closed.\u001b[0m\n",
      "\u001b[36m2024-10-27 04:35:09,705 - distributed.nanny - INFO - Worker closed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,692 - distributed.scheduler - INFO - Retiring worker tcp://10.2.82.8:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,693 - distributed.scheduler - INFO - Retiring worker tcp://10.2.91.117:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,693 - distributed.scheduler - INFO - Retiring worker tcp://10.2.96.89:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,693 - distributed.scheduler - INFO - Retiring worker tcp://10.2.84.151:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,693 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.2.84.151:9000; no unique keys need to be moved away.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,693 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.2.91.117:9000; no unique keys need to be moved away.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,693 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.2.96.89:9000; no unique keys need to be moved away.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,693 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.2.82.8:9000; no unique keys need to be moved away.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,693 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.2.82.8:9000', status: closing_gracefully, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,693 - distributed.core - INFO - Removing comms to tcp://10.2.82.8:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,694 - distributed.scheduler - INFO - Retired worker tcp://10.2.82.8:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,694 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.2.91.117:9000', status: closing_gracefully, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,694 - distributed.core - INFO - Removing comms to tcp://10.2.91.117:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,694 - distributed.scheduler - INFO - Retired worker tcp://10.2.91.117:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,694 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.2.96.89:9000', status: closing_gracefully, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,694 - distributed.core - INFO - Removing comms to tcp://10.2.96.89:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,694 - distributed.scheduler - INFO - Retired worker tcp://10.2.96.89:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,694 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.2.84.151:9000', status: closing_gracefully, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,695 - distributed.core - INFO - Removing comms to tcp://10.2.84.151:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,695 - distributed.scheduler - INFO - Lost all workers\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,695 - distributed.scheduler - INFO - Retired worker tcp://10.2.84.151:9000\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,699 - distributed.worker - INFO - Stopping worker at tcp://10.2.82.8:9000. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,702 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.2.82.8:36973'. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,703 - distributed.core - INFO - Received 'close-stream' from tcp://10.2.84.151:38700; closing.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,703 - distributed.core - INFO - Connection to tcp://10.2.82.8:8786 has been closed.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,704 - distributed.core - INFO - Received 'close-stream' from tcp://10.2.96.89:55634; closing.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,704 - distributed.core - INFO - Received 'close-stream' from tcp://10.2.82.8:34140; closing.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,704 - distributed.core - INFO - Received 'close-stream' from tcp://10.2.91.117:57158; closing.\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:09,705 - distributed.nanny - INFO - Worker closed\u001b[0m\n",
      "\u001b[32m2024-10-27 04:35:09,699 - distributed.worker - INFO - Stopping worker at tcp://10.2.96.89:9000. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[32m2024-10-27 04:35:09,702 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.2.96.89:33217'. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[32m2024-10-27 04:35:09,703 - distributed.core - INFO - Connection to tcp://10.2.82.8:8786 has been closed.\u001b[0m\n",
      "\u001b[32m2024-10-27 04:35:09,705 - distributed.nanny - INFO - Worker closed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:10,884 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:46798 closed before handshake completed\u001b[0m\n",
      "\u001b[36m2024-10-27 04:35:11,706 - distributed.nanny - ERROR - Worker process died unexpectedly\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:11,707 - distributed.nanny - ERROR - Worker process died unexpectedly\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:11,895 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.2.82.8:36973'. Reason: nanny-close-gracefully\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:11,896 - distributed.dask_worker - INFO - End worker\u001b[0m\n",
      "\u001b[32m2024-10-27 04:35:11,706 - distributed.nanny - ERROR - Worker process died unexpectedly\u001b[0m\n",
      "\u001b[32m2024-10-27 04:35:11,906 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.2.96.89:33217'. Reason: nanny-close-gracefully\u001b[0m\n",
      "\u001b[32m2024-10-27 04:35:11,907 - distributed.dask_worker - INFO - End worker\u001b[0m\n",
      "\u001b[32m2024-10-27 04:35:11,972 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[36m2024-10-27 04:35:11,896 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.2.91.117:40795'. Reason: nanny-close-gracefully\u001b[0m\n",
      "\u001b[36m2024-10-27 04:35:11,897 - distributed.dask_worker - INFO - End worker\u001b[0m\n",
      "\u001b[36m2024-10-27 04:35:11,959 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:12,886 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:46814 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:14,888 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:46816 closed before handshake completed\u001b[0m\n",
      "\u001b[35m2024-10-27 04:35:09,699 - distributed.worker - INFO - Stopping worker at tcp://10.2.84.151:9000. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[35m2024-10-27 04:35:09,702 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.2.84.151:33667'. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[35m2024-10-27 04:35:09,703 - distributed.core - INFO - Connection to tcp://10.2.82.8:8786 has been closed.\u001b[0m\n",
      "\u001b[35m2024-10-27 04:35:09,705 - distributed.nanny - INFO - Worker closed\u001b[0m\n",
      "\u001b[35m2024-10-27 04:35:11,706 - distributed.nanny - ERROR - Worker process died unexpectedly\u001b[0m\n",
      "\u001b[35m2024-10-27 04:35:11,903 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.2.84.151:33667'. Reason: nanny-close-gracefully\u001b[0m\n",
      "\u001b[35m2024-10-27 04:35:11,904 - distributed.dask_worker - INFO - End worker\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:16,891 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:46826 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:18,893 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:59968 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:20,895 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:59976 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:22,898 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:59980 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:24,900 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:59994 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:26,902 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:60002 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:28,903 - distributed.comm.tcp - INFO - Connection from tcp://10.2.84.151:45840 closed before handshake completed\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:29,717 - distributed.scheduler - INFO - Scheduler closing...\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:29,717 - distributed.scheduler - INFO - Scheduler closing all comms\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:29,718 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.2.82.8:8786'\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:29,718 - distributed.scheduler - INFO - End scheduler\u001b[0m\n",
      "\u001b[34m2024-10-27 04:35:30,165 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35mINFO:root:Received a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[35m2024-10-27 04:35:31,178 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-10-27 04:35:48 Uploading - Uploading generated training model\n",
      "2024-10-27 04:35:48 Completed - Resource retained for reuse\n",
      "Training seconds: 1004\n",
      "Billable seconds: 1004\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "import random\n",
    "\n",
    "# For single instance training, set to 1\n",
    "instance_count = 4\n",
    "\n",
    "if instance_count >1:\n",
    "    hyperparameters[\"tree_learner\"] = \"voting\"  # use AllReduce method for distributed training\n",
    "\n",
    "    del hyperparameters[\n",
    "        \"early_stopping_rounds\"\n",
    "    ]  # current distributed training with early stopping has some issues. See https://github.com/microsoft/SynapseML/issues/728#issuecomment-1221599961\n",
    "    # thus it is disabled for distributed training.\n",
    "\n",
    "# SageMaker Mlflow tracking arn\n",
    "tracking_server_arn = \"arn:aws:sagemaker:us-east-1:12345678:mlflow-tracking-server/test\"\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tabular_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir= \"model_cat\", # Local dir path to source code\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"train.py\", # training logic file name\n",
    "    instance_count= instance_count,  ### select the instance count you would like to use for distributed training\n",
    "    volume_size=20,  ### volume_size (int or PipelineVariable): Size in GB of the storage volume to use for storing input and output data during training (default: 30).\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    metric_definitions = [{\"Name\": \"auc\", \"Regex\": \"auc: ([0-9\\\\.]+)\"}],\n",
    "    environment={\"MLFLOW_TRACKING_ARN\": tracking_server_arn},\n",
    "    keep_alive_period_in_seconds = 1000 #Keep instance warm for fast experimentation iteration else experience cold start for each trials (note you will incur cost of warm instances)\n",
    ")\n",
    "\n",
    "if use_amt:\n",
    "    tuner = HyperparameterTuner(\n",
    "        tabular_estimator,\n",
    "        \"auc\",\n",
    "        hyperparameter_ranges_lgb,\n",
    "        [{\"Name\": \"auc\", \"Regex\": \"auc: ([0-9\\\\.]+)\"}],\n",
    "        max_jobs=6, # Total number of jobs (candidates)\n",
    "        max_parallel_jobs=3, # Number of jobs (candidates) to run in parallel\n",
    "        objective_type=\"Maximize\",\n",
    "        strategy = \"Bayesian\"\n",
    "    )\n",
    "\n",
    "    tuner.fit(\n",
    "        {\n",
    "            \"train\": training_dataset_s3_path,\n",
    "            \"validation\": validation_dataset_s3_path,\n",
    "        },\n",
    "        logs=True,\n",
    "    )\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    tabular_estimator.fit(\n",
    "        {\n",
    "            \"train\": training_dataset_s3_path,\n",
    "            \"validation\": validation_dataset_s3_path,\n",
    "        },\n",
    "        logs=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199060d",
   "metadata": {},
   "source": [
    "### 3.5. Deploy and Run Inference on the Trained Tabular Model\n",
    "\n",
    "---\n",
    "\n",
    "In this section, you learn how to query an existing endpoint and make predictions of the examples you input. For each example, the model will output the probability of the sample for each class in the model. \n",
    "Next, the predicted class label is obtained by taking the class label with the maximum probability over others.\n",
    "\n",
    "\n",
    "We start by retrieving the artifacts and deploy the `tabular_estimator` that we trained.\n",
    "\n",
    "SageMaker Inference Directory Setup with Custom Serving Logic:\n",
    "\n",
    "    - Create a root project directory.\n",
    "    - Place main inference script (e.g., inference.py) in root. Include Sagemaker specific model_fn(), input_fn(), predict_fn(), output_fn() in inference script.\n",
    "    - Include any additional helper scripts or modules.\n",
    "    - Add requirements.txt for dependencies.\n",
    "[For detailed information on sagemaker inference setup](https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_inferenece_script_mode.html) \n",
    "\n",
    "Example structure:\n",
    "```\n",
    "inference_project/\n",
    "    ├── inference.py\n",
    "    ├── requirements.txt\n",
    "    ├── utils.py\n",
    "```\n",
    "SageMaker model creation:\n",
    "```\n",
    "model = Model(\n",
    "    model_data='s3://bucket/model.tar.gz',\n",
    "    entry_point='inference.py',\n",
    "    source_dir='path/to/inference_project',\n",
    "    ...\n",
    ")\n",
    "```\n",
    "Key points:\n",
    "\n",
    "    inference.py should handle loading model and making predictions.\n",
    "    Include all necessary dependencies in requirements.txt.\n",
    "    SageMaker packages entire directory content.\n",
    "    Model artifacts are typically stored separately in S3.\n",
    "    Implement SageMaker-specific functions (model_fn, etc.) as needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f480f9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-10-27 03:06:04 Starting - Preparing the instances for training\n",
      "2024-10-27 03:06:04 Downloading - Downloading the training image\n",
      "2024-10-27 03:06:04 Training - Training image download completed. Training in progress.\n",
      "2024-10-27 03:06:04 Uploading - Uploading generated training model\n",
      "2024-10-27 03:06:04 Completed - Resource reused by training job: pytorch-training-241027-0300-006-86c2d616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-east-1-715253196401/jumpstart-example-tabular-training/output_lgb/pytorch-training-241027-0300-002-ee0339e7/output/model.tar.gz), script artifact (model_cat/js_inference_code), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-27-03-18-48-300/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2024-10-27-03-18-48-300\n",
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-distr-lgb-g1729999123\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-distr-lgb-g1729999123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "endpoint_name = \"jumpstart-distr-lgb-g\" + str(int(time.time()))\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "predictor = (tuner if use_amt else tabular_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=\"model_cat/js_inference_code\",\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a8372",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we read the customer churn test data into pandas data frame, prepare the ground truth target and predicting features to send into the endpoint. \n",
    "\n",
    "Below is the screenshot of the first 5 examples in the test set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "22675cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe test dataset contains 22500 examples and 20 columns.\u001b[0m\n",
      "\n",
      "\u001b[1mThe first 5 observations of the data: \u001b[0m \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>Feature_10</th>\n",
       "      <th>Feature_11</th>\n",
       "      <th>Feature_12</th>\n",
       "      <th>Feature_13</th>\n",
       "      <th>Feature_14</th>\n",
       "      <th>Feature_15</th>\n",
       "      <th>Feature_16</th>\n",
       "      <th>Feature_17</th>\n",
       "      <th>Feature_18</th>\n",
       "      <th>Feature_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>145</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.291344</td>\n",
       "      <td>3</td>\n",
       "      <td>6.916209</td>\n",
       "      <td>4.393400</td>\n",
       "      <td>3</td>\n",
       "      <td>8.056665</td>\n",
       "      <td>7.192453</td>\n",
       "      <td>3</td>\n",
       "      <td>5.380750</td>\n",
       "      <td>3.830169</td>\n",
       "      <td>5</td>\n",
       "      <td>6.442558</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>74</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3.458087</td>\n",
       "      <td>4</td>\n",
       "      <td>3.128629</td>\n",
       "      <td>6.648148</td>\n",
       "      <td>1</td>\n",
       "      <td>4.442749</td>\n",
       "      <td>5.827775</td>\n",
       "      <td>4</td>\n",
       "      <td>3.160940</td>\n",
       "      <td>3.158770</td>\n",
       "      <td>5</td>\n",
       "      <td>7.818480</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>184</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.455195</td>\n",
       "      <td>3</td>\n",
       "      <td>4.531812</td>\n",
       "      <td>3.691651</td>\n",
       "      <td>0</td>\n",
       "      <td>8.389032</td>\n",
       "      <td>4.682667</td>\n",
       "      <td>1</td>\n",
       "      <td>7.673444</td>\n",
       "      <td>4.283764</td>\n",
       "      <td>6</td>\n",
       "      <td>4.824685</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.174580</td>\n",
       "      <td>0</td>\n",
       "      <td>6.888873</td>\n",
       "      <td>5.252473</td>\n",
       "      <td>0</td>\n",
       "      <td>6.673767</td>\n",
       "      <td>1.898277</td>\n",
       "      <td>5</td>\n",
       "      <td>6.162483</td>\n",
       "      <td>4.460356</td>\n",
       "      <td>4</td>\n",
       "      <td>5.001799</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.558506</td>\n",
       "      <td>7</td>\n",
       "      <td>1.671145</td>\n",
       "      <td>5.553033</td>\n",
       "      <td>0</td>\n",
       "      <td>3.440175</td>\n",
       "      <td>3.947812</td>\n",
       "      <td>3</td>\n",
       "      <td>5.565771</td>\n",
       "      <td>5.964464</td>\n",
       "      <td>6</td>\n",
       "      <td>2.683223</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0       0         26        145         16          0          0          0   \n",
       "1       1         35         74         20          0          1          7   \n",
       "2       1         37        184         21          0          0          0   \n",
       "3       1          1        146          0          1          0          0   \n",
       "4       1          3         93          2          1          0          0   \n",
       "\n",
       "   Feature_7  Feature_8  Feature_9  Feature_10  Feature_11  Feature_12  \\\n",
       "0  10.291344          3   6.916209    4.393400           3    8.056665   \n",
       "1   3.458087          4   3.128629    6.648148           1    4.442749   \n",
       "2  10.455195          3   4.531812    3.691651           0    8.389032   \n",
       "3   7.174580          0   6.888873    5.252473           0    6.673767   \n",
       "4   2.558506          7   1.671145    5.553033           0    3.440175   \n",
       "\n",
       "   Feature_13  Feature_14  Feature_15  Feature_16  Feature_17  Feature_18  \\\n",
       "0    7.192453           3    5.380750    3.830169           5    6.442558   \n",
       "1    5.827775           4    3.160940    3.158770           5    7.818480   \n",
       "2    4.682667           1    7.673444    4.283764           6    4.824685   \n",
       "3    1.898277           5    6.162483    4.460356           4    5.001799   \n",
       "4    3.947812           3    5.565771    5.964464           6    2.683223   \n",
       "\n",
       "   Feature_19  \n",
       "0           5  \n",
       "1           5  \n",
       "2           3  \n",
       "3           6  \n",
       "4           7  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read the data\n",
    "test_data_file_name = \"test.csv\"\n",
    "test_data = pd.read_csv(test_data_file_name, header=None)\n",
    "test_data.columns = [\"Target\"] + [f\"Feature_{i}\" for i in range(1, test_data.shape[1])]\n",
    "\n",
    "num_examples, num_columns = test_data.shape\n",
    "print(\n",
    "    f\"{bold}The test dataset contains {num_examples} examples and {num_columns} columns.{unbold}\\n\"\n",
    ")\n",
    "\n",
    "# prepare the ground truth target and predicting features to send into the endpoint.\n",
    "ground_truth_label, features = test_data.iloc[:, :1], test_data.iloc[:, 1:]\n",
    "\n",
    "print(f\"{bold}The first 5 observations of the data: {unbold} \\n\")\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbda7e4",
   "metadata": {},
   "source": [
    "---\n",
    "The following code queries the endpoint you have created to get the prediction for each test example. \n",
    "The `query_endpoint()` function returns an array-like of shape (num_examples, num_classes), where each row indicates \n",
    "the probability of the example for each class in the model. The num_classes is 2 in above test data. \n",
    "Next, the predicted class label is obtained by taking the class label with the maximum probability over others for each example. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b7b0bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_type = \"text/csv\"\n",
    "\n",
    "\n",
    "def query_endpoint(encoded_tabular_data, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=content_type,\n",
    "        Body=encoded_tabular_data,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    predicted_probabilities = model_predictions[\"probabilities\"]\n",
    "    return np.array(predicted_probabilities)\n",
    "\n",
    "\n",
    "# split the test data into smaller size of batches to query the endpoint if test data has large size.\n",
    "batch_size = 1500\n",
    "predict_prob = []\n",
    "for i in np.arange(0, num_examples, step=batch_size):\n",
    "    query_response_batch = query_endpoint(\n",
    "        features.iloc[i : (i + batch_size), :].to_csv(header=False, index=False).encode(\"utf-8\"),\n",
    "        endpoint_name,\n",
    "    )\n",
    "    predict_prob_batch = parse_response(query_response_batch)  # prediction probability per batch\n",
    "    predict_prob.append(predict_prob_batch)\n",
    "\n",
    "\n",
    "predict_prob = np.concatenate(predict_prob, axis=0)\n",
    "predict_label = np.argmax(predict_prob, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c94ae",
   "metadata": {},
   "source": [
    "## 3.6. Evaluate the Prediction Results Returned from the Endpoint\n",
    "\n",
    "---\n",
    "We evaluate the predictions results returned from the endpoint by following two ways.\n",
    "\n",
    "* Visualize the predictions results by plotting the confusion matrix.\n",
    "\n",
    "* Measure the prediction results quantitatively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "94720689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoUAAAKqCAYAAABM0yQ3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABD9ElEQVR4nO3dd3gU5cL38d+k100hEkKLQZCmIkWldxAUpamgCCh4EEQFRD3YAIVXBX2woUgRBM9BEKUJKM1E6QeQXkQ0QOiSkGwahCTz/sHJnsT0TVmSfD/XlevZzN4zc+/yHPg6uzNjmKZpCgAAABWak6MnAAAAAMcjCgEAAEAUAgAAgCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEcIM5cOCAHnnkEYWEhMjFxUWGYejOO+902HwiIiJkGIYMw3DYHJCzEydO2P5sTpw44ejpAGUeUQiUQ2lpafrmm280aNAg3XrrrfL395ebm5sqV66s1q1b65VXXtHBgwcdPc1sIiMj1apVKy1ZskTnz5+Xn5+fgoODFRQU5OiplUkZwWQYhurXr5/v+J07d2ZZ54knnijW+ezdu1cTJ07Uhx9+WKzbBVA8XBw9AQDFa/v27Ro8eLCOHTtmW+bq6ipfX19FR0dry5Yt2rJli95991316dNHX3/9tdzc3Bw44/+ZOXOm4uPjVbt2bYWHh6t69eqOnpK8vLxUt25dR0+jyI4ePapt27apRYsWuY6ZO3duic5h7969evPNNxUaGqrRo0cXeXuurq62PxtXV9cibw+o6DhSCJQj33//vdq3b69jx46pUqVKeuedd3Ts2DGlpKQoOjpaKSkp2rlzp8aNGyeLxaKlS5cqKSnJ0dO2OXDggCSpZ8+eN0QQStLdd9+to0eP6ujRo46eit1uvvlmSdK8efNyHXPlyhUtWrRIhmGoZs2apTSzoqlWrZrtz6ZatWqOng5Q5hGFQDnx+++/6/HHH9fVq1fVoEED7d27V+PGjVOdOnVsY5ydndWsWTO98847ioyMVM+ePR044+wyAtXHx8fBMylfBg0aJMMwtHjx4lz/I2Dp0qWKjY1Vu3btFBYWVsozBHAjIAqBcuL111+X1WqVh4eHli1blu+RtsDAQC1fvlx+fn7Znjt//rxeeuklNWzYUD4+PvL29lbDhg318ssv68KFCzlu7+9f+r9w4YJGjRqlsLAweXh4KDg4WP3798/xiNvNN98swzAUEREhSXrzzTezfLctY/nEiRNlGIbat2+f6+vK78SQHTt2aMCAAbZ5eXt7KzQ0VO3atdOkSZN0+vTpQm3PEe9XYYWFhaldu3ayWq367rvvchyT8dHxk08+mee2kpOTtXLlSv3jH//QnXfeqZtuuknu7u6qWrWqevXqpR9++CHH9QzDsG375MmTWf58DcPQxIkTbWOfeOIJ23caTdPUnDlz1Lp1a1WqVEmGYejLL7+UlPuJJtHR0apevboMw1Dv3r1znE9aWppatWolwzB0xx136MqVK3m+bqBCMAGUeefPnzednJxMSebQoUOLtK2IiAjT39/flGRKMr28vExvb2/b7wEBAeamTZuyrRcZGWkbs2rVKrNy5cq29d3d3W3PWSwWc+/evVnWbdasmRkcHGy6urqakkxvb28zODjY9rNlyxbTNE1zwoQJpiSzXbt2uc4/PDzctq+/+/LLL03DMGzPu7u7mxaLxfa7JHPevHkF3p6j3q+Cyvya5s+fb0oyO3TokG3cyZMnTcMwTF9fXzMxMdFs166dKckcPHhwtrHz5s3L8n55enqaXl5eWZaNHTs223rBwcG299rJySnLn29wcLD53nvv2cYOHjzYlGQOGjTIfOihh2zrBAQEmE5OTrY/o8zvYWRkZJb9RURE2P43MX369Gzzee2112zzP3jwYOHeWKCcIgqBcuDrr7/OEhj2OnXqlC1wGjRoYG7evNn23C+//GLWrVvXlGQGBgaap0+fzrJu5n+gAwICzFatWpk7d+40TdM0r127Zq5fv94MCQkxJZlt2rTJcf8ZMTJhwoQcny9KFCYmJpq+vr6mJPPxxx83jx8/bnsuISHB3LVrl/nSSy+Zq1evLtD2boT3Kz+ZozAxMdG0WCymYRjmn3/+mWXcxIkTTUnmU089ZZqmmWcULlu2zBw2bJgZHh5uXrp0ybb87Nmz5ptvvmkL+xUrVmRbNyMoQ0ND85x3RhT6+PiYLi4u5vvvv2/GxcWZpmma8fHx5tmzZ03TzDsKTdM033jjDVOS6eHhYe7fv9+2PDw83BaMn3/+eZ5zASoSohAoB15//XXbP45nzpyxezvDhw+3Rcq5c+eyPR8VFWU72jNy5Mgsz2X+B7pevXpmUlJStvVXrlxpGxMVFZXt+ZKMwh07dtiOQl67di3X9Qu6PdN0/PuVn78f/XzqqadMSeb48eNtY9LT082wsDBTku2IbF5RmJ/33nvPlGR26tQp23OFjUJJ5scff5zruPyiMDU11WzVqpUt2pOSksxLly6Z1apVMyWZffr0KezLA8o1vlMIlAPR0dG2x4GBgXZtwzRNffPNN5Kk4cOHq0qVKtnGVK9eXcOHD5ckLVq0KNdtjR07Vp6entmWd+/e3Xb5m4wzjUuLv7+/JNnOxC6qsvh+DRkyRJI0f/58maYpSQoPD1dkZKTq1q2rli1bFnkf999/vyRp27ZtSktLK9K2AgIC9PTTT9u9vrOzsxYuXKiAgAAdPnxYo0aN0pAhQ3TmzBnVqFFDc+bMKdL8gPKGKATKgYx/4IsiMjJSMTExkqTOnTvnOq5Lly6SrodoZGRkjmPuueeeHJe7uLjopptukiTbvkrLLbfconr16unatWu65557NGXKFO3du9fucCmL71eLFi1Ur149nTx5Uhs3bpRU8BNMMrtw4YImTJigFi1aqFKlSrY7zxiGoQYNGki6fib55cuXizTfu+66q8jX0KxZs6Zmz54tSZo9e7ZWrlwpJycn/etf/1JAQECRtg2UN0QhUA5kvuOHvfFw8eJF2+O8rvmW+azmzOtk5uvrm+v6Li7Xr5l/7dq1wk6xSJydnbVo0SKFhYXp5MmTGjdunBo3biyLxaIuXbpoxowZhbpmY1l9vzLib968ebJarVq6dKmcnZ01aNCgAq2/bds21atXT2+99Za2b9+umJgYeXp6qnLlytnuPpOYmFikuVauXLlI62fo27ev+vbta/v9pZdeUtu2bYtl20B5QhQC5UDDhg1tj/fs2VPk7RX0Pr9l7X7AjRo10tGjR/Xdd99p2LBhuu2225ScnKwNGzbomWeeUb169ez6mLYsvV8DBw6Us7Ozli1bps8//1zJycnq1q2bQkJC8l03NTVVjz76qGJjY3XnnXdqzZo1slqtio+P14ULF3T+/Hlt377dNr6oR7CdnZ2LtH6GEydOaMOGDbbft2zZUuSPtoHyiCgEyoEOHTrIyen6/5yXLVtm1zYyH5WJiorKdVzm6/hlfLRZWjKOmuV1Tbm4uLg8t+Hm5qY+ffpo5syZOnDggP766y99/vnnCgwMVFRUlAYPHlyguZSF9ysnISEh6tatm5KTk/XGG29IKvhHx9u2bdPJkyfl7OysVatWqXv37tmOcp4/f77Y51wUGSEbFxenW2+9Ve7u7tq8ebMmTZrk6KkBNxyiECgHgoODbR+PLVy4MMt9j/OTcTQnLCzMdpJKxvfNcpJxxKVSpUqlfueLjO+A5RVhO3bsKNQ2K1WqpKefflpTpkyRdP1Ia0FORCkL71duMk44SUlJUVBQkB544IECrZfxvt900025fmSe+Yjc32X8h0txfAe2oCZMmKDt27fLy8tLy5cvt/05T548WZs3by61eQBlAVEIlBOTJ0+Wj4+PkpOT1adPH505cybP8ZcvX1bfvn1tR9YMw1C/fv0kSTNnzszxiM/Zs2c1c+ZMSdKjjz5azK8gf40aNbLNI/PHlBkuXrxoO6ng765evZrntjOf/VuQjy3LwvuVmwceeEAvv/yyxo4dqw8//LDAJ3Nk3P3mwoULOd6p5fTp0/r4449zXd9isUiSYmNjCz9pO4SHh+vdd9+VJH3wwQeqX7++Ro0apfvvv19paWkaMGBAkU+GAcoTohAoJ2699VZ99dVXcnNz06FDh3TnnXdqypQpOn78uG1MWlqa9uzZo/Hjx6tWrVpaunRplm28+uqr8vf3V0xMjDp37qytW7fantuyZYs6d+6s2NhYBQYGaty4caX22jK0bNlSoaGhkq7fCm3Xrl0yTVPp6emKiIhQ+/btlZ6enuO6ixYtUqtWrTRz5kz9+eeftuVpaWlau3at7fW0aNHCdvma/Nzo71duXF1dNWXKFL3//vsaMGBAgddr3bq1vL29ZZqmHnnkEdsR6Yz3sH379nl+b/K2226TJFmtVtvlfEpKdHS0Bg4cqPT0dPXp00fDhg2zPTdv3jyFhITo1KlT+sc//lGi8wDKFIddIRFAidi8ebNZu3btLLcdc3NzMwMDA213cZBkGoZhPvroo2ZKSkqW9SMiIkw/Pz/bOG9v7yy3bfP39zd/+eWXbPvN70LCGUJDQ3O8nZxp5n/xatM0zR9//NF21wz997ZwHh4epiSzTp06We7uktnfb8/m7u5uVqpUKct7UrVqVfPIkSNZ1ivIbe4c9X7lJ2P7hV03r4tXz5gxI8v76OPjY3v/g4KCslxwO6fX1alTJ9vzvr6+ZmhoqBkaGmp+8MEHtjEZF6/O7+LZeb2HPXv2NCWZNWrUMGNiYrKtu379etstD2fNmlWAdwUo/zhSCJQzrVq10tGjR/X1119rwIABql27tjw8PBQfH6/AwEC1bt1ar732mo4cOaKFCxfK1dU1y/rt2rXT0aNHNXbsWNWvX1/p6ekyTVP169fXiy++qCNHjqhNmzYOenXSvffeq02bNqlHjx4KCAhQWlqaatSooXHjxmn37t05XkRakh588EEtWLBATz75pBo1aiQ/Pz/FxcXJ19dXd999tyZNmqRDhw6pXr16hZrPjf5+Fbfhw4dr9erVat++vXx8fJSamqpq1arpueee0759+3T77bfnuf63336rMWPG6NZbb9W1a9d08uRJnTx5slg/Uv7000+1YsWKPK9H2LlzZ7300kuSpNGjR+vIkSPFtn+grDJMsxS/8QsAAIAbEkcKAQAAQBQCAACAKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKATy9dlnnyksLEweHh5q2rSpNm3a5OgpAagAfvnlFz3wwAOqWrWqDMPQ8uXLHT0llHNEIZCHxYsXa/To0Xrttde0Z88etWnTRt27d9epU6ccPTUA5VxiYqIaNWqk6dOnO3oqqCC4zR2Qh3vuuUdNmjTRjBkzbMvq16+vXr166Z133nHgzABUJIZhaNmyZerVq5ejp4JyjCOFQC5SUlK0e/dude3aNcvyrl27auvWrQ6aFQAAJYMoBHJx6dIlpaWlKTg4OMvy4OBgnT9/3kGzAgCgZBCFQD4Mw8jyu2ma2ZYBAFDWEYVALoKCguTs7JztqODFixezHT0EAKCsIwqBXLi5ualp06Zav359luXr169Xy5YtHTQrAABKhoujJwDcyF544QUNHDhQzZo1U4sWLTRr1iydOnVKw4cPd/TUAJRzCQkJOn78uO33yMhI7d27V4GBgapZs6YDZ4byikvSAPn47LPPNHXqVJ07d0633XabPvjgA7Vt29bR0wJQzkVERKhDhw7Zlg8ePFhffvll6U8I5R5RCAAAAL5TCAAAAKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKgQK5evWqJk6cqKtXrzp6KgAqGP7+QWnhOoVAAVitVvn5+SkuLk4Wi8XR0wFQgfD3D0oLRwoBAABAFAIAAEBycfQESkN6errOnj0rX19fGYbh6OmgDLJarVn+LwCUFv7+QVGZpqn4+HhVrVpVTk65Hw+sEN8pPH36tGrUqOHoaQAAADhMVFSUqlevnuvzFeJIoa+vryRp/rfr5eXl7eDZAKiIOrVo6OgpAKigrFarbg6tYeuh3FSIKMz4yNjLy1te3j4Ong2AioizRgE4Wn5foeNEEwAAABCFAAAAIAoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACDJxdETAHKTlJSo/Xv+o9+PHrr+89shWeNiJUmz/71KVavXzHG99PR0Hdi7S78fPajffzus348e1IXzZyVJz780Qff26JvrPtNSU7X31x3auW2Tjhzaq7OnTynl6hX5WvxVp15Ddb2vt1q06Zjr+tPeeV0bf1yZ5+u6q0VbTXx3erbl97e7I8/1Mnv3o7m6/c5mBR4PoOyIiorSsmVLtXHDBu3fv08XLlyQm5ubatWqpXu7ddfzz49SSEiIo6eJcogoxA1r3+4dmvz66EKvl5SYoFfHPGXXPj+dNllrVy+1/e7i4iI3dw9djrmk/2z9Wf/Z+rNateuil8e/KxcX11y34+HpKQ9Prxyf8/Gx5LjcP7BSnnNLTkzU1atX5OLqqtCw2gV4NQDKmqioKNUKC5VpmrZlfn5+SkhI0P79+7V//37NmT1L3yz5Th06dHDgTFEeEYW4ofkHBKp23Ya6tV5DVQqqrE/ef6tA63l4euqWOvVVp25D1anXUHM+fV+XYy7lu15qWqoq3VRZ3Xr0Vcs2nRRaq44Mw1D0pYv65l9ztGrZIm35eb3mz66qoSPG5rqdPv0Ga8CTzxT4dUrSv5eF5/n8s0MfVuTx33R3i7ay+PkXatsAyoa0tDRJUo8eD2jw4CfUoWNH+fv7KyUlRRs3btTzz41UZGSk+vTuqcNHfuOIIYoVUYgb1t0t2+nfbSJsv184d6ZA63n7+GrJmm1ycvrfV2bnz/64QOve36ufnhs7Xq5ublmWVwqqrBGjX9WV5GRt+HGFVi9frMeHjJS7u0eBtltUf/x+VJHHf5Mkder2YKnsE0DpCwgI0O5f9+qOO7J+ncTNzU3du3fX96vWqFnTxoqPj9fs2bM0fvwEB80U5REnmuCG5ezsbNd6hmFkCcLCqFv/9mxBmFnn7j0lSVevXFHUyT/t2oc9Nq69/j1FP/8A3XVPm1LbL4DS5efnly0IM6tXr57uad5ckvTr7t2lNS1UEEQhUAi+Fj/b4/S09FLZZ1pqqn7esEaS1L7L/XJ24QA/UJFV+u/3jzM+agaKC/+6AIVwcN/1/zJ3dnZRtRqhuY6LWL9G639YocvRf8nD00s1Qmupeav2uq/nI/Ly9inUPnft2KzYyzGSpE738tExUJGlpaVp27atkqQGDRs6eDYob8rMkcLPPvtMYWFh8vDwUNOmTbVp0yZHTwkVzJXkJH379TxJUsu2neTt45vr2LNnTtmCMDEhXkcO7tW8mR/qmSf76s//fjewoDb8uEKSFHbLrbqlTj37XwCAMm/GjM907tw5OTk5aeDAQY6eDsqZMnGkcPHixRo9erQ+++wztWrVSjNnzlT37t11+PBh1ayZ87XqgOL2yf9N0l8XzsnTy1tPPD0qxzG169RXvQZ36K4WbVUpqLKcnJyUEG/VpvC1mjfzQ/114ZwmvPyMPp33XYHOII63xuk/236RJHXq1rM4Xw6AMmbfvn169ZVxkqQRI57Rbbfd5uAZobwpE0cKp02bpqFDh+qpp55S/fr19eGHH6pGjRqaMWNGjuOvXr0qq9Wa5QcoisVfzVbE+tUyDEPPvTheVUKq5zjuwYcG6L6ej+imylVsJ7v4+FrU/cGH9c4Hc+Ti6qqY6L+0bPH8Au33540/KPXaNTk7u6hDl/uK7fUAKFvOnTunPr17KikpSY0bN9aUqe85ekooh274KExJSdHu3bvVtWvXLMu7du2qrVu35rjOO++8Iz8/P9tPjRo1SmOqKKd+WLlEC+Z8IkkaOmKs2nXqbtd2brm1vtp17CZJ2rH15wKtk3F3lKb3tJJ/QN4XtwZQPkVHR6t7t646efKk6tSpo1Wrf5CHR+lcDgsVyw0fhZcuXVJaWpqCg4OzLA8ODtb58+dzXOeVV15RXFyc7ScqKqo0popy6Ke13+uzD/6fJGnAkyPUu1/RvsNza/3bJUnnz53Od+ypE3/q2NGDkqTOXJsQqJDi4uJ0/33ddPDgQdWsWVNr123I9u8hUFzKxHcKpevXnsvMNM1syzK4u7vL3d29NKaFcmxT+Dp9MGW80tPT1af/YD32xIhi23Zu/7+bWcYJJr4WP93dsn2x7RtA2ZCYmKgePe7Trl27VKVKFa1bv5Hv0aNE3fBHCoOCguTs7JztqODFixf5ryWUmB1bIvT+5HFKT0vT/b365XlLu8I4duSAJCm4StU8x6Wnpyti/WpJUrtO3eXqmvt9lgGUP8nJyerZ8wFt27pVQUFBWrd+o2rX5p7nKFk3fBS6ubmpadOmWr9+fZbl69evV8uWLR00K5Rne3Zt1zsTX1Rqaqo6d+upEaNfLdB6mW9gn5M/j/+mn3/6UZLUrHnbfOawTdGXLkri2oRARZOSkqKH+vZRRHi4/P399cOP69SgQQNHTwsVQJn4+PiFF17QwIED1axZM7Vo0UKzZs3SqVOnNHz4cEdPDSUsLvay7XFCvDXL48zP+Vr8stzaLjEhXqmpqbbfzfTrdx9JTk7Ksp6Xl3eW29odPrBHk18fpWspKWrbsZtG/fPNAn3UK0nh61Zp+5YIdby3hxre3sR295PEhHjbJWlSr12Tf0Cg+vYfnOe2Mk4wqXnzLbq1PpedACqKtLQ0PT7gMa1d+6N8fX21es2Paty4saOnhQqiTERhv379FB0drbfeekvnzp3TbbfdpjVr1ig0NPc7SqB8eKxnuxyXjxn+WJbf5y76QcEh1Wy/T3ptlA7s3ZVtvdnT39Ps6f+7lMPocZPUpfv/rv/31RfTdSU5WZK0d/cODezbKde5Pf3cP9X2v2cTS1Jaerq2/LxeW36+flTb08tbLi4uSoi32o4i3hQcotcnfyA//8Bct5uUmKDtm8MlSZ3ufSDXcQDKny1btmjp0u8kSdeuXVOf3rlfn7RGjRravmNnaU0NFUCZiEJJeuaZZ/TMM884ehoo5zJ/BGyNu5zHyOvXw8ysUeO7NHDoszp8YI9OR52QNS5WSYmJsvj56+ZadXRPq/bqcl9veXl557ndTeFrdfXqFTk5OalD1x72vxgAZU56+v/uqX7lyhVduXIl17FclgbFzTDz+yJUOWC1WuXn56cla7YW+r6zAFAc7m19u6OnAKCCslqtCgzwU1xcnCwWS67jbvgTTQAAAFDyiEIAAAAQhQAAACAKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACIKAQAAIKIQAAAAIgoBAAAgohAAAAAiCgEAACCiEAAAACrhKLx8+bKsVmtJ7gIAAADFwO4oPHv2rBYsWKAff/wx23OHDh1Ss2bNFBQUpICAALVp00bHjh0r0kQBAABQcuyOwrlz5+rJJ59UREREluXJycm67777tGfPHpmmKdM0tWXLFnXu3JmjhgAAADcou6Nww4YNkqR+/fplWT5//nxFRUUpMDBQs2fP1r/+9S9Vr15dZ86c0aefflq02QIAAKBE2B2FJ06ckCTVq1cvy/KlS5fKMAy9/fbbGjp0qB577DHNnj1bpmlq5cqVRZosAAAASobdUXjp0iVZLBZ5enralqWnp2vr1q0yDEMPPfSQbXmXLl3k5OSk3377rWizBQAAQImwOwrT0tJ09erVLMsOHDigpKQkNWzYUAEBAf/biZOTAgIClJiYaP9MAQAAUGLsjsKQkBBdvXpVkZGRtmVr166VJLVs2TLb+ISEBAUGBtq7OwAAAJQgu6OwRYsWkqQ333xT6enp+uuvvzRjxgwZhqF77703y9jIyEhdvXpVISEhRZstAAAASoTdUThq1ChJ0ldffSV/f3/VqFFDJ0+eVFhYmHr06JFl7Pr16yVJTZo0KcJUAQAAUFLsjsK7775bc+fOlY+PjxISEpSSkqJ69epp6dKlcnFxyTJ2wYIFkqQOHToUbbYAAAAoEYZpmmZRNpCcnKyDBw/K399ft9xyi5ycsnZmSkqKFi1aJNM01bNnT/n7+xdld3axWq3y8/PTkjVb5eXtU+r7B4B7W9/u6CkAqKCsVqsCA/wUFxcni8WS6ziXXJ8pIE9PT9111125Pu/m5qZBgwYVdTcAAAAoQXZ/fAwAAIDygygEAABAwT4+rlWrVrHszDAM/fHHH8WyLQAAABSfAkVhxn2Oi8owjGLZDgAAAIpXgaJw3rx5JT0PAAAAOFCBonDw4MElPQ8AAAA4ECeaAAAAgCgEAAAAUQgAAAAVQxTu27dPw4YNU4MGDWSxWOTs7Jzrz9/viQwAAIAbQ5Eqbfr06XrhhReUlpamIt5CGQAAAA5k95HCHTt2aNSoUUpLS9MzzzyjNWvWSJICAwO1YcMG/etf/9ITTzwhNzc3BQUFaeHChfrpp5+KbeIAAAAoPnYfKfz4449lmqZGjx6tadOm2Za7ubmpY8eOkqTHHntMzz//vO6991698cYb+vXXX4s+YwAAABQ7u48UbtmyRYZhaNSoUVmW//1j5DvvvFOffPKJ/vjjD7333nv27g4AAAAlyO4ovHDhgtzd3RUaGvq/jTk56cqVK9nG9u7dW66urlq6dKm9uwMAAEAJsjsKvby85OrqmmWZr6+vrFarrl69mmW5q6urvLy8dPLkSXt3BwAAgBJkdxRWq1ZNCQkJslqttmW33HKLJGnnzp1Zxp49e1ZxcXGcoQwAAHCDsjsK77jjDknSb7/9ZlvWvn17maapt956y/YxckpKip5//nlJ0u23316UuQIAAKCE2B2FPXr0kGmaWrx4sW3ZyJEj5e7uro0bN6p69epq1aqVqlWrpmXLlskwDD377LPFMmkAAAAUL7uj8L777tOECRNUp04d27KwsDAtXLhQvr6+iomJ0bZt2xQdHS3DMPTyyy9rwIABxTJpAAAAFC/DLIEv+sXExGjNmjWKioqSn5+funbtqtq1axf3bgrMarXKz89PS9ZslZe3j8PmAaDiurc1X58B4BhWq1WBAX6Ki4uTxWLJdVyJ3Iw4MDBQjz/+eElsGgAAACXA7o+PAQAAUH4QhQAAALD/4+OM+xsXhmEY2rhxo727BAAAQAmxOwojIiIKNM4wDEnX74mc8RgAAAA3FrujcMKECXk+HxcXpx07dmjbtm2qVKmSRowYIWdnZ3t3BwAAgBJUYlGY4aefflKfPn10+PBhffvtt/buDgAAACWoxE806dixoz766CMtW7ZMc+bMKendAQAAwA4lcvHqv7ty5YosFouaNGmi7du3l/Tussm4eHXM5bwv2ggAJWXt5gOOngKACiopMUEP39cy34tXl8olaTw8POTt7a0jR46Uxu4AAABQSKUShWfOnFFcXJxK4aAkAAAA7FDiUZicnKxnnnlGknT77dz7EwAA4EZk99nHb731Vp7PX7lyRVFRUVq7dq2io6NlGIZGjhxp7+4AAABQguyOwokTJxboYtSmacrJyUmvvfaaHnvsMXt3BwAAgBJkdxS2bds2zyh0cXFRQECAGjVqpEceeUR16tSxd1cAAAAoYSV+mzsAAADc+Erl7GMAAADc2OyOwrfeekvTpk0r8PiPP/4435NTAAAA4Bh239HEyclJVapU0dmzZws0PiwsTKdOnVJaWpo9uysS7mgCwNG4owkAR7mh7mgCAACAG1upRWFMTIw8PDxKa3cAAAAohFKJwiVLlig+Pl41a9Ysjd0BAACgkAp8SZqPPvpIH330UZZlf/31l2rVqpXrOqZpKjY2VlarVYZh6P7777d/pgAAACgxBY7C2NhYnThxIsuytLS0bMty06lTJ40fP74wcwMAAEApKXAU9urVSzfffLOk60cAhwwZIj8/P3344Ye5ruPk5CSLxaLbbrtNt9xyS1HnCgAAgBJSapekcSQuSQPA0bgkDQBHKeglaey+zV16erq9qwIAAOAGw3UKAQAAYH8Ubt++XU2aNNHIkSPzHfvUU0+pSZMm2rVrl727AwAAQAmyOwoXLlyoffv2qU2bNvmObd68ufbu3auFCxfauzsAAACUILuj8Oeff5YktWvXLt+xGdcnDA8Pt3d3AAAAKEF2R+Hp06fl7u6ukJCQfMeGhITI3d1dZ86csXd3AAAAKEF2R2FycrLc3NwKPN7d3V3x8fH27g4AAAAlyO4orFy5suLj4wt0ncIzZ87IarUqKCjI3t0BAACgBNkdhc2bN5ckffrpp/mOzRhzzz332Ls7AAAAlCC7o3Do0KEyTVNTp07VrFmzch03c+ZMTZ06VYZhaOjQofbuDgAAACXI7juadOnSRQ899JC+/fZbjRgxQtOnT9cDDzyg0NBQGYahEydO6Pvvv9ehQ4dkmqb69u2r7t27F+fcAQAAUEzsjkJJmj9/vgzD0JIlS3Tw4EEdOnQoy/MZt1Xu37+/vvjii6LsCgAAACWoSLe58/T01OLFi7VhwwY99thjCg0Nlbu7uzw8PHTzzTdrwIAB+umnn7Rw4UJ5enoW15wBAABQzIp0pDBDx44d1bFjx1yfT09P1+rVq/XFF19o+fLlxbFLAAAAFKNiicLcHDt2THPnztWCBQt04cKFktwVAAAAiqDYozApKUnffPON5s6dqy1btkj633cL69evX9y7AwAAQDEotijcvn275s6dq8WLFyshIUHS9RisV6+eHn74YT388MO67bbbimt3AAAAKEZFisK//vpLX331lb744gsdPXpU0v+OChqGoZ07d6pp06ZFnyUAAABKVKGj0DRN/fDDD/riiy+0atUqpaamyjRNeXp6qlevXho8eLC6desmiY+LAQAAyooCR+Eff/yhuXPnav78+Tp37pxM05RhGGrdurUGDRqkRx55RL6+viU5VwAAAJSQAkdhnTp1ZBiGTNNUrVq1NHDgQA0aNEhhYWElOT8AAACUgkJ/fPz8889r6tSpcnNzK4n5AAAAwAEKfEcTNzc3maapTz75RFWrVtXIkSO1ffv2kpwbAAAASkmBo/D8+fP6+OOPdccddygmJkYzZsxQq1atVLduXb399ts6depUSc4TAAAAJajAUejv769nn31We/bs0e7duzVixAj5+fnp999/1xtvvKFatWqpY8eOmjdvXknOFwAAACWgwFGYWePGjfXpp5/q3Llz+uqrr9SuXTuZpqmIiAg99dRTtnHr1q1TampqsU0WAAAAJcOuKMzg7u6uAQMG6KefftLx48f16quvqlq1apKuX8+wb9++qly5sp588kmtWbOGQAQAALhBGWbGLUiKiWmaWrt2rebMmaPvv/9e165dk2EYkq5/BB0dHV2cuysQq9UqPz8/xVyOk8ViKfX9A8DazQccPQUAFVRSYoIevq+l4uLy7qAiHSnMiWEY6tatm7799ludOXNG77//vho0aCDTNBUbG1vcuwMAAEAxKPYozCwoKEgvvPCCDhw4oK1bt2ro0KEluTsAAADYqdAXr7ZX8+bN1bx589LaHQAAAAqhRI8UAgAAoGwgCgEAAEAUAgAAgCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgIhCAAAAiCgEAACAiEIAAACIKAQAAICIQgAAAIgoBAAAgCQXR08AuJGdP39eU959R6tXr9KZM2fk5+enu+66W8+PGq1OnTo5enoASlBSUqL27/mPfj966PrPb4dkjYuVJM3+9ypVrV4zx/XS09N1YO8u/X70oH7/7bB+P3pQF86flSQ9/9IE3dujb677TEtN1d5fd2jntk06cmivzp4+pZSrV+Rr8Vedeg3V9b7eatGmY6Fex++/HdYLIwYoPS1NkjR30Q8KDqlWbK8X5QdRCORi//796tK5o6KjoyVJFotFly5d0urVq7RmzWpN/n9v65//HOfgWQIoKft279Dk10cXer2kxAS9OuYpu/b56bTJWrt6qe13FxcXubl76HLMJf1n68/6z9af1apdF708/l25uLjmu720tDRN/7+3bEGYF3tfL8oPPj4GcpCcnKzevR5UdHS0GjdurH37DyrmcpwuRV/WmBfGyjRNvfbqK1q3bp2jpwqgBPkHBKpZ8zZ67Inheu7F8QVez8PTUw3vaKJeDw/US2+8q4DAoAKtl5qWqko3VdaAJ0fo07nfavmG3VqyZqsWfLdBPXr3lyRt+Xm95s/+uEDbW7Xsax3/7bDqNri9QOPtfb0oHzhSCORg1qyZOnnypHx8fLR8xfeqVu36Ry0Wi0Xvvfe+/vzjD61YsVyvv/aKunbt6uDZAigJd7dsp3+3ibD9fuHcmQKt5+3jqyVrtsnJ6X/HXQoacff36qfnxo6Xq5tbluWVgiprxOhXdSU5WRt+XKHVyxfr8SEj5e7ukeu2Ll08r6+++FRBNwWr/6Cn9ea4Z/Pct72vF+UHRwqBHHy98N+SpEcffcwWhJmNffElSdKvv/6qo0ePlurcAJQOZ2dnu9YzDCNLEBZG3fq3ZwvCzDp37ylJunrliqJO/pnntj7/6F0lJyVq2HMvy8PDM9992/t6UX4QhcDfxMfHa/fu3ZKkrl3vzXFM8+bN5efnJ0kKD/+p1OYGoGLztfjZHqenpec6bvuWcG3b/JOa3t1Krdp1KY2poRwgCoG/OXLkiEzTlCQ1aNgwxzFOTk66tW7d6+MPHy61uQGo2A7uu/4frM7OLqpWIzTHMVeSk/T5R+/K1c1Nw0e9UprTQxlXJqLwl19+0QMPPKCqVavKMAwtX77c0VNCOXb+3Dnb46pVq+Y6rmrI9efOZRoPACXlSnKSvv16niSpZdtO8vbxzXHcV198qr8unNPDjw3hMjIolDIRhYmJiWrUqJGmT5/u6KmgAkhMTLQ99vTM/Xs4Xl5ekqSEhIQSnxMAfPJ/k/TXhXPy9PLWE0+PynHMH78f1cqlCxVSrYYefmxoKc8QZV2ZOPu4e/fu6t69e4HHX716VVevXrX9brVaS2JaKKcyPjoGgBvF4q9mK2L9ahmGoedeHK8qIdWzjUlPT9f0969fk3D48+Pk5u7ugJmiLCsTRwoL65133pGfn5/tp0aNGo6eEsoQHx8f2+Pk5ORcxyUlJWUbDwDF7YeVS7RgzieSpKEjxqpdp5wPkqxatkjHjh5Uy7ad1Kx5m9KcIsqJchmFr7zyiuLi4mw/UVFRjp4SypAqISG2x2fPns113Nlz158LyTQeAIrTT2u/12cf/D9J0oAnR6h3v0E5jktMiNdXX0yXm5u7Bg59TslJSVl+Uq5esY29evWKkpOSdC0lpVReA8qOMvHxcWG5u7vLncPmsFP9+vVlGIZM09ThQ4dU979nGWeWnp6uY7/9dn18gwalPUUAFcCm8HX6YMp4paenq0//wXrsiRG5jk2Ityop8fr3m0cM7pXndkcM7i1J6tTtQb3wyuRimy/KvnJ5pBAoCl9fXzVt2lSStGHD+hzH7NixQ3FxcZKkDh0Kd3N6AMjPji0Ren/yOKWnpen+Xv00dMRYR08JFUC5PFIIFFW//o9q165dWrjw33r9jfHZPiKe9n/vS5IaN26sevXqOWKKAMqpPbu2652JLyo1NVWdu/XUiNGv5rtOcEg1rf55f67P79+zU6+Mvn428txFPyg4JPudmoAyEYUJCQk6fvy47ffIyEjt3btXgYGBqlmTazCh+A0fPkIff/ShoqKi1PPBHvpy/ldq0KCB4uPjNXnyJC1btlSSNGny2w6eKYCSFBd72fY4Id6a5XHm53wtfllubZeYEK/U1FTb72b69buPJCcnZVnPy8s7y23tDh/Yo8mvj9K1lBS17dhNo/75pgzDKN4XlQd7Xy/KB8MsA9ffiIiIUIcOHbItHzx4sL788st817darfLz81PM5ThZLJYSmCHKoz179ujerp0VExMjSbJYLEpISFD6f/9yf2vSZL366muOnCLKkLWbDzh6CrDD/e3uKNC4vx99GzdqiA7s3ZXveqPHTVKX/97PWJJeGT1U+/fslCRZ/ALk5Jx7eD393D/VtmO3As2voEcK7X29uLElJSbo4ftaKi4u7w4qE0cK27dvz7XjUOoaN26sffsPasq772j16lU6e/asAgMDddddd2vU6DHq3Lmzo6cIoJzJ/G+dNe5yHiOV5Xq8QHEoE0cKi4ojhQAcjSOFAByloEcK+UIAAAAAiEIAAAAQhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAABBRCAAAABGFAAAAEFEIAAAAEYUAAAAQUQgAAAARhQAAAJDk4ugJlAbTNCVJVqvVwTMBUFElJSY4egoAKqikpERJ/+uh3FSIKIyPj5ck3Rxaw8EzAQAAcIz4+Hj5+fnl+rxh5peN5UB6errOnj0rX19fGYbh6OmgDLJarapRo4aioqJksVgcPR0AFQh//6CoTNNUfHy8qlatKien3L85WCGOFDo5Oal69eqOngbKAYvFwl/KAByCv39QFHkdIczAiSYAAAAgCgEAAEAUAgXi7u6uCRMmyN3d3dFTAVDB8PcPSkuFONEEAAAAeeNIIQAAAIhCAAAAEIUAAAAQUQgAAAARhQCQpyeeeEKGYeiJJ57I9lz79u1lGIYmTpxYqnOKiIiQYRjcoQlAsSIKAZSoiRMn2gIm84+Hh4eqV6+uBx98UN98802+N2qvCGJjYzVx4kRNnDhRsbGxjp4OgAqmQtzmDsCNITg42PY4Li5OZ86c0ZkzZ/T999/ryy+/1LJly8rUtdhq1qypunXrKigoqFi2FxsbqzfffFPS9SOU/v7+OY7z8vJS3bp1i2WfAJCBKARQas6fP297nJ6eriNHjmjMmDFav369fvjhB73++ut67733HDjDwlmwYIFD9nv33Xfr6NGjDtk3gPKLj48BOISTk5MaNmyolStXqnbt2pKkmTNnKjU11cEzA4CKiSgE4FAeHh56+OGHJUnx8fE6evSoTpw4Yfvu4YkTJ/THH39o2LBhCgsLk7u7u26++eZs21m+fLl69eqlqlWrys3NTQEBAWrbtq0+//xzXbt2Lc85/Pvf/1arVq3k6+srPz8/3XPPPZo1a1a+33MsyIkmR44c0ciRI9WgQQP5+vrKx8dHdevWVf/+/fXdd98pPT3dtq2wsDDbemFhYVm+g9m+fXvbcwU50eT8+fN66aWX1LBhQ/n4+Mjb21sNGzbUyy+/rAsXLuS4zt/f9wsXLmjUqFEKCwuTh4eHgoOD1b9//zyPUp4+fVpjxoxRw4YN5e3tLXd3d1WtWlVNmzbVmDFjtHPnzlzXBeBYfHwMwOGqV69ue2y1WuXj42P7fevWrXr66aeVkJAgLy8vubq6Zlk3ISFBjz76qFatWmVbZrFYFBcXp02bNmnTpk1asGCBVq9erYCAgCzrmqapoUOHat68eZIkwzDk7++vXbt26T//+Y/Cw8OL9B3HKVOm6NVXX7WFn4eHh1xdXXXs2DEdO3ZMixcv1uXLl+Xv76/AwEAFBQXp0qVLkqSgoCA5OzvbthUYGFjg/f7888/q1auX7WQVLy8vGYahw4cP6/Dhw5ozZ45Wrlyp1q1b57qNQ4cOaciQIbp48aK8vLwkSRcvXtTixYv1ww8/6JdfflGjRo2yrLNv3z516NBBly9fliQ5OzvLYrHo/PnzOnfunH799VddvnxZX375ZYFfC4DSw5FCAA534sQJ2+O/x8/TTz+thg0baufOnUpMTFRCQoLWrVtne37gwIFatWqVateurYULF8pqtSouLk5JSUlasWKFatWqpW3btmnIkCHZ9vvJJ5/YgvDZZ5/VxYsXFRMTo5iYGE2cOFGLFy/WihUr7HpNM2bM0Lhx45Senq4HH3xQe/bsUXJysqxWq6Kjo7Vu3Tr169dPTk7X/xpeunRplqNoO3fu1Pnz520/S5cuLdB+o6KibEHYoEEDbd682fa+/fLLL6pbt64uX76snj176syZM7luZ+DAgapTp06W9339+vUKCQmR1WrVc889l22dsWPH6vLly2rSpIm2bduma9euKSYmRleuXNGxY8f0/vvvq2HDhoV8JwGUGhMAStCECRNMSWZuf93ExcWZVatWNSWZgYGBZlpamhkZGWlbJzQ01IyPj89x3VWrVpmSzCpVqpinT5/OcUxUVJTp7e1tSjL37NljW56cnGwGBgaaksyBAwfmuO64ceNs8xg8eHC259u1a2dKMidMmJBleUxMjOnr62tKMvv372+mp6fnuP2/y/y6IyMjcx0XHh6e63s6fPhwU5IZEBBgnjt3LtvzUVFRpsViMSWZI0eOzHX/9erVM5OSkrKtv3LlStuYqKioLM95enqaksytW7cW6PUCuLFwpBCAQ8TGxmrjxo3q2LGjzp49K0kaNWqU7chZhmeffTbLx8mZzZkzR9L1o1rVqlXLcUz16tXVoUMHSdLatWtty9etW6eYmBhJ0vjx43Ncd9y4cfLw8CjEq7ru22+/VXx8vFxdXTVt2rRSu8i0aZr65ptvJEnDhw9XlSpVso2pXr26hg8fLklatGhRrtsaO3asPD09sy3v3r273NzcJEkHDhzI8lzGJXTOnTtn1/wBOBZRCKDUZD5xIiAgQJ07d9bu3bslSY8//rhee+21bOu0atUq1+1t3rxZkjRr1ixVqVIl158NGzZIkk6ePGlbd9euXZKkGjVq2M5+/js/Pz81bdq00K9z69atkqSmTZsqJCSk0OvbKzIy0ha6nTt3znVcly5dJEnR0dGKjIzMccw999yT43IXFxfddNNNkmTbV4YePXpIkgYPHqyxY8fq559/VlJSUuFeBACH4UQTAKUm88Wr3d3dFRQUpMaNG2vAgAG2o3l/V7ly5RyXX7t2zXZSRlxcnOLi4vLdf+ZAuXjxoiTleoQxQ+aTYAoq43qMoaGhhV63KDJek5T368r8mi5evJjlrOcMvr6+ua7v4nL9n46/n9U9depUHT9+XOHh4Zo2bZqmTZsmZ2dn3Xnnnbr//vs1bNiwfN9vAI5DFAIoNZkvXl1Qmc/AzSwtLc32eNGiRerXr59dcyrJj3YdeW/igu67OOfo7++vn376SZs3b9b333+vLVu2aNeuXdq9e7d2796t9957T1988YUeffTRYtsngOLDx8cAyiQPDw/5+flJyv7dtoLIOAJ5+vTpPMfldYZubjI+Ms58VnVpyHxUNSoqKtdxmV9zxkfBxal169aaMmWKNm/erNjYWK1YsUK33367kpOTNWTIkFyvkwjAsYhCAGVWxvcNlyxZYrsWYEE1a9ZM0vV4+uOPP3IcY7Vabd95LIyWLVtKuv69xcKcdJH5JBsznwtn5yQsLMx2SZ+NGzfmOi7jO5aVKlXK8aPj4uTh4aEHH3zQdkmdK1eu2L4LCuDGQhQCKLOGDRsmSTp27Fi+90xOTExUSkqK7fcuXbrYLmY9adKkHNeZOnWqkpOTCz2vhx9+WBaLRampqRozZkyBA89isdgeZ1x4ujAMw7B9jD5z5swcP64/e/asZs6cKUnF+jFuampqnmGe+Uzm3L4SAMCxiEIAZVbPnj3Vu3dvSdcvHzNixAgdO3bM9nxKSop27Nihf/7znwoNDc1yIoanp6feeOMNSdL8+fM1evRoRUdHS7p+hHDSpEl6++23bZdZKQw/Pz9NnTpVkrR48WL17t1be/futT1/+fJlrV69Wj179pTVarUt9/f3t52IMW/ePLvuA/3qq6/K399fMTEx6ty5s+1MaEnasmWLOnfurNjYWAUGBmrcuHGF3n5uTp8+rTp16mjy5Mnas2dPlrnv379fjz/+uCTJ29tbbdu2Lbb9AihGDr5OIoByLr+LV+ekoBdxNk3TTExMNPv3728bL8n09vY2AwICTCcnpyzL/36B67S0NHPgwIG2552cnMyAgADT2dnZduHpwYMHF/ri1RnefvvtLHPw9PS0XdQ64+fy5ctZ1pk0aZLtOXd3d7NGjRpmaGio2a9fP9uYvC5ebZqmGRERYfr5+WV5PzIu4C3J9Pf3N3/55Re73/fQ0FBTkjlv3rwc15VkOjs7m4GBgaabm5ttmZubm7lkyZJctwvAsThSCKBM8/Ly0tdff63w8HANHDhQtWrVUnp6uhISElS5cmV17NhRU6dO1e+//57tcihOTk5asGCBFixYoObNm8vT01Opqalq0qSJPv/8cy1cuLBIc3vllVe0b98+/eMf/7BdC9E0TdWtW1ePPvqoli5dmuUjY+n6kb6PPvpIzZo1k6urq06fPq2TJ08W6sztdu3a6ejRoxo7dqzq16+v9PR0maap+vXr68UXX9SRI0fUpk2bIr22v6tWrZpWrlypMWPGqHnz5goJCVFCQoJcXFzUoEEDjRw5UgcPHtRDDz1UrPsFUHwM07Tj28wAAAAoVzhSCAAAAKIQAAAARCEAAABEFAIAAEBEIQAAAEQUAgAAQEQhAAAARBQCAABARCEAAABEFAIAAEBEIQAAAEQUAgAAQEQhAAAAJP1/2eYPGo+aCM4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 750x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the predictions results by plotting the confusion matrix.\n",
    "conf_matrix = confusion_matrix(y_true=ground_truth_label.values, y_pred=predict_label)\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i, s=conf_matrix[i, j], va=\"center\", ha=\"center\", size=\"xx-large\")\n",
    "\n",
    "plt.xlabel(\"Predictions\", fontsize=18)\n",
    "plt.ylabel(\"Actuals\", fontsize=18)\n",
    "plt.title(\"Confusion Matrix\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "79bb9864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LightGBM with AMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.999911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.999911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LightGBM with AMT\n",
       "Accuracy           0.999911\n",
       "F1                 0.999911\n",
       "AUC                1.000000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the prediction results quantitatively.\n",
    "eval_accuracy = accuracy_score(ground_truth_label.values, predict_label)\n",
    "eval_f1 = f1_score(ground_truth_label.values, predict_label)\n",
    "eval_auc = roc_auc_score(ground_truth_label.values, predict_prob[:, 1])\n",
    "\n",
    "lgb_results = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"Accuracy\": eval_accuracy,\n",
    "        \"F1\": eval_f1,\n",
    "        \"AUC\": eval_auc,\n",
    "    },\n",
    "    orient=\"index\",\n",
    "    columns=[\"LightGBM with AMT\"],\n",
    ")\n",
    "\n",
    "lgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5bacee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: sagemaker-jumpstart-2024-10-27-03-18-48-300\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: jumpstart-distr-lgb-g1729999123\n",
      "INFO:sagemaker:Deleting endpoint with name: jumpstart-distr-lgb-g1729999123\n"
     ]
    }
   ],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2098d9",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/introduction_to_applying_machine_learning|sagemaker_lightgbm_distributed_training_dask|sagemaker-lightgbm-distributed-training-dask.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
