{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce11b1f7-803e-4623-8499-97478594f1e9",
   "metadata": {},
   "source": [
    "# Train Pipeline (SageMaker Pipelines)- Arch One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b06ac-9c5d-49ca-8df6-dc53283494be",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4cd6d8-b36d-474a-af41-027c26cbff06",
   "metadata": {},
   "source": [
    "The following diagram illustrates the high-level architecture of the ML workflow with the different steps to train the model.\n",
    "\n",
    "![](images/pipelines.PNG)\n",
    "\n",
    "Train Pipeline consists of the following steps:\n",
    "\n",
    "1. Preprocess data to build features required and split data into train, validation, and test datasets.\n",
    "2. Apply hyperparameter tuning based on the ranges provided with the SageMaker LightGBM framework to give the best model, which is determined based on AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6d4de-4761-430a-9145-07f7cf3c264c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install sagemaker -U\n",
    "pip install boto3 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c0074-467f-46e9-a57b-091e8db6d171",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1184d1cc-f1aa-469f-80b9-062f0b7248b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"churn.txt\")\n",
    "df.to_csv(\"churn.csv\",index=False)\n",
    "bucket = \"BUCKET\" #Bucket Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c4d57-4ebd-461a-a206-7373eb03f342",
   "metadata": {},
   "source": [
    "#### Uplaod training Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4a0152a-5b68-4594-978c-b66b1fa7c923",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./churn.csv to s3://fairstone/ml_training/churn.csv       \n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp churn.csv s3://{bucket}/ml_training/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d89f34-3526-41df-a964-5f2a7112f48c",
   "metadata": {},
   "source": [
    "### Step 1: Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "70816378-1c97-4511-931c-b1dbd3d91e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import pandas as pd \n",
    "import sagemaker \n",
    "from sagemaker.workflow.pipeline_context import PipelineSession \n",
    "\n",
    "s3_client = boto3.resource('s3') \n",
    "pipeline_name = \"LightGBM-ML-Pipeline\" \n",
    "sagemaker_session = sagemaker.session.Session(default_bucket=bucket) \n",
    "region = sagemaker_session.boto_region_name \n",
    "role = sagemaker.get_execution_role() \n",
    "pipeline_session = PipelineSession() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc57301-08f3-458d-80cf-307c683e0f15",
   "metadata": {},
   "source": [
    "## Step 2: Define SageMaker Pipeline Parameters \n",
    "\n",
    "SageMaker Pipelines supports parameterization. This allows ausers to alter the values of each parameters for each initiated pipeline execution. You can add or remove parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7c0d50be-8cef-45e6-9f7c-827c05a98c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    "    ParameterBoolean\n",
    ")\n",
    "from sagemaker.workflow.functions import Join\n",
    "# LightGBM tunable parameters for SageMaker Pipelines\n",
    "learning_rate_min = ParameterFloat(name=\"LearningRateMin\", default_value=0.001)\n",
    "learning_rate_max = ParameterFloat(name=\"LearningRateMax\", default_value=0.1)\n",
    "\n",
    "num_boost_round_min = ParameterInteger(name=\"NumberOfBoostRoundMin\", default_value=2)\n",
    "num_boost_round_max = ParameterInteger(name=\"NumberOfBoostRoundMax\", default_value=30)\n",
    "\n",
    "num_leaves_min = ParameterInteger(name=\"NumLeavesMin\", default_value=10)\n",
    "num_leaves_max = ParameterInteger(name=\"NumLeavesMax\", default_value=50)\n",
    "\n",
    "feature_fraction_min = ParameterFloat(name=\"FeatureFractionMin\", default_value=0.1)\n",
    "feature_fraction_max = ParameterFloat(name=\"FeatureFractionMax\", default_value=1.0)\n",
    "\n",
    "bagging_fraction_min = ParameterFloat(name=\"BaggingFractionMin\", default_value=0.1)\n",
    "bagging_fraction_max = ParameterFloat(name=\"BaggingFractionMax\", default_value=1.0)\n",
    "\n",
    "bagging_freq_min = ParameterInteger(name=\"BaggingFreqMin\", default_value=1)\n",
    "bagging_freq_max = ParameterInteger(name=\"BaggingFreqMax\", default_value=10)\n",
    "\n",
    "max_depth_min = ParameterInteger(name=\"MaxDepthMin\", default_value=5)\n",
    "max_depth_max = ParameterInteger(name=\"MaxDepthMax\", default_value=30)\n",
    "\n",
    "min_data_in_leaf_min = ParameterInteger(name=\"MinDataInLeafMin\", default_value=10)\n",
    "min_data_in_leaf_max = ParameterInteger(name=\"MinDataInLeafMax\", default_value=50)\n",
    "\n",
    "tuner_objective_metric = ParameterString( name=\"TunerObjectiveMetric\", default_value=\"auc\")\n",
    "tuner_metric_definition = ParameterString( name=\"TunerMetricDefinition\", default_value=\"auc: ([0-9\\\\.]+)\")\n",
    "algo_metric = ParameterString( name=\"AlgorithmMetric\", default_value=\"auc\")\n",
    "\n",
    "max_tuning_jobs = ParameterInteger(name=\"MaxTuningJobs\", default_value=2)\n",
    "max_tuning_parallel_job = ParameterInteger(name=\"TuningParallelJobs\", default_value=3)\n",
    "tuning_strategy = ParameterString( name=\"TuningStrategy\", default_value=\"Bayesian\", enum_values = [\"Bayesian\",\"Random\",\"Grid\", \"Hyperband\"])\n",
    "optimization_direction = ParameterString( name=\"OptimizationDirection\", default_value=\"Maximize\", enum_values = [\"Maximize\",\"Minimize\"])\n",
    "supervised_training_task = ParameterString( name=\"TrainingTask\", default_value=\"classification\", enum_values = [\"classification\",\"regression\"])\n",
    "\n",
    "# Infra Parameters\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString( name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\") \n",
    "training_instance_type = ParameterString( name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\") \n",
    "training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=2)\n",
    "training_volume_size =  ParameterInteger(name=\"TrainingVolumeSize\", default_value=30)\n",
    "processing_volume_size =  ParameterInteger(name=\"ProcessingVolumeSize\", default_value=30)\n",
    "\n",
    "# Artifacts location Parameters\n",
    "model_approval_status = ParameterString( name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")\n",
    "model_output_bucket = ParameterString( name=\"ModelOutput\", default_value=f\"s3://{bucket}/output/model\")\n",
    "train_output_bucket = ParameterString( name=\"TrainOutput\", default_value=f\"s3://{bucket}/output/train\")\n",
    "validation_output_bucket = ParameterString( name=\"ValidationOutput\", default_value=f\"s3://{bucket}/output/validation\")\n",
    "test_output_bucket = ParameterString( name=\"TestOutput\", default_value=f\"s3://{bucket}/output/test\")\n",
    "s3_input_data_location = ParameterString( name=\"S3InputDataURI\", default_value=f\"s3://{bucket}/ml_training/churn.csv\")\n",
    "\n",
    "# Mlflow\n",
    "ml_flow_arn = ParameterString( name=\"MLflow\", default_value=\"arn:aws:sagemaker:us-east-1:1234567890:mlflow-tracking-server/test\")\n",
    "\n",
    "model_evaluation_threshold = ParameterFloat( name=\"EvalThreshold\", default_value=0.75)\n",
    "data_split_ratio = ParameterString( name=\"DataSplitRatio\", default_value=\"0.3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42ce11-837e-4d3c-bcba-ea37e9e43bed",
   "metadata": {},
   "source": [
    "### Step 3: Define Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01574fd-ccac-4fa0-96d7-d7eaab248636",
   "metadata": {},
   "source": [
    "#### Create a Logic for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e85b6816-3ef0-43ec-a5eb-615a8f804917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline_scripts/churn_preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"pipeline_scripts/churn_preprocess.py\"\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from time import strftime, gmtime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def load_and_combine_csv_files(directory):\n",
    "    \"\"\"\n",
    "    Load all CSV files from a directory and combine them into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "    directory (str): Path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Combined DataFrame of all CSV files.\n",
    "    \"\"\"\n",
    "    # Use glob to get all the csv files in the folder\n",
    "    csv_files = glob.glob(os.path.join(directory, \"*.csv\"))\n",
    "\n",
    "    # List to hold individual DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    total_rows = 0\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            # Read each file into a DataFrame\n",
    "            df = pd.read_csv(file)\n",
    "            total_rows += len(df)\n",
    "            df_list.append(df)\n",
    "            print(f\"Loaded {file}: {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "\n",
    "    # Combine all DataFrames in the list\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    print(f\"\\nTotal files processed: {len(csv_files)}\")\n",
    "    print(f\"Total rows in combined DataFrame: {len(combined_df)}\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "\n",
    "def detect_and_encode_categorical(df, max_categories=10, include_dates=True):\n",
    "    \"\"\"\n",
    "    Detect categorical columns (including object, int, and datetime), encode them, \n",
    "    and create a mapping of their indexes. Excludes the first column (assumed to be the target).\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    max_categories (int): Maximum number of unique values to consider a column categorical\n",
    "    include_dates (bool): Whether to treat date columns as categorical\n",
    "\n",
    "    Returns:\n",
    "    tuple: (preprocessed DataFrame, dict of categorical column indexes, dict of label encoders)\n",
    "    \"\"\"\n",
    "    categorical_columns = []\n",
    "    categorical_indexes = {}\n",
    "    label_encoders = {}\n",
    "\n",
    "    # Get the name of the first column (assumed to be the target)\n",
    "    target_column = df.columns[0]\n",
    "\n",
    "    for idx, (col, dtype) in enumerate(df.dtypes.items()):\n",
    "        # Skip the first column (target)\n",
    "        if col == target_column:\n",
    "            continue\n",
    "\n",
    "        if (dtype == 'object' or \n",
    "            (df[col].nunique() <= max_categories and dtype != 'float64') or\n",
    "            pd.api.types.is_integer_dtype(dtype) or\n",
    "            (include_dates and pd.api.types.is_datetime64_any_dtype(dtype))):\n",
    "\n",
    "            categorical_columns.append(col)\n",
    "            categorical_indexes[col] = idx  # Adjust index to account for skipped target column\n",
    "\n",
    "            # Handle datetime columns\n",
    "            if pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "                if include_dates:\n",
    "                    df[col] = df[col].dt.strftime('%Y-%m-%d')  # Convert to string format\n",
    "                else:\n",
    "                    continue  # Skip datetime columns if not included\n",
    "\n",
    "            # Encode categorical variables\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "\n",
    "    print(f\"Detected {len(categorical_columns)} categorical columns: {categorical_columns}\")\n",
    "    return df, categorical_indexes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--split-ratio', type=str, default=\"0.3\",dest='split_ratio')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    base_dir_input = \"/opt/ml/processing/input\"\n",
    "    base_dir = \"/opt/ml/processing/\"\n",
    "    #Read Data\n",
    "    df = load_and_combine_csv_files(base_dir_input)\n",
    "    # Sample Analysis \n",
    "    df = df.drop(\"Phone\", axis=1)\n",
    "    df[\"Area Code\"] = df[\"Area Code\"].astype(object)\n",
    "    \n",
    "    df[\"target\"] = df[\"Churn?\"].map({\"True.\": 1, \"False.\": 0})\n",
    "    df.drop([\"Churn?\"], axis=1, inplace=True)\n",
    "    \n",
    "    df = df[[\"target\"] + df.columns.tolist()[:-1]]\n",
    "    # df = pd.concat([churn]*50, ignore_index=True)\n",
    "    df, cat_columns = detect_and_encode_categorical(df, max_categories=10, include_dates=True)\n",
    "    cat_idx = list(cat_columns.values())\n",
    "    \n",
    "    # Save categorical information\n",
    "    with open(f\"{base_dir}/train/cat_idx.json\", \"w\") as outfile:\n",
    "        json.dump({\"cat_idx\": cat_idx}, outfile)\n",
    "        \n",
    "    # train, test, validation\n",
    "    train, val_n_test = train_test_split(\n",
    "        df, test_size=float(args.split_ratio), random_state=42, stratify=df[\"target\"]\n",
    "    )\n",
    "    validation, test = train_test_split(\n",
    "        val_n_test, test_size=float(args.split_ratio), random_state=42, stratify=val_n_test[\"target\"]\n",
    "    )\n",
    "    \n",
    "    # Save datasets\n",
    "    train.to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    validation.to_csv(f\"{base_dir}/validation/validation.csv\", header=False, index=False)\n",
    "    test.to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f53f8523-f2d1-4d75-b18e-7bcb5fd2c1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "# Define Processing Step for Feature Engineering\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "framework_version = \"1.0-1\"\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    volume_size_in_gb = processing_volume_size,\n",
    "    base_job_name=\"sklearn-pre-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "      ProcessingInput(source=s3_input_data_location, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\",\\\n",
    "                         destination = train_output_bucket),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\",\\\n",
    "                        destination = validation_output_bucket),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\",\\\n",
    "                        destination = test_output_bucket)\n",
    "    ],\n",
    "    code=\"pipeline_scripts/churn_preprocess.py\",\n",
    "    arguments =[\n",
    "        \"--split-ratio\",data_split_ratio\n",
    "    ],\n",
    ")\n",
    "step_process = ProcessingStep(name=\"LightGBMDataPreProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e4ef6-2077-4281-a769-d492e156b7f6",
   "metadata": {},
   "source": [
    "### Step 4: Define HyperParameter Tuning  Step for Classification\n",
    "\n",
    "Here we define teh training and tuning estimator for lightGBM using SageMaker Pipeline paramters. This lets up modify the structure of the tuning job like `instance_count`, `instance_type`, `volume_size` etc as well as hyperparameters for the LightGBM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a6fe5621-bc56-479f-a4ca-f7636866c0b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_boost_round': '200', 'metric': ParameterString(name='AlgorithmMetric', parameter_type=<ParameterTypeEnum.STRING: 'String'>, default_value='auc'), 'learning_rate': '0.009', 'num_leaves': '67', 'feature_fraction': '0.74', 'bagging_fraction': '0.53', 'bagging_freq': '5', 'max_depth': '11', 'min_data_in_leaf': '26', 'max_delta_step': '0.0', 'lambda_l1': '0.0', 'lambda_l2': '0.0', 'boosting': 'gbdt', 'min_gain_to_split': '0.0', 'scale_pos_weight': '1.0', 'tree_learner': 'voting', 'feature_fraction_bynode': '1.0', 'is_unbalance': 'False', 'max_bin': '255', 'num_threads': '0', 'verbosity': '1', 'use_dask': 'False'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.estimator import Estimator\n",
    "import random\n",
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "train_model_id, train_model_version, train_scope = \"lightgbm-classification-model\", \"*\", \"training\"\n",
    "from sagemaker import hyperparameters\n",
    "\n",
    "# # Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version\n",
    ")\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"num_boost_round\"] = \"200\"\n",
    "hyperparameters[\"metric\"] = algo_metric # pipeline parameter\n",
    "\n",
    "# Recommended for distributed training\n",
    "hyperparameters[\"tree_learner\"] = \"voting\" \n",
    "del hyperparameters[\"early_stopping_rounds\"]\n",
    "\n",
    "print(hyperparameters)\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tabular_estimator = Estimator(\n",
    "    role=role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir= \"model_cat\", \n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"train.py\", \n",
    "    instance_count= training_instance_count,  # pipeline paramter\n",
    "    volume_size=training_volume_size,  # pipeline paramter\n",
    "    instance_type=training_instance_type, # pipeline paramter\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=model_output_bucket,\n",
    "    sagemaker_session=pipeline_session, # Tells it its part of a Sagemaker Pipeline and not to execute individually\n",
    "    environment={\"MLFLOW_TRACKING_ARN\": ml_flow_arn}, # pipeline paramter\n",
    "    keep_alive_period_in_seconds = 1000 #Keep instance warm for fast experimentation iteration else experience cold start for each trials (note you will incur cost of warm instances)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3962cf-d2ca-4fe2-bad2-2afb33e64bb3",
   "metadata": {},
   "source": [
    "Here we also parameterize the HyperParameter tuning ranges for the job so that we can modify during SageMaker Pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9fd27af1-28c8-4af5-a80e-08314ecd9088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter, HyperparameterTuner\n",
    "\n",
    "# Define hyperparameter ranges (Pipeline parameters)\n",
    "hyperparameter_ranges_lgb = {\n",
    "    \"learning_rate\": ContinuousParameter(learning_rate_min , learning_rate_max , scaling_type=\"Auto\"),\n",
    "    \"num_boost_round\": IntegerParameter(num_boost_round_min , num_boost_round_max),\n",
    "    \"num_leaves\": IntegerParameter(num_leaves_min , num_leaves_max),\n",
    "    \"feature_fraction\": ContinuousParameter(feature_fraction_min, feature_fraction_max),\n",
    "    \"bagging_fraction\": ContinuousParameter(bagging_fraction_min, bagging_fraction_max),\n",
    "    \"bagging_freq\": IntegerParameter(bagging_freq_min, bagging_freq_max),\n",
    "    \"max_depth\": IntegerParameter(max_depth_min, max_depth_max),\n",
    "    \"min_data_in_leaf\": IntegerParameter(min_data_in_leaf_min, min_data_in_leaf_max),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator = tabular_estimator,\n",
    "    objective_metric_name = tuner_objective_metric, # pipeline paramter\n",
    "    hyperparameter_ranges = hyperparameter_ranges_lgb,  # pipeline paramter\n",
    "    metric_definitions = [{\"Name\": tuner_objective_metric, \"Regex\": Join(on=':',values=[tuner_objective_metric ,\" ([0-9\\\\.]+)\" ] )}], # pipeline paramter\n",
    "    max_jobs=max_tuning_jobs, # pipeline paramter\n",
    "    max_parallel_jobs=max_tuning_parallel_job, # pipeline paramter\n",
    "    objective_type=optimization_direction, # pipeline paramter\n",
    "    strategy = tuning_strategy # pipeline paramter\n",
    ") \n",
    "\n",
    "# Here we create an implicit dependencies between the processing step and Tuning step\n",
    "hpo_args = tuner.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "step_tuning = TuningStep(\n",
    "    name=\"LightGBMClassifierHyperParameterTuning\",\n",
    "    step_args=hpo_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9b418-0b96-4670-903e-84bf1b7045ab",
   "metadata": {},
   "source": [
    "### Step 5: Define Tunning  Step for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec8b67b3-7c85-4021-8e1b-e7caa9ee6431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_boost_round': '200', 'metric': ParameterString(name='AlgorithmMetric', parameter_type=<ParameterTypeEnum.STRING: 'String'>, default_value='auc'), 'learning_rate': '0.009', 'num_leaves': '67', 'feature_fraction': '0.74', 'bagging_fraction': '0.53', 'bagging_freq': '5', 'max_depth': '11', 'min_data_in_leaf': '26', 'max_delta_step': '0.0', 'lambda_l1': '0.0', 'lambda_l2': '0.0', 'boosting': 'gbdt', 'min_gain_to_split': '0.0', 'tree_learner': 'voting', 'feature_fraction_bynode': '1.0', 'is_unbalance': 'False', 'max_bin': '255', 'tweedie_variance_power': '1.5', 'num_threads': '0', 'verbosity': '1', 'use_dask': 'False'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.estimator import Estimator\n",
    "import random\n",
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "train_model_id, train_model_version, train_scope = \"lightgbm-regression-model\", \"*\", \"training\"\n",
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version\n",
    ")\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"num_boost_round\"] = \"200\"\n",
    "hyperparameters[\"metric\"] = algo_metric\n",
    "\n",
    "# Recommended for distributed training\n",
    "hyperparameters[\"tree_learner\"] = \"voting\" \n",
    "del hyperparameters[\"early_stopping_rounds\"]\n",
    "print(hyperparameters)\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "reg_estimator = Estimator(\n",
    "    role=role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir= \"model_reg\", \n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"train.py\", \n",
    "    instance_count= training_instance_count,  \n",
    "    volume_size=training_volume_size, \n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=model_output_bucket,\n",
    "    sagemaker_session=pipeline_session, # Tells it its part of a Sagemaker Pipeline and not to execute individually\n",
    "    environment={\"MLFLOW_TRACKING_ARN\": ml_flow_arn},\n",
    "    keep_alive_period_in_seconds = 1000 #Keep instance warm for fast experimentation iteration else experience cold start for each trials (note you will incur cost of warm instances)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d7800d7b-e5c1-4773-b266-356616a03e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter, HyperparameterTuner\n",
    "\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges_lgb = {\n",
    "    \"learning_rate\": ContinuousParameter(learning_rate_min , learning_rate_max , scaling_type=\"Auto\"),\n",
    "  \"num_boost_round\": IntegerParameter(num_boost_round_min , num_boost_round_max),\n",
    "    \"num_leaves\": IntegerParameter(num_leaves_min , num_leaves_max),\n",
    "    \"feature_fraction\": ContinuousParameter(feature_fraction_min, feature_fraction_max),\n",
    "    \"bagging_fraction\": ContinuousParameter(bagging_fraction_min, bagging_fraction_max),\n",
    "    \"bagging_freq\": IntegerParameter(bagging_freq_min, bagging_freq_max),\n",
    "    \"max_depth\": IntegerParameter(max_depth_min, max_depth_max),\n",
    "    \"min_data_in_leaf\": IntegerParameter(min_data_in_leaf_min, min_data_in_leaf_max),\n",
    "}\n",
    "\n",
    "tuner_reg = HyperparameterTuner(\n",
    "    estimator = reg_estimator,\n",
    "    objective_metric_name = tuner_objective_metric,\n",
    "    hyperparameter_ranges = hyperparameter_ranges_lgb, \n",
    "    metric_definitions = [{\"Name\": tuner_objective_metric, \"Regex\": Join(on=':',values=[tuner_objective_metric ,\" ([0-9\\\\.]+)\" ] )}],\n",
    "    max_jobs=max_tuning_jobs,\n",
    "    max_parallel_jobs=max_tuning_parallel_job, \n",
    "    objective_type=optimization_direction,\n",
    "    strategy = tuning_strategy)\n",
    "\n",
    "# Here we create an implicit dependencies between the processing step and Tuning step\n",
    "hpo_args_reg = tuner_reg.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "step_tuning_reg = TuningStep(\n",
    "    name=\"LightGBMHyperParameterTuningRegression\",\n",
    "    step_args=hpo_args_reg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e90a2b3-6de9-4e57-8b95-dc2e4c16100b",
   "metadata": {},
   "source": [
    "### Step 6: Define a condition step to Route to the appropiate Hyperparameter Tuning Step (Regression vs Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "362a1c48-25c4-4c33-bdbe-13779a9efba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThan,ConditionEquals\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "# Define Condition. Here we evaluate the condition based on the training task passed as a pipeline parameter \n",
    "cond_task = ConditionEquals(\n",
    "    left=supervised_training_task,\n",
    "    right=\"classification\",\n",
    ")\n",
    "\n",
    "# Condition Step\n",
    "\"\"\"\n",
    "Here we create a condition syep to swith the branch based on training task type. \n",
    "Run Classifier tuner if its a classification model or Regression tuner if its a regression model\n",
    "\"\"\"\n",
    "step_cond = ConditionStep(\n",
    "    depends_on = [step_process], # Depends on the processing step\n",
    "    name=\"TrainingTaskTypes\",\n",
    "    conditions=[cond_task], \n",
    "    if_steps=[step_tuning], # If condition is true\n",
    "    else_steps=[step_tuning_reg] # If condition is false\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafd7f0-907b-4b55-9c43-e67888e26cc6",
   "metadata": {},
   "source": [
    "## Build and Trigger the pipeline run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648ad67-80e5-404f-b47e-27df019c6d81",
   "metadata": {},
   "source": [
    "After defining all of the component steps, you can assemble them into a Pipelines object. You don’t need to specify the order of pipeline because Pipelines automatically infers the order sequence based on the dependencies between the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3123803e-f286-4acc-b016-4620eb2017a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Version': '2020-12-01', 'Metadata': {}, 'Parameters': [{'Name': 'LearningRateMin', 'Type': 'Float', 'DefaultValue': 0.001}, {'Name': 'LearningRateMax', 'Type': 'Float', 'DefaultValue': 0.1}, {'Name': 'NumLeavesMin', 'Type': 'Integer', 'DefaultValue': 10}, {'Name': 'NumLeavesMax', 'Type': 'Integer', 'DefaultValue': 50}, {'Name': 'FeatureFractionMin', 'Type': 'Float', 'DefaultValue': 0.1}, {'Name': 'FeatureFractionMax', 'Type': 'Float', 'DefaultValue': 1.0}, {'Name': 'BaggingFractionMin', 'Type': 'Float', 'DefaultValue': 0.1}, {'Name': 'BaggingFractionMax', 'Type': 'Float', 'DefaultValue': 1.0}, {'Name': 'BaggingFreqMin', 'Type': 'Integer', 'DefaultValue': 1}, {'Name': 'BaggingFreqMax', 'Type': 'Integer', 'DefaultValue': 10}, {'Name': 'MaxDepthMin', 'Type': 'Integer', 'DefaultValue': 5}, {'Name': 'MaxDepthMax', 'Type': 'Integer', 'DefaultValue': 30}, {'Name': 'MinDataInLeafMin', 'Type': 'Integer', 'DefaultValue': 10}, {'Name': 'MinDataInLeafMax', 'Type': 'Integer', 'DefaultValue': 50}, {'Name': 'NumberOfBoostRoundMax', 'Type': 'Integer', 'DefaultValue': 30}, {'Name': 'NumberOfBoostRoundMin', 'Type': 'Integer', 'DefaultValue': 2}, {'Name': 'ProcessingVolumeSize', 'Type': 'Integer', 'DefaultValue': 30}, {'Name': 'TrainingVolumeSize', 'Type': 'Integer', 'DefaultValue': 30}, {'Name': 'TunerMetricDefinition', 'Type': 'String', 'DefaultValue': 'auc: ([0-9\\\\.]+)'}, {'Name': 'TunerObjectiveMetric', 'Type': 'String', 'DefaultValue': 'auc'}, {'Name': 'AlgorithmMetric', 'Type': 'String', 'DefaultValue': 'auc'}, {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1}, {'Name': 'ProcessingInstanceType', 'Type': 'String', 'DefaultValue': 'ml.m5.xlarge'}, {'Name': 'TrainingInstanceType', 'Type': 'String', 'DefaultValue': 'ml.m5.xlarge'}, {'Name': 'TrainingInstanceCount', 'Type': 'Integer', 'DefaultValue': 2}, {'Name': 'ModelApprovalStatus', 'Type': 'String', 'DefaultValue': 'PendingManualApproval'}, {'Name': 'ModelOutput', 'Type': 'String', 'DefaultValue': 's3://fairstone/output/model'}, {'Name': 'TrainOutput', 'Type': 'String', 'DefaultValue': 's3://fairstone/output/train'}, {'Name': 'ValidationOutput', 'Type': 'String', 'DefaultValue': 's3://fairstone/output/validation'}, {'Name': 'TestOutput', 'Type': 'String', 'DefaultValue': 's3://fairstone/output/test'}, {'Name': 'MaxTuningJobs', 'Type': 'Integer', 'DefaultValue': 2}, {'Name': 'TuningParallelJobs', 'Type': 'Integer', 'DefaultValue': 3}, {'Name': 'TuningStrategy', 'Type': 'String', 'DefaultValue': 'Bayesian', 'EnumValues': ['Bayesian', 'Random', 'Grid', 'Hyperband']}, {'Name': 'OptimizationDirection', 'Type': 'String', 'DefaultValue': 'Maximize', 'EnumValues': ['Maximize', 'Minimize']}, {'Name': 'MLflow', 'Type': 'String', 'DefaultValue': 'arn:aws:sagemaker:us-east-1:715253196401:mlflow-tracking-server/test'}, {'Name': 'TrainingTask', 'Type': 'String', 'DefaultValue': 'classification', 'EnumValues': ['classification', 'regression']}, {'Name': 'EvalThreshold', 'Type': 'Float', 'DefaultValue': 0.75}, {'Name': 'S3InputDataURI', 'Type': 'String', 'DefaultValue': 's3://fairstone/ml_training/churn.csv'}, {'Name': 'DataSplitRatio', 'Type': 'String', 'DefaultValue': '0.3'}], 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'}, 'TrialName': {'Get': 'Execution.PipelineExecutionId'}}, 'Steps': [{'Name': 'TrainingTaskTypes', 'Type': 'Condition', 'Arguments': {'Conditions': [{'Type': 'Equals', 'LeftValue': {'Get': 'Parameters.TrainingTask'}, 'RightValue': 'classification'}], 'IfSteps': [{'Name': 'LightGBMClassifierHyperParameterTuning', 'Type': 'Tuning', 'Arguments': {'HyperParameterTuningJobConfig': {'Strategy': {'Get': 'Parameters.TuningStrategy'}, 'ResourceLimits': {'MaxNumberOfTrainingJobs': {'Get': 'Parameters.MaxTuningJobs'}, 'MaxParallelTrainingJobs': {'Get': 'Parameters.TuningParallelJobs'}}, 'TrainingJobEarlyStoppingType': 'Off', 'HyperParameterTuningJobObjective': {'Type': {'Get': 'Parameters.OptimizationDirection'}, 'MetricName': {'Get': 'Parameters.TunerObjectiveMetric'}}, 'ParameterRanges': {'ContinuousParameterRanges': [{'Name': 'learning_rate', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.LearningRateMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.LearningRateMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'feature_fraction', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.FeatureFractionMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.FeatureFractionMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'bagging_fraction', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.BaggingFractionMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.BaggingFractionMax'}]}}, 'ScalingType': 'Auto'}], 'CategoricalParameterRanges': [], 'IntegerParameterRanges': [{'Name': 'num_boost_round', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.NumberOfBoostRoundMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.NumberOfBoostRoundMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'num_leaves', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.NumLeavesMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.NumLeavesMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'bagging_freq', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.BaggingFreqMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.BaggingFreqMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'max_depth', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.MaxDepthMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.MaxDepthMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'min_data_in_leaf', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.MinDataInLeafMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.MinDataInLeafMax'}]}}, 'ScalingType': 'Auto'}]}}, 'TrainingJobDefinition': {'StaticHyperParameters': {'metric': {'Get': 'Parameters.AlgorithmMetric'}, 'max_delta_step': '\"0.0\"', 'lambda_l1': '\"0.0\"', 'lambda_l2': '\"0.0\"', 'boosting': '\"gbdt\"', 'min_gain_to_split': '\"0.0\"', 'scale_pos_weight': '\"1.0\"', 'tree_learner': '\"voting\"', 'feature_fraction_bynode': '\"1.0\"', 'is_unbalance': '\"False\"', 'max_bin': '\"255\"', 'num_threads': '\"0\"', 'verbosity': '\"1\"', 'use_dask': '\"False\"', 'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-31-22-30-35-948/source/sourcedir.tar.gz\"', 'sagemaker_program': '\"train.py\"', 'sagemaker_container_log_level': '20', 'sagemaker_job_name': '\"sagemaker-jumpstart-2024-10-31-22-30-35-948\"', 'sagemaker_region': '\"us-east-1\"'}, 'RoleArn': 'arn:aws:iam::715253196401:role/service-role/AmazonSageMaker-ExecutionRole-20230601T114337', 'OutputDataConfig': {'S3OutputPath': {'Get': 'Parameters.ModelOutput'}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 360000}, 'HyperParameterTuningResourceConfig': {'InstanceCount': {'Get': 'Parameters.TrainingInstanceCount'}, 'InstanceType': {'Get': 'Parameters.TrainingInstanceType'}, 'VolumeSizeInGB': {'Get': 'Parameters.TrainingVolumeSize'}}, 'AlgorithmSpecification': {'TrainingInputMode': 'File', 'MetricDefinitions': [{'Name': {'Get': 'Parameters.TunerObjectiveMetric'}, 'Regex': {'Std:Join': {'On': ':', 'Values': [{'Get': 'Parameters.TunerObjectiveMetric'}, ' ([0-9\\\\.]+)']}}}], 'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.9.0-cpu-py38'}, 'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': {'Get': \"Steps.LightGBMDataPreProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"}, 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'text/csv', 'ChannelName': 'train'}, {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': {'Get': \"Steps.LightGBMDataPreProcess.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"}, 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'text/csv', 'ChannelName': 'validation'}, {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://jumpstart-cache-prod-us-east-1/lightgbm-training/train-lightgbm-classification-model.tar.gz', 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'application/x-sagemaker-model', 'InputMode': 'File', 'ChannelName': 'model'}], 'Environment': {'MLFLOW_TRACKING_ARN': {'Get': 'Parameters.MLflow'}}}, 'Tags': [{'Key': 'aws-jumpstart-training-model-uri', 'Value': 's3://jumpstart-cache-prod-us-east-1/lightgbm-training/train-lightgbm-classification-model.tar.gz'}]}}], 'ElseSteps': [{'Name': 'LightGBMHyperParameterTuningRegression', 'Type': 'Tuning', 'Arguments': {'HyperParameterTuningJobConfig': {'Strategy': {'Get': 'Parameters.TuningStrategy'}, 'ResourceLimits': {'MaxNumberOfTrainingJobs': {'Get': 'Parameters.MaxTuningJobs'}, 'MaxParallelTrainingJobs': {'Get': 'Parameters.TuningParallelJobs'}}, 'TrainingJobEarlyStoppingType': 'Off', 'HyperParameterTuningJobObjective': {'Type': {'Get': 'Parameters.OptimizationDirection'}, 'MetricName': {'Get': 'Parameters.TunerObjectiveMetric'}}, 'ParameterRanges': {'ContinuousParameterRanges': [{'Name': 'learning_rate', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.LearningRateMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.LearningRateMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'feature_fraction', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.FeatureFractionMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.FeatureFractionMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'bagging_fraction', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.BaggingFractionMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.BaggingFractionMax'}]}}, 'ScalingType': 'Auto'}], 'CategoricalParameterRanges': [], 'IntegerParameterRanges': [{'Name': 'num_boost_round', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.NumberOfBoostRoundMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.NumberOfBoostRoundMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'num_leaves', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.NumLeavesMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.NumLeavesMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'bagging_freq', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.BaggingFreqMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.BaggingFreqMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'max_depth', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.MaxDepthMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.MaxDepthMax'}]}}, 'ScalingType': 'Auto'}, {'Name': 'min_data_in_leaf', 'MinValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.MinDataInLeafMin'}]}}, 'MaxValue': {'Std:Join': {'On': '', 'Values': [{'Get': 'Parameters.MinDataInLeafMax'}]}}, 'ScalingType': 'Auto'}]}}, 'TrainingJobDefinition': {'StaticHyperParameters': {'metric': {'Get': 'Parameters.AlgorithmMetric'}, 'max_delta_step': '\"0.0\"', 'lambda_l1': '\"0.0\"', 'lambda_l2': '\"0.0\"', 'boosting': '\"gbdt\"', 'min_gain_to_split': '\"0.0\"', 'tree_learner': '\"voting\"', 'feature_fraction_bynode': '\"1.0\"', 'is_unbalance': '\"False\"', 'max_bin': '\"255\"', 'tweedie_variance_power': '\"1.5\"', 'num_threads': '\"0\"', 'verbosity': '\"1\"', 'use_dask': '\"False\"', 'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-715253196401/sagemaker-jumpstart-2024-10-31-22-30-36-760/source/sourcedir.tar.gz\"', 'sagemaker_program': '\"train.py\"', 'sagemaker_container_log_level': '20', 'sagemaker_job_name': '\"sagemaker-jumpstart-2024-10-31-22-30-36-760\"', 'sagemaker_region': '\"us-east-1\"'}, 'RoleArn': 'arn:aws:iam::715253196401:role/service-role/AmazonSageMaker-ExecutionRole-20230601T114337', 'OutputDataConfig': {'S3OutputPath': {'Get': 'Parameters.ModelOutput'}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 360000}, 'HyperParameterTuningResourceConfig': {'InstanceCount': {'Get': 'Parameters.TrainingInstanceCount'}, 'InstanceType': {'Get': 'Parameters.TrainingInstanceType'}, 'VolumeSizeInGB': {'Get': 'Parameters.TrainingVolumeSize'}}, 'AlgorithmSpecification': {'TrainingInputMode': 'File', 'MetricDefinitions': [{'Name': {'Get': 'Parameters.TunerObjectiveMetric'}, 'Regex': {'Std:Join': {'On': ':', 'Values': [{'Get': 'Parameters.TunerObjectiveMetric'}, ' ([0-9\\\\.]+)']}}}], 'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.9.0-cpu-py38'}, 'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': {'Get': \"Steps.LightGBMDataPreProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"}, 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'text/csv', 'ChannelName': 'train'}, {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': {'Get': \"Steps.LightGBMDataPreProcess.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"}, 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'text/csv', 'ChannelName': 'validation'}, {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://jumpstart-cache-prod-us-east-1/lightgbm-training/train-lightgbm-regression-model.tar.gz', 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'application/x-sagemaker-model', 'InputMode': 'File', 'ChannelName': 'model'}], 'Environment': {'MLFLOW_TRACKING_ARN': {'Get': 'Parameters.MLflow'}}}, 'Tags': [{'Key': 'aws-jumpstart-training-model-uri', 'Value': 's3://jumpstart-cache-prod-us-east-1/lightgbm-training/train-lightgbm-regression-model.tar.gz'}]}}]}, 'DependsOn': ['LightGBMDataPreProcess']}, {'Name': 'LightGBMDataPreProcess', 'Type': 'Processing', 'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'}, 'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'}, 'VolumeSizeInGB': {'Get': 'Parameters.ProcessingVolumeSize'}}}, 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3', 'ContainerArguments': ['--split-ratio', {'Get': 'Parameters.DataSplitRatio'}], 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/churn_preprocess.py']}, 'RoleArn': 'arn:aws:iam::715253196401:role/service-role/AmazonSageMaker-ExecutionRole-20230601T114337', 'ProcessingInputs': [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': {'Get': 'Parameters.S3InputDataURI'}, 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-715253196401/LightGBM-ML-Pipeline/code/8bfcc7b244a13a0833cb390dc0d312a6/churn_preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train', 'AppManaged': False, 'S3Output': {'S3Uri': {'Get': 'Parameters.TrainOutput'}, 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'validation', 'AppManaged': False, 'S3Output': {'S3Uri': {'Get': 'Parameters.ValidationOutput'}, 'LocalPath': '/opt/ml/processing/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'AppManaged': False, 'S3Output': {'S3Uri': {'Get': 'Parameters.TestOutput'}, 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]}}}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        # LightGBM tunable parameters\n",
    "        learning_rate_min,\n",
    "        learning_rate_max,\n",
    "        num_leaves_min,\n",
    "        num_leaves_max,\n",
    "        feature_fraction_min,\n",
    "        feature_fraction_max,\n",
    "        bagging_fraction_min,\n",
    "        bagging_fraction_max,\n",
    "        bagging_freq_min,\n",
    "        bagging_freq_max,\n",
    "        max_depth_min,\n",
    "        max_depth_max,\n",
    "        min_data_in_leaf_min,\n",
    "        min_data_in_leaf_max,\n",
    "        num_boost_round_max,\n",
    "        num_boost_round_min,\n",
    "\n",
    "        # Other parameters\n",
    "        processing_volume_size,\n",
    "        training_volume_size,\n",
    "        tuner_metric_definition,\n",
    "        tuner_objective_metric,\n",
    "        algo_metric,\n",
    "        processing_instance_count,\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        training_instance_count,\n",
    "        model_approval_status,\n",
    "        model_output_bucket,\n",
    "        train_output_bucket,\n",
    "        validation_output_bucket,\n",
    "        test_output_bucket,\n",
    "        max_tuning_jobs,\n",
    "        max_tuning_parallel_job,\n",
    "        tuning_strategy,\n",
    "        optimization_direction,\n",
    "        ml_flow_arn,\n",
    "        supervised_training_task,\n",
    "        model_evaluation_threshold,\n",
    "        s3_input_data_location,\n",
    "        data_split_ratio,\n",
    "    ],\n",
    "    steps=[step_cond], # we pass only the condition step as we have declared all steps as dependencies to the condition step\n",
    ")\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "print(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ee7f3338-f9d3-49b5-8d70-9bfdb7676770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:715253196401:pipeline/LightGBM-ML-Pipeline/execution/g8mop5lkt7la', sagemaker_session=<sagemaker.session.Session object at 0x7f23cfeaf700>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new or update existing Pipeline\n",
    "pipeline.upsert(role_arn=role)\n",
    "# start Pipeline execution\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d1543-13c4-47e4-86df-32ab166c31e6",
   "metadata": {},
   "source": [
    "## Create a new pipeline execution with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31c5ddf3-a779-401d-bb09-6a96e1284623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:715253196401:pipeline/LightGBM-ML-Pipeline/execution/js8l47918nca', sagemaker_session=<sagemaker.session.Session object at 0x7f23cfeaf700>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets modify the objective metric to \"binary_error\" and create a new pipeline execution. We use the Pipelien Parameter Names instead of teh varaible names\n",
    "pipeline.start(\n",
    "    parameters=dict(\n",
    "        AlgorithmMetric=\"binary_error\",\n",
    "        TunerObjectiveMetric = \"binary_error\",\n",
    "        TunerMetricDefinition=\"binary_error: ([0-9\\\\.]+)\",\n",
    "        OptimizationDirection = \"Minimize\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5bb64-bfe1-437b-8064-f7111564d49c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
