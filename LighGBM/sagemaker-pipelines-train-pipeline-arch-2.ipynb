{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce11b1f7-803e-4623-8499-97478594f1e9",
   "metadata": {},
   "source": [
    "# Train Pipeline (SageMaker Pipelines) Arch Two\n",
    "\n",
    "In this approach, the ML tuning pipelines is set up as a framework. This enables the user to send a custom code as an S3 URI parameter as part of invoking the pipeline. This is a flexible approach that allows user to pass their training script for model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b06ac-9c5d-49ca-8df6-dc53283494be",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4cd6d8-b36d-474a-af41-027c26cbff06",
   "metadata": {},
   "source": [
    "The following diagram illustrates the high-level architecture of the ML workflow with the different steps to train the model.\n",
    "\n",
    "![](images/pipeline1.PNG)\n",
    "\n",
    "Train Pipeline consists of the following steps:\n",
    "\n",
    "1. Preprocess data to build features required and split data into train, validation, and test datasets.\n",
    "2. Apply hyperparameter tuning based on the ranges provided with the SageMaker LightGBM framework to give the best model, which is determined based on AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6d4de-4761-430a-9145-07f7cf3c264c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install sagemaker -U\n",
    "pip install boto3 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c0074-467f-46e9-a57b-091e8db6d171",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184d1cc-f1aa-469f-80b9-062f0b7248b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"churn.txt\")\n",
    "df.to_csv(\"churn.csv\",index=False)\n",
    "bucket = \"fairstone\" #Bucket Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c4d57-4ebd-461a-a206-7373eb03f342",
   "metadata": {},
   "source": [
    "#### Uplaod training Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0152a-5b68-4594-978c-b66b1fa7c923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp churn.csv s3://{bucket}/ml_training/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f1230-2abd-4c72-99cb-6f9a564a7ed7",
   "metadata": {},
   "source": [
    "#### Package Code and dependencies used for training Job \n",
    "\n",
    "Our training code and dependencies are stored in the `model_cat` directory. To prepare them for sagemaker:\n",
    "1. Modify file read and write location from **local directory** to SageMaker Container Paths as shown in the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/model-train-storage.html)\n",
    "\n",
    "<img src=\"images/sagemaker train path.PNG\" width=\"800\"/>\n",
    "\n",
    "\n",
    "2. Compress the training code and dependencies to a **`tar.gz`**  format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1ae20-e837-4b73-956a-6cc3321346b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change to the model_cat directory\n",
    "%cd model_cat\n",
    "\n",
    "# Compress contents of model_cat (training logic) to tar.gz format and save the archive in the parent directory\n",
    "!tar zcvf ../code.tar.gz *\n",
    "\n",
    "# Change to the parent directory\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecbc4b9-dadf-4bb1-8072-506dc9296e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload compressed code archive to S3\n",
    "code_s3_location = f\"s3://{bucket}/training_code/code.tar.gz\"\n",
    "! aws s3 cp code.tar.gz {code_s3_location}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d89f34-3526-41df-a964-5f2a7112f48c",
   "metadata": {},
   "source": [
    "### Step 1: Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70816378-1c97-4511-931c-b1dbd3d91e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import pandas as pd \n",
    "import sagemaker \n",
    "from sagemaker.workflow.pipeline_context import PipelineSession \n",
    "\n",
    "s3_client = boto3.resource('s3') \n",
    "pipeline_name = \"LightGBM-ML-Pipeline-3\" \n",
    "sagemaker_session = sagemaker.session.Session(default_bucket=bucket) \n",
    "region = sagemaker_session.boto_region_name \n",
    "role = sagemaker.get_execution_role() \n",
    "pipeline_session = PipelineSession() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34806a9-8d49-4a03-9c3b-a9000b5298f8",
   "metadata": {},
   "source": [
    "# Step 1b: Modify Config File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045889c0-4e02-44f3-aedd-de3ea13206f7",
   "metadata": {},
   "source": [
    "#### Here we use a configuration file to set the defaults for our Pipeline parameters\n",
    "\n",
    "Change the value of these config file (bucket name, mlflow tracking server etc.) to ones that match your account resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1378ff-0867-4a46-8720-d996e9badd62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"training_config2.json\", \"r\") as f:\n",
    "    print(f\"Here is a preview of the configuration file:\\n\\n {json.loads(f.read())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc57301-08f3-458d-80cf-307c683e0f15",
   "metadata": {},
   "source": [
    "## Step 2: Define SageMaker Pipeline Parameters \n",
    "\n",
    "SageMaker Pipelines supports parameterization. This allows ausers to alter the values of each parameters for each initiated pipeline execution. You can add or remove parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d50be-8cef-45e6-9f7c-827c05a98c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    "    ParameterBoolean\n",
    ")\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "import json\n",
    "# Read Default Config from Configuration File\n",
    "with open('training_config2.json', 'r') as config_file:\n",
    "    training_config = json.load(config_file)\n",
    "\n",
    "# Now you can use training_config dictionary in your script\n",
    "\n",
    "# LightGBM Algorithm Parameters\n",
    "learning_rate = ParameterString(name=\"learning_rate\", default_value=training_config[\"learning_rate\"])\n",
    "num_leaves = ParameterString(name=\"num_leaves\", default_value=training_config[\"num_leaves\"])\n",
    "feature_fraction = ParameterString(name=\"feature_fraction\", default_value=training_config[\"feature_fraction\"])\n",
    "bagging_fraction = ParameterString(name=\"bagging_fraction\", default_value=training_config[\"bagging_fraction\"])\n",
    "bagging_freq = ParameterString(name=\"bagging_freq\", default_value=training_config[\"bagging_freq\"])\n",
    "max_depth = ParameterString(name=\"max_depth\", default_value=training_config[\"max_depth\"])\n",
    "min_data_in_leaf = ParameterString(name=\"min_data_in_leaf\", default_value=training_config[\"min_data_in_leaf\"])\n",
    "max_delta_step = ParameterString(name=\"max_delta_step\", default_value=training_config[\"max_delta_step\"])\n",
    "lambda_l1 = ParameterString(name=\"lambda_l1\", default_value=training_config[\"lambda_l1\"])\n",
    "lambda_l2 = ParameterString(name=\"lambda_l2\", default_value=training_config[\"lambda_l2\"])\n",
    "boosting = ParameterString(name=\"boosting\", default_value=training_config[\"boosting\"])\n",
    "min_gain_to_split = ParameterString(name=\"min_gain_to_split\", default_value=training_config[\"min_gain_to_split\"])\n",
    "scale_pos_weight = ParameterString(name=\"scale_pos_weight\", default_value=training_config[\"scale_pos_weight\"])\n",
    "tree_learner = ParameterString(name=\"tree_learner\", default_value=training_config[\"tree_learner\"])\n",
    "feature_fraction_bynode = ParameterString(name=\"feature_fraction_bynode\", default_value=training_config[\"feature_fraction_bynode\"])\n",
    "is_unbalance = ParameterString(name=\"is_unbalance\", default_value=training_config[\"is_unbalance\"])\n",
    "max_bin = ParameterString(name=\"max_bin\", default_value=training_config[\"max_bin\"])\n",
    "num_threads = ParameterString(name=\"num_threads\", default_value=training_config[\"num_threads\"])\n",
    "verbosity = ParameterString(name=\"verbosity\", default_value=training_config[\"verbosity\"])\n",
    "use_dask = ParameterString(name=\"use_dask\", default_value=training_config[\"use_dask\"])\n",
    "num_boost_round = ParameterString(name=\"NumberBoostRound\", default_value=training_config[\"NumberBoostRound\"])\n",
    "\n",
    "# LightGBM tunable parameters for SageMaker Pipelines\n",
    "learning_rate_min = ParameterFloat(name=\"LearningRateMin\", default_value=float(training_config[\"LearningRateMin\"]))\n",
    "learning_rate_max = ParameterFloat(name=\"LearningRateMax\", default_value=float(training_config[\"LearningRateMax\"]))\n",
    "\n",
    "num_boost_round_min = ParameterInteger(name=\"NumberOfBoostRoundMin\", default_value=int(training_config[\"NumberOfBoostRoundMin\"]))\n",
    "num_boost_round_max = ParameterInteger(name=\"NumberOfBoostRoundMax\", default_value=int(training_config[\"NumberOfBoostRoundMax\"]))\n",
    "\n",
    "num_leaves_min = ParameterInteger(name=\"NumLeavesMin\", default_value=int(training_config[\"NumLeavesMin\"]))\n",
    "num_leaves_max = ParameterInteger(name=\"NumLeavesMax\", default_value=int(training_config[\"NumLeavesMax\"]))\n",
    "\n",
    "feature_fraction_min = ParameterFloat(name=\"FeatureFractionMin\", default_value=float(training_config[\"FeatureFractionMin\"]))\n",
    "feature_fraction_max = ParameterFloat(name=\"FeatureFractionMax\", default_value=float(training_config[\"FeatureFractionMax\"]))\n",
    "\n",
    "bagging_fraction_min = ParameterFloat(name=\"BaggingFractionMin\", default_value=float(training_config[\"BaggingFractionMin\"]))\n",
    "bagging_fraction_max = ParameterFloat(name=\"BaggingFractionMax\", default_value=float(training_config[\"BaggingFractionMax\"]))\n",
    "\n",
    "bagging_freq_min = ParameterInteger(name=\"BaggingFreqMin\", default_value=int(training_config[\"BaggingFreqMin\"]))\n",
    "bagging_freq_max = ParameterInteger(name=\"BaggingFreqMax\", default_value=int(training_config[\"BaggingFreqMax\"]))\n",
    "\n",
    "max_depth_min = ParameterInteger(name=\"MaxDepthMin\", default_value=int(training_config[\"MaxDepthMin\"]))\n",
    "max_depth_max = ParameterInteger(name=\"MaxDepthMax\", default_value=int(training_config[\"MaxDepthMax\"]))\n",
    "\n",
    "min_data_in_leaf_min = ParameterInteger(name=\"MinDataInLeafMin\", default_value=int(training_config[\"MinDataInLeafMin\"]))\n",
    "min_data_in_leaf_max = ParameterInteger(name=\"MinDataInLeafMax\", default_value=int(training_config[\"MinDataInLeafMax\"]))\n",
    "\n",
    "tuner_objective_metric = ParameterString(name=\"TunerObjectiveMetric\", default_value=training_config[\"TunerObjectiveMetric\"])\n",
    "tuner_metric_definition = ParameterString(name=\"TunerMetricDefinition\", default_value=training_config[\"TunerMetricDefinition\"])\n",
    "algo_metric = ParameterString(name=\"AlgorithmMetric\", default_value=training_config[\"AlgorithmMetric\"])\n",
    "\n",
    "# Automatic Model Tuning parameters\n",
    "max_tuning_jobs = ParameterInteger(name=\"MaxTuningJobs\", default_value=int(training_config[\"MaxTuningJobs\"]))\n",
    "max_tuning_parallel_job = ParameterInteger(name=\"TuningParallelJobs\", default_value=int(training_config[\"TuningParallelJobs\"]))\n",
    "tuning_strategy = ParameterString(name=\"TuningStrategy\", default_value=training_config[\"TuningStrategy\"], enum_values=[\"Bayesian\", \"Random\", \"Grid\", \"Hyperband\"])\n",
    "optimization_direction = ParameterString(name=\"OptimizationDirection\", default_value=training_config[\"OptimizationDirection\"], enum_values=[\"Maximize\", \"Minimize\"])\n",
    "\n",
    "# Infra Parameters\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=int(training_config[\"ProcessingInstanceCount\"]))\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=training_config[\"ProcessingInstanceType\"])\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=training_config[\"TrainingInstanceType\"])\n",
    "training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=int(training_config[\"TrainingInstanceCount\"]))\n",
    "training_volume_size = ParameterInteger(name=\"TrainingVolumeSize\", default_value=int(training_config[\"TrainingVolumeSize\"]))\n",
    "processing_volume_size = ParameterInteger(name=\"ProcessingVolumeSize\", default_value=int(training_config[\"ProcessingVolumeSize\"]))\n",
    "\n",
    "# Artifacts location Parameters\n",
    "model_output_bucket = ParameterString(name=\"ModelOutput\", default_value=training_config[\"ModelOutput\"])\n",
    "train_output_bucket = ParameterString(name=\"TrainOutput\", default_value=training_config[\"TrainOutput\"])\n",
    "validation_output_bucket = ParameterString(name=\"ValidationOutput\", default_value=training_config[\"ValidationOutput\"])\n",
    "test_output_bucket = ParameterString(name=\"TestOutput\", default_value=training_config[\"TestOutput\"])\n",
    "s3_input_data_location = ParameterString(name=\"S3InputDataURI\", default_value=training_config[\"S3InputDataURI\"])\n",
    "s3_training_code_location = ParameterString(name=\"S3TrainingCodeLocation\",default_value=training_config[\"S3TrainingCodeLocation\"])\n",
    "training_code_entry_point = ParameterString( name=\"CodeEntryPoint\", default_value=training_config[\"CodeEntryPoint\"]) # Name of the training script\n",
    "\n",
    "# Mlflow\n",
    "ml_flow_arn = ParameterString( name=\"MLflow\", default_value=training_config[\"MLflow\"])\n",
    "\n",
    "model_evaluation_threshold = ParameterFloat( name=\"EvalThreshold\", default_value=float(training_config[\"EvalThreshold\"]))\n",
    "data_split_ratio = ParameterString( name=\"DataSplitRatio\", default_value=training_config[\"DataSplitRatio\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42ce11-837e-4d3c-bcba-ea37e9e43bed",
   "metadata": {},
   "source": [
    "### Step 3: Define Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01574fd-ccac-4fa0-96d7-d7eaab248636",
   "metadata": {},
   "source": [
    "#### Create a Logic for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b6816-3ef0-43ec-a5eb-615a8f804917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile \"pipeline_scripts/churn_preprocess.py\"\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from time import strftime, gmtime\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def load_and_combine_csv_files(directory):\n",
    "    \"\"\"\n",
    "    Load all CSV files from a directory and combine them into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "    directory (str): Path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Combined DataFrame of all CSV files.\n",
    "    \"\"\"\n",
    "    # Use glob to get all the csv files in the folder\n",
    "    csv_files = glob.glob(os.path.join(directory, \"*.csv\"))\n",
    "\n",
    "    # List to hold individual DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    total_rows = 0\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            # Read each file into a DataFrame\n",
    "            df = pd.read_csv(file)\n",
    "            total_rows += len(df)\n",
    "            df_list.append(df)\n",
    "            print(f\"Loaded {file}: {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "\n",
    "    # Combine all DataFrames in the list\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    print(f\"\\nTotal files processed: {len(csv_files)}\")\n",
    "    print(f\"Total rows in combined DataFrame: {len(combined_df)}\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "\n",
    "def detect_and_encode_categorical(df, max_categories=10, include_dates=True):\n",
    "    \"\"\"\n",
    "    Detect categorical columns (including object, int, and datetime), encode them, \n",
    "    and create a mapping of their indexes. Excludes the first column (assumed to be the target).\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    max_categories (int): Maximum number of unique values to consider a column categorical\n",
    "    include_dates (bool): Whether to treat date columns as categorical\n",
    "\n",
    "    Returns:\n",
    "    tuple: (preprocessed DataFrame, dict of categorical column indexes, dict of label encoders)\n",
    "    \"\"\"\n",
    "    categorical_columns = []\n",
    "    categorical_indexes = {}\n",
    "    label_encoders = {}\n",
    "\n",
    "    # Get the name of the first column (assumed to be the target)\n",
    "    target_column = df.columns[0]\n",
    "\n",
    "    for idx, (col, dtype) in enumerate(df.dtypes.items()):\n",
    "        # Skip the first column (target)\n",
    "        if col == target_column:\n",
    "            continue\n",
    "\n",
    "        if (dtype == 'object' or \n",
    "            (df[col].nunique() <= max_categories and dtype != 'float64') or\n",
    "            pd.api.types.is_integer_dtype(dtype) or\n",
    "            (include_dates and pd.api.types.is_datetime64_any_dtype(dtype))):\n",
    "\n",
    "            categorical_columns.append(col)\n",
    "            categorical_indexes[col] = idx  # Adjust index to account for skipped target column\n",
    "\n",
    "            # Handle datetime columns\n",
    "            if pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "                if include_dates:\n",
    "                    df[col] = df[col].dt.strftime('%Y-%m-%d')  # Convert to string format\n",
    "                else:\n",
    "                    continue  # Skip datetime columns if not included\n",
    "\n",
    "            # Encode categorical variables\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "\n",
    "    print(f\"Detected {len(categorical_columns)} categorical columns: {categorical_columns}\")\n",
    "    return df, categorical_indexes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--split-ratio', type=str, default=\"0.3\",dest='split_ratio')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    base_dir_input = \"/opt/ml/processing/input\"\n",
    "    base_dir = \"/opt/ml/processing/\"\n",
    "    #Read Data\n",
    "    df = load_and_combine_csv_files(base_dir_input)\n",
    "    # Sample Analysis \n",
    "    df = df.drop(\"Phone\", axis=1)\n",
    "    df[\"Area Code\"] = df[\"Area Code\"].astype(object)\n",
    "    \n",
    "    df[\"target\"] = df[\"Churn?\"].map({\"True.\": 1, \"False.\": 0})\n",
    "    df.drop([\"Churn?\"], axis=1, inplace=True)\n",
    "    \n",
    "    df = df[[\"target\"] + df.columns.tolist()[:-1]]\n",
    "    # df = pd.concat([churn]*50, ignore_index=True)\n",
    "    df, cat_columns = detect_and_encode_categorical(df, max_categories=10, include_dates=True)\n",
    "    cat_idx = list(cat_columns.values())\n",
    "    \n",
    "    # Save categorical information\n",
    "    with open(f\"{base_dir}/train/cat_idx.json\", \"w\") as outfile:\n",
    "        json.dump({\"cat_idx\": cat_idx}, outfile)\n",
    "        \n",
    "    # train, test, validation\n",
    "    train, val_n_test = train_test_split(\n",
    "        df, test_size=float(args.split_ratio), random_state=42, stratify=df[\"target\"]\n",
    "    )\n",
    "    validation, test = train_test_split(\n",
    "        val_n_test, test_size=float(args.split_ratio), random_state=42, stratify=val_n_test[\"target\"]\n",
    "    )\n",
    "    \n",
    "    # Save datasets\n",
    "    train.to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    validation.to_csv(f\"{base_dir}/validation/validation.csv\", header=False, index=False)\n",
    "    test.to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f8523-f2d1-4d75-b18e-7bcb5fd2c1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Processing Step for Feature Engineering\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "framework_version = \"1.0-1\"\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    volume_size_in_gb = processing_volume_size,\n",
    "    base_job_name=\"sklearn-pre-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "      ProcessingInput(source=s3_input_data_location, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\",\\\n",
    "                         destination = train_output_bucket),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\",\\\n",
    "                        destination = validation_output_bucket),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\",\\\n",
    "                        destination = test_output_bucket)\n",
    "    ],\n",
    "    code=   \"pipeline_scripts/churn_preprocess.py\",\n",
    "    arguments =[\n",
    "        \"--split-ratio\",data_split_ratio\n",
    "    ],\n",
    ")\n",
    "step_process = ProcessingStep(name=\"LightGBMDataPreProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e4ef6-2077-4281-a769-d492e156b7f6",
   "metadata": {},
   "source": [
    "### Step 4: Define HyperParameter Tuning  Step for Classification\n",
    "\n",
    "Here we define teh training and tuning estimator for lightGBM using SageMaker Pipeline paramters. This lets up modify the structure of the tuning job like `instance_count`, `instance_type`, `volume_size` etc as well as hyperparameters for the LightGBM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe5621-bc56-479f-a4ca-f7636866c0b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.estimator import Estimator\n",
    "import random\n",
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "train_model_id, train_model_version, train_scope = \"lightgbm-classification-model\", \"*\", \"training\"\n",
    "from sagemaker import hyperparameters\n",
    "\n",
    "# # Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")\n",
    "\n",
    "# LightGBM parameters\n",
    "hyperparameters = {'learning_rate':learning_rate,\n",
    "    'num_leaves': num_leaves,\n",
    "    'feature_fraction': feature_fraction,\n",
    "    'bagging_fraction':bagging_fraction,\n",
    "    'bagging_freq':bagging_freq,\n",
    "    'max_depth': max_depth,\n",
    "    'min_data_in_leaf':min_data_in_leaf,\n",
    "    'max_delta_step':max_delta_step,\n",
    "    'lambda_l1':lambda_l1,\n",
    "    'lambda_l2': lambda_l2,\n",
    "    'boosting': boosting,\n",
    "    'min_gain_to_split': min_gain_to_split,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'tree_learner': tree_learner,\n",
    "    'feature_fraction_bynode': feature_fraction_bynode,\n",
    "    'is_unbalance': is_unbalance,\n",
    "    'max_bin': max_bin,\n",
    "    'num_threads': num_threads,\n",
    "    'verbosity':verbosity,\n",
    "    'use_dask': use_dask,\n",
    "    'sagemaker_program' : training_code_entry_point,\n",
    "    'sagemaker_submit_directory': s3_training_code_location,\n",
    "    'num_boost_round': num_boost_round,\n",
    "    'metric': algo_metric\n",
    "}\n",
    "print(hyperparameters)\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tabular_estimator = Estimator(\n",
    "    role=role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir= \"model_cat\", \n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"train.py\", \n",
    "    instance_count= training_instance_count,  # pipeline paramter\n",
    "    volume_size=training_volume_size,  # pipeline paramter\n",
    "    instance_type=training_instance_type, # pipeline paramter\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=model_output_bucket,\n",
    "    sagemaker_session=pipeline_session, # Tells it its part of a Sagemaker Pipeline and not to execute individually\n",
    "    environment={\"MLFLOW_TRACKING_ARN\": ml_flow_arn}, # pipeline paramter\n",
    "    keep_alive_period_in_seconds = 1000 #Keep instance warm for fast experimentation iteration else experience cold start for each trials (note you will incur cost of warm instances)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3962cf-d2ca-4fe2-bad2-2afb33e64bb3",
   "metadata": {},
   "source": [
    "Here we also parameterize the HyperParameter tuning ranges for the job so that we can modify during SageMaker Pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd27af1-28c8-4af5-a80e-08314ecd9088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter, HyperparameterTuner\n",
    "\n",
    "# Define hyperparameter ranges (Pipeline parameters)\n",
    "hyperparameter_ranges_lgb = {\n",
    "    \"learning_rate\": ContinuousParameter(learning_rate_min , learning_rate_max , scaling_type=\"Auto\"),\n",
    "    \"num_boost_round\": IntegerParameter(num_boost_round_min , num_boost_round_max),\n",
    "    \"num_leaves\": IntegerParameter(num_leaves_min , num_leaves_max),\n",
    "    \"feature_fraction\": ContinuousParameter(feature_fraction_min, feature_fraction_max),\n",
    "    \"bagging_fraction\": ContinuousParameter(bagging_fraction_min, bagging_fraction_max),\n",
    "    \"bagging_freq\": IntegerParameter(bagging_freq_min, bagging_freq_max),\n",
    "    \"max_depth\": IntegerParameter(max_depth_min, max_depth_max),\n",
    "    \"min_data_in_leaf\": IntegerParameter(min_data_in_leaf_min, min_data_in_leaf_max),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator = tabular_estimator,\n",
    "    objective_metric_name = tuner_objective_metric, # pipeline paramter\n",
    "    hyperparameter_ranges = hyperparameter_ranges_lgb,  # pipeline paramter\n",
    "    metric_definitions = [{\"Name\": tuner_objective_metric, \"Regex\": Join(on=':',values=[tuner_objective_metric ,\" ([0-9\\\\.]+)\" ] )}], # pipeline paramter\n",
    "    max_jobs=max_tuning_jobs, # pipeline paramter\n",
    "    max_parallel_jobs=max_tuning_parallel_job, # pipeline paramter\n",
    "    objective_type=optimization_direction, # pipeline paramter\n",
    "    strategy = tuning_strategy # pipeline paramter\n",
    ") \n",
    "\n",
    "# Here we create an implicit dependencies between the processing step and Tuning step\n",
    "hpo_args = tuner.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "step_tuning = TuningStep(\n",
    "    name=\"LightGBMHyperParameterTuning\",\n",
    "    step_args=hpo_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafd7f0-907b-4b55-9c43-e67888e26cc6",
   "metadata": {},
   "source": [
    "## Build and Trigger the pipeline run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648ad67-80e5-404f-b47e-27df019c6d81",
   "metadata": {},
   "source": [
    "After defining all of the component steps, you can assemble them into a Pipelines object. You donâ€™t need to specify the order of pipeline because Pipelines automatically infers the order sequence based on the dependencies between the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123803e-f286-4acc-b016-4620eb2017a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        # LightGBM Algo Parameters      \n",
    "            learning_rate,\n",
    "            num_leaves,\n",
    "            feature_fraction,\n",
    "           bagging_fraction,\n",
    "            bagging_freq,\n",
    "           max_depth,\n",
    "            min_data_in_leaf,\n",
    "           max_delta_step,\n",
    "          lambda_l1,\n",
    "           lambda_l2,\n",
    "            boosting,\n",
    "            min_gain_to_split,\n",
    "            scale_pos_weight,\n",
    "            tree_learner,\n",
    "            feature_fraction_bynode,\n",
    "           is_unbalance,\n",
    "            max_bin,\n",
    "           num_threads,\n",
    "            verbosity,\n",
    "            use_dask,\n",
    "        num_boost_round,\n",
    "        \n",
    "        # LightGBM tunable parameters\n",
    "        learning_rate_min,\n",
    "        learning_rate_max,\n",
    "        num_leaves_min,\n",
    "        num_leaves_max,\n",
    "        feature_fraction_min,\n",
    "        feature_fraction_max,\n",
    "        bagging_fraction_min,\n",
    "        bagging_fraction_max,\n",
    "        bagging_freq_min,\n",
    "        bagging_freq_max,\n",
    "        max_depth_min,\n",
    "        max_depth_max,\n",
    "        min_data_in_leaf_min,\n",
    "        min_data_in_leaf_max,\n",
    "        num_boost_round_max,\n",
    "        num_boost_round_min,\n",
    "\n",
    "        # Other parameters\n",
    "        training_code_entry_point,\n",
    "        s3_training_code_location,\n",
    "        processing_volume_size,\n",
    "        training_volume_size,\n",
    "        tuner_metric_definition,\n",
    "        tuner_objective_metric,\n",
    "        algo_metric,\n",
    "        processing_instance_count,\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        training_instance_count,\n",
    "        model_output_bucket,\n",
    "        train_output_bucket,\n",
    "        validation_output_bucket,\n",
    "        test_output_bucket,\n",
    "        max_tuning_jobs,\n",
    "        max_tuning_parallel_job,\n",
    "        tuning_strategy,\n",
    "        optimization_direction,\n",
    "        ml_flow_arn,\n",
    "        model_evaluation_threshold,\n",
    "        s3_input_data_location,\n",
    "        data_split_ratio,\n",
    "    ],\n",
    "    steps=[step_process,step_tuning], # we pass only the condition step as we have declared all steps as dependencies to the condition step\n",
    ")\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "print(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f3338-f9d3-49b5-8d70-9bfdb7676770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new or update existing Pipeline\n",
    "pipeline.upsert(role_arn=role)\n",
    "# start Pipeline execution\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d1543-13c4-47e4-86df-32ab166c31e6",
   "metadata": {},
   "source": [
    "## Create a new pipeline execution with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5ddf3-a779-401d-bb09-6a96e1284623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets modify the objective metric to \"binary_error\" and create a new pipeline execution. We use the Pipelien Parameter Names instead of teh varaible names\n",
    "pipeline.start(\n",
    "    parameters=dict(\n",
    "        AlgorithmMetric=\"binary_error\",\n",
    "        TunerObjectiveMetric = \"binary_error\",\n",
    "        TunerMetricDefinition=\"binary_error: ([0-9\\\\.]+)\",\n",
    "        OptimizationDirection = \"Minimize\",\n",
    "        TrainingInstanceType = \"ml.c5.4xlarge\",\n",
    "        ProcessingInstanceType = \"ml.m5.4xlarge\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1aefa9-74a6-4467-affb-fda64ec1da9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
